{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "70-XsHkDGUKu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15799778641130541447]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BsYPkY7gYGx"
   },
   "source": [
    "# Multiclass classification\n",
    "Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.49</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.68</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>520.01</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.67</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.30</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
       "0      1400.60  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1      1403.14  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2      1404.20  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3      1401.87  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4      1406.22  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "...        ...    ...  ...     ...      ...      ...     ...   ...  ...   ...   \n",
       "20626  1428.63  14.62  ...  519.49  2388.26  8137.60  8.4956  0.03  397  2388   \n",
       "20627  1433.58  14.62  ...  519.68  2388.22  8136.50  8.5139  0.03  395  2388   \n",
       "20628  1428.18  14.62  ...  520.01  2388.24  8141.05  8.5646  0.03  398  2388   \n",
       "20629  1426.53  14.62  ...  519.67  2388.23  8139.29  8.5389  0.03  395  2388   \n",
       "20630  1432.14  14.62  ...  519.30  2388.26  8137.33  8.5036  0.03  396  2388   \n",
       "\n",
       "         s19    s20      s21  \n",
       "0      100.0  39.06  23.4190  \n",
       "1      100.0  39.00  23.4236  \n",
       "2      100.0  38.95  23.3442  \n",
       "3      100.0  38.88  23.3739  \n",
       "4      100.0  38.90  23.4044  \n",
       "...      ...    ...      ...  \n",
       "20626  100.0  38.49  22.9735  \n",
       "20627  100.0  38.30  23.1594  \n",
       "20628  100.0  38.44  22.9333  \n",
       "20629  100.0  38.29  23.0640  \n",
       "20630  100.0  38.37  23.0522  \n",
       "\n",
       "[20631 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "w1 = 30\n",
    "w0 = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I want to separate the training set into a training and validation set. I will use 80 training sets for the training and 20 training sets as evaluation sets for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ID = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I separate into training and validation set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16141</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4493 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "16138   81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91   \n",
       "16139   81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25   \n",
       "16140   81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42   \n",
       "16141   81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89   \n",
       "16142   81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s15   s16  s17   s18    s19    s20      s21  \\\n",
       "16138  1406.63  14.62  ...  8.4455  0.03  391  2388  100.0  38.87  23.3365   \n",
       "16139  1407.88  14.62  ...  8.4573  0.03  392  2388  100.0  38.91  23.3452   \n",
       "16140  1396.40  14.62  ...  8.4522  0.03  394  2388  100.0  39.04  23.3610   \n",
       "16141  1404.86  14.62  ...  8.4403  0.03  392  2388  100.0  38.77  23.4206   \n",
       "16142  1409.58  14.62  ...  8.3971  0.03  392  2388  100.0  39.04  23.3311   \n",
       "...        ...    ...  ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "20626  1428.63  14.62  ...  8.4956  0.03  397  2388  100.0  38.49  22.9735   \n",
       "20627  1433.58  14.62  ...  8.5139  0.03  395  2388  100.0  38.30  23.1594   \n",
       "20628  1428.18  14.62  ...  8.5646  0.03  398  2388  100.0  38.44  22.9333   \n",
       "20629  1426.53  14.62  ...  8.5389  0.03  395  2388  100.0  38.29  23.0640   \n",
       "20630  1432.14  14.62  ...  8.5036  0.03  396  2388  100.0  38.37  23.0522   \n",
       "\n",
       "       RUL  label1  label2  \n",
       "16138  239       0       0  \n",
       "16139  238       0       0  \n",
       "16140  237       0       0  \n",
       "16141  236       0       0  \n",
       "16142  235       0       0  \n",
       "...    ...     ...     ...  \n",
       "20626    4       1       2  \n",
       "20627    3       1       2  \n",
       "20628    2       1       2  \n",
       "20629    1       1       2  \n",
       "20630    0       1       2  \n",
       "\n",
       "[4493 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the min max scaling of the training data set\n",
    "# use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.425154</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.473456</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.732001</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.386193</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.661565</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387283</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.269082</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.704790</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>80</td>\n",
       "      <td>181</td>\n",
       "      <td>0.739884</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840361</td>\n",
       "      <td>0.756892</td>\n",
       "      <td>0.787812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>0.369473</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.498615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.736614</td>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.504155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16136</th>\n",
       "      <td>80</td>\n",
       "      <td>184</td>\n",
       "      <td>0.358382</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.728412</td>\n",
       "      <td>0.809926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291339</td>\n",
       "      <td>0.127551</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.506925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0.583815</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.661040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>0.177438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16138 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0       1      1  0.456647  0.166667       0.0  0.0  0.183735  0.425154   \n",
       "1       1      2  0.606936  0.250000       0.0  0.0  0.283133  0.473456   \n",
       "2       1      3  0.248555  0.750000       0.0  0.0  0.343373  0.386193   \n",
       "3       1      4  0.537572  0.500000       0.0  0.0  0.343373  0.267715   \n",
       "4       1      5  0.387283  0.333333       0.0  0.0  0.349398  0.269082   \n",
       "...    ..    ...       ...       ...       ...  ...       ...       ...   \n",
       "16133  80    181  0.739884  0.666667       0.0  0.0  0.840361  0.756892   \n",
       "16134  80    182  0.416185  0.833333       0.0  0.0  0.783133  0.621554   \n",
       "16135  80    183  0.601156  0.500000       0.0  0.0  0.686747  0.736614   \n",
       "16136  80    184  0.358382  0.666667       0.0  0.0  0.789157  0.728412   \n",
       "16137  80    185  0.583815  0.500000       0.0  0.0  0.804217  0.805195   \n",
       "\n",
       "             s4   s5  ...  s16       s17  s18  s19       s20       s21  RUL  \\\n",
       "0      0.309757  0.0  ...  0.0  0.363636  0.0  0.0  0.708661  0.725482  191   \n",
       "1      0.352633  0.0  ...  0.0  0.363636  0.0  0.0  0.661417  0.732001  190   \n",
       "2      0.370527  0.0  ...  0.0  0.181818  0.0  0.0  0.622047  0.619473  189   \n",
       "3      0.331195  0.0  ...  0.0  0.363636  0.0  0.0  0.566929  0.661565  188   \n",
       "4      0.404625  0.0  ...  0.0  0.454545  0.0  0.0  0.582677  0.704790  187   \n",
       "...         ...  ...  ...  ...       ...  ...  ...       ...       ...  ...   \n",
       "16133  0.787812  0.0  ...  0.0  0.818182  0.0  0.0  0.181102  0.369473    4   \n",
       "16134  0.743754  0.0  ...  0.0  0.727273  0.0  0.0  0.141732  0.151786    3   \n",
       "16135  0.878629  0.0  ...  0.0  0.818182  0.0  0.0  0.141732  0.037698    2   \n",
       "16136  0.809926  0.0  ...  0.0  0.818182  0.0  0.0  0.291339  0.127551    1   \n",
       "16137  0.661040  0.0  ...  0.0  0.818182  0.0  0.0  0.149606  0.177438    0   \n",
       "\n",
       "       label1  label2  cycle_norm  \n",
       "0           0       0    0.000000  \n",
       "1           0       0    0.002770  \n",
       "2           0       0    0.005540  \n",
       "3           0       0    0.008310  \n",
       "4           0       0    0.011080  \n",
       "...       ...     ...         ...  \n",
       "16133       1       2    0.498615  \n",
       "16134       1       2    0.501385  \n",
       "16135       1       2    0.504155  \n",
       "16136       1       2    0.506925  \n",
       "16137       1       2    0.509695  \n",
       "\n",
       "[16138 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "zSInZu-EkFtf",
    "outputId": "eeaa639d-5fe6-41e1-a114-5de359a792c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 50, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target. \n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label2']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When modeling multi-class classification problems using neural networks, \n",
    "# it is good practice to reshape the output attribute from a vector that contains values for each class value to be \n",
    "# a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "# This is called one hot encoding or creating dummy variables from a categorical variable.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_label_array = to_categorical(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 100)           50400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 80,753\n",
      "Trainable params: 80,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# # fit the network\n",
    "# history = model.fit(seq_array, dummy_label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "#                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "#           )\n",
    "\n",
    "# # list all data in history\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every time I retrain the algorithm I get different training results, i.e., also different evaluation of the decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set created during the training (i.e., validation_split=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1393
    },
    "id": "FpVYSzXkmk5l",
    "outputId": "50af192e-e9df-46ad-ffa3-0b7bd0e3f29c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-79fcddb0a351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# summarize history for Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 2s 39ms/step - loss: 0.0586 - accuracy: 0.9749\n",
      "Accurracy: 0.9748722910881042\n"
     ]
    }
   ],
   "source": [
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, dummy_label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n",
      "[[9585   73    0]\n",
      " [ 139 1426   35]\n",
      " [   1   56  823]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "# y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\") # this way (>0.5) the outcome goes from a probability to 0,1\n",
    "y_true = dummy_label_array\n",
    "\n",
    "# test_set = pd.DataFrame(y_pred)\n",
    "# # test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Second PdM policy evaluation on the validation set.\n",
    "\n",
    "For each validation set, I need to give the on-line sensor data as input to the trained LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 333\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['cycle_norm'] = validation_df['cycle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second PdM policy evaluation on a single validation data set (id).\n",
    "\n",
    "## The scaling of the dataset is performed anew on-line every time new data appears.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id=82\n",
    "# preventive_replacement = False\n",
    "# order                  = False\n",
    "\n",
    "# for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1): \n",
    "    \n",
    "#     if cycle in array_decisions:\n",
    "#         print('cycle:', sequence_length+cycle)\n",
    "        \n",
    "#         norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==82][cols_normalize][:sequence_length+cycle]), \n",
    "#                  columns=cols_normalize, \n",
    "#                  index=validation_df[validation_df['id']==82][:sequence_length+cycle].index)\n",
    "        \n",
    "#         join_df = validation_df[validation_df['id']==82][:sequence_length+cycle][validation_df[validation_df['id']==82][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "#         validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==82][:sequence_length+cycle].columns)        \n",
    "#         print(validation_df_eval_online.shape)        \n",
    "        \n",
    "#         seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "#         seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "#         prob_RUL_smaller_DT    = estimator.predict(seq_array_validation_k).reshape(3)[2]\n",
    "#         prob_RUL_smaller_w1    = estimator.predict(seq_array_validation_k).reshape(3)[1]\n",
    "        \n",
    "#         print('LSTM output probabilities:', estimator.predict(seq_array_validation_k).reshape(3))\n",
    "                \n",
    "#         # evaluate decision heuristics\n",
    "#         if order == False:\n",
    "#             if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "#                 t_order = sequence_length+cycle\n",
    "#                 order = True\n",
    "#                 print('component ordering at cycle:', t_order)\n",
    "    \n",
    "#         if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "#             t_LC = sequence_length+cycle\n",
    "#             cost_rep_id = C_p\n",
    "#             print('preventive replacement informed at cycle:', t_LC)\n",
    "#             print('component lifecycle:', t_LC)\n",
    "#             preventive_replacement = True\n",
    "#             cost_delay_id = max(t_order+L-t_LC, 0) * C_unav\n",
    "            \n",
    "#             cost_stock_id    = max(t_LC -(t_order+L), 0)*C_inv\n",
    "#             print('delay time', max(t_order+L-t_LC, 0))\n",
    "#             print('cost delay:',cost_delay_id)\n",
    "#             print('cost stock:', cost_stock_id)\n",
    "#             break\n",
    "            \n",
    "# if preventive_replacement == False:\n",
    "#     t_LC = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "#     print('Component failure at t:', t_LC)\n",
    "#     cost_rep_id = C_c\n",
    "#     if order == False:\n",
    "#         cost_delay_id = L * C_unav\n",
    "#     else:\n",
    "#         cost_delay_id = max(t_order+L-t_LC, 0) * C_unav\n",
    "#         cost_stock_id    = max(t_LC -(t_order+L), 0)*C_inv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second PdM policy evaluation on a the whole validation data set (ids 81 to 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array   = np.zeros(20)\n",
    "costs_delay_array = np.zeros(20)\n",
    "costs_stock_array = np.zeros(20)\n",
    "\n",
    "t_LC_array        = np.zeros(20)\n",
    "t_order_array     = np.zeros(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 81\n",
      "prob_RUL_smaller_w1: 0.9236914\n",
      "component ordering at cycle: 210.0\n",
      "prob_RUL_smaller_DT: 0.517332\n",
      "preventive replacement informed at cycle: 230.0\n",
      "True failure: 240\n",
      "-----------------------------------------\n",
      "ID: 82\n",
      "prob_RUL_smaller_w1: 0.97790694\n",
      "component ordering at cycle: 190.0\n",
      "prob_RUL_smaller_DT: 0.9996543\n",
      "preventive replacement informed at cycle: 210.0\n",
      "True failure: 214\n",
      "-----------------------------------------\n",
      "ID: 83\n",
      "prob_RUL_smaller_w1: 0.988364\n",
      "component ordering at cycle: 270.0\n",
      "prob_RUL_smaller_DT: 0.9964598\n",
      "preventive replacement informed at cycle: 290.0\n",
      "True failure: 293\n",
      "-----------------------------------------\n",
      "ID: 84\n",
      "prob_RUL_smaller_w1: 0.45661032\n",
      "component ordering at cycle: 240.0\n",
      "prob_RUL_smaller_DT: 0.62771994\n",
      "preventive replacement informed at cycle: 260.0\n",
      "True failure: 267\n",
      "-----------------------------------------\n",
      "ID: 85\n",
      "prob_RUL_smaller_w1: 0.30327806\n",
      "component ordering at cycle: 160.0\n",
      "prob_RUL_smaller_DT: 0.9666913\n",
      "preventive replacement informed at cycle: 180.0\n",
      "True failure: 188\n",
      "-----------------------------------------\n",
      "ID: 86\n",
      "prob_RUL_smaller_w1: 0.986754\n",
      "component ordering at cycle: 250.0\n",
      "prob_RUL_smaller_DT: 0.9907217\n",
      "preventive replacement informed at cycle: 270.0\n",
      "True failure: 278\n",
      "-----------------------------------------\n",
      "ID: 87\n",
      "prob_RUL_smaller_w1: 0.35670143\n",
      "component ordering at cycle: 140.0\n",
      "prob_RUL_smaller_DT: 0.99033886\n",
      "preventive replacement informed at cycle: 170.0\n",
      "True failure: 178\n",
      "-----------------------------------------\n",
      "ID: 88\n",
      "prob_RUL_smaller_w1: 0.8389868\n",
      "component ordering at cycle: 180.0\n",
      "prob_RUL_smaller_DT: 0.9381065\n",
      "preventive replacement informed at cycle: 200.0\n",
      "True failure: 213\n",
      "-----------------------------------------\n",
      "ID: 89\n",
      "prob_RUL_smaller_w1: 0.9664196\n",
      "component ordering at cycle: 190.0\n",
      "prob_RUL_smaller_DT: 0.99843544\n",
      "preventive replacement informed at cycle: 210.0\n",
      "True failure: 217\n",
      "-----------------------------------------\n",
      "ID: 90\n",
      "prob_RUL_smaller_w1: 0.76993424\n",
      "component ordering at cycle: 130.0\n",
      "prob_RUL_smaller_DT: 0.99869883\n",
      "preventive replacement informed at cycle: 150.0\n",
      "True failure: 154\n",
      "-----------------------------------------\n",
      "ID: 91\n",
      "prob_RUL_smaller_w1: 0.5216109\n",
      "component ordering at cycle: 100.0\n",
      "prob_RUL_smaller_DT: 0.9928704\n",
      "preventive replacement informed at cycle: 130.0\n",
      "True failure: 135\n",
      "-----------------------------------------\n",
      "ID: 92\n",
      "prob_RUL_smaller_w1: 0.96523976\n",
      "component ordering at cycle: 320.0\n",
      "prob_RUL_smaller_DT: 0.9974406\n",
      "preventive replacement informed at cycle: 340.0\n",
      "True failure: 341\n",
      "-----------------------------------------\n",
      "ID: 93\n",
      "prob_RUL_smaller_w1: 0.99191886\n",
      "component ordering at cycle: 130.0\n",
      "prob_RUL_smaller_DT: 0.99898547\n",
      "preventive replacement informed at cycle: 150.0\n",
      "True failure: 155\n",
      "-----------------------------------------\n",
      "ID: 94\n",
      "prob_RUL_smaller_w1: 0.7932366\n",
      "component ordering at cycle: 230.0\n",
      "prob_RUL_smaller_DT: 0.6757899\n",
      "preventive replacement informed at cycle: 250.0\n",
      "True failure: 258\n",
      "-----------------------------------------\n",
      "ID: 95\n",
      "prob_RUL_smaller_w1: 0.60433894\n",
      "component ordering at cycle: 260.0\n",
      "prob_RUL_smaller_DT: 0.99752074\n",
      "preventive replacement informed at cycle: 280.0\n",
      "True failure: 283\n",
      "-----------------------------------------\n",
      "ID: 96\n",
      "prob_RUL_smaller_w1: 0.8536873\n",
      "component ordering at cycle: 310.0\n",
      "prob_RUL_smaller_DT: 0.8109263\n",
      "preventive replacement informed at cycle: 330.0\n",
      "True failure: 336\n",
      "-----------------------------------------\n",
      "ID: 97\n",
      "prob_RUL_smaller_w1: 0.98505753\n",
      "component ordering at cycle: 180.0\n",
      "prob_RUL_smaller_DT: 0.9996693\n",
      "preventive replacement informed at cycle: 200.0\n",
      "True failure: 202\n",
      "-----------------------------------------\n",
      "ID: 98\n",
      "prob_RUL_smaller_w1: 0.960182\n",
      "component ordering at cycle: 130.0\n",
      "prob_RUL_smaller_DT: 0.9961319\n",
      "preventive replacement informed at cycle: 150.0\n",
      "True failure: 156\n",
      "-----------------------------------------\n",
      "ID: 99\n",
      "prob_RUL_smaller_w1: 0.9870342\n",
      "component ordering at cycle: 160.0\n",
      "prob_RUL_smaller_DT: 0.99917006\n",
      "preventive replacement informed at cycle: 180.0\n",
      "True failure: 185\n",
      "-----------------------------------------\n",
      "ID: 100\n",
      "prob_RUL_smaller_w1: 0.97795826\n",
      "component ordering at cycle: 170.0\n",
      "prob_RUL_smaller_DT: 0.68556046\n",
      "preventive replacement informed at cycle: 190.0\n",
      "True failure: 200\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for id in validation_df['id'].unique():\n",
    "    print('ID:', id)\n",
    "    preventive_replacement = False\n",
    "    order                  = False\n",
    "\n",
    "    for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1): \n",
    "\n",
    "        if cycle in array_decisions:\n",
    "            \n",
    "            norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==id][cols_normalize][:sequence_length+cycle]), \n",
    "                 columns=cols_normalize, \n",
    "                 index=validation_df[validation_df['id']==id][:sequence_length+cycle].index)\n",
    "            \n",
    "            join_df = validation_df[validation_df['id']==id][:sequence_length+cycle][validation_df[validation_df['id']==id][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "            validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==id][cycle:sequence_length+cycle].columns)        \n",
    "            \n",
    "            seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "            seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "            prob_RUL_smaller_DT    = estimator.predict(seq_array_validation_k).reshape(3)[2]\n",
    "            prob_RUL_smaller_w1    = estimator.predict(seq_array_validation_k).reshape(3)[1]\n",
    "\n",
    "            \n",
    "            \n",
    "            # evaluate decision heuristics\n",
    "            if order == False:\n",
    "                if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                    t_order_array[counter] = sequence_length+cycle\n",
    "                    order = True\n",
    "                    print('component ordering at cycle:', t_order_array[counter])\n",
    "\n",
    "            if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                \n",
    "                t_LC_array[counter] = sequence_length+cycle\n",
    "                costs_rep_array[counter] = C_p\n",
    "                print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                # print('component lifecycle:', t_LC)\n",
    "                preventive_replacement = True\n",
    "                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "\n",
    "                costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "                # print('delay time', max(t_order+L-t_LC, 0))\n",
    "                # print('cost_delay_id:',cost_delay_id)\n",
    "                # print('cost of stock:', cost_stock_id)\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        t_LC_array[counter] = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "        print('Component failure at t:', t_LC_array[counter])\n",
    "        costs_rep_array[counter] = C_c\n",
    "        \n",
    "        if order == False:\n",
    "            costs_delay_array[counter] = L * C_unav\n",
    "        else:\n",
    "            costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "            costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "    \n",
    "    print('True failure:', validation_df[validation_df['id']==id]['cycle'].iloc[-1])\n",
    "    print('-----------------------------------------')\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0., 10.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_tot = costs_rep_array+costs_delay_array+costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 110., 100., 100., 100., 110.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230., 210., 290., 260., 180., 270., 170., 200., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 200., 150., 180., 190.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210., 190., 270., 240., 160., 250., 140., 180., 190., 130., 100.,\n",
       "       320., 130., 230., 260., 310., 180., 130., 160., 170.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4622425629290618"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT    \n",
    "    counter+=1\n",
    "    \n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 210., 290., 260., 180., 270., 170., 210., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 200., 150., 180., 200.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016933638443935983"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6933638443935983"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "name": "Predictive-Maintenance-using-LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
