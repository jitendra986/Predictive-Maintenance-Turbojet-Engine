{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70-XsHkDGUKu",
    "outputId": "4a5f97ed-c7cc-44c4-fc13-65ae479147b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/eragroup/anaconda3/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: matplotlib in /home/eragroup/anaconda3/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.33.6)\n",
      "Requirement already satisfied: setuptools in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (62.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0)\n",
      "Requirement already satisfied: requests in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /home/eragroup/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (1.26.19)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2019.9.11)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /home/eragroup/anaconda3/lib/python3.7/site-packages (0.17.4)\n",
      "Requirement already satisfied: platformdirs in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.5.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (62.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (5.6.3)\n",
      "Requirement already satisfied: pyyaml in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (5.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.7.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.12.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (3.13.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: setproctitle in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/eragroup/anaconda3/lib/python3.7/site-packages (from click!=8.0.0,>=7.1->wandb) (0.23)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2019.9.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.19)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click!=8.0.0,>=7.1->wandb) (3.8.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib numpy\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmWTwJyIYWDo"
   },
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Yeksw_AMYVaV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for max_split_size_mb\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3wOjKm186Pth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available GPU:  1\n",
      "Available devices:\n",
      "- <torch.cuda.device object at 0x7fe55703e450>\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#device_lib.list_local_devices()\n",
    "\n",
    "# Get the available devices\n",
    "devices = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print('number of available GPU: ',torch.cuda.device_count())  # Should print the number of visible devices\n",
    "\n",
    "# Print the available devices\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(f\"- {device}\")\n",
    "device = torch.device(\"cuda:0\")  # Use the first GPU\n",
    "\n",
    "# Print the current device\n",
    "#print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6859390976, 8512602112)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  9 16:24:01 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P4000        Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 46%   39C    P8     5W / 105W |   1576MiB /  8118MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2319      G   ...ATLAB2/bin/glnxa64/MATLAB        2MiB |\n",
      "|    0   N/A  N/A      2605      G   ...features=BackForwardCache        2MiB |\n",
      "|    0   N/A  N/A      2978      G   ...ATLAB2/bin/glnxa64/MATLAB        2MiB |\n",
      "|    0   N/A  N/A      3247      G   ...ATLAB2/bin/glnxa64/MATLAB        2MiB |\n",
      "|    0   N/A  N/A      3357      G   ...features=BackForwardCache        2MiB |\n",
      "|    0   N/A  N/A      3443      G   /usr/bin/systemsettings5           11MiB |\n",
      "|    0   N/A  N/A      3769      G   ...features=BackForwardCache        2MiB |\n",
      "|    0   N/A  N/A      4087      G   ...ATLAB2/bin/glnxa64/MATLAB        2MiB |\n",
      "|    0   N/A  N/A      4606      G   ...features=BackForwardCache        2MiB |\n",
      "|    0   N/A  N/A      5319      G   ...ATLAB2/bin/glnxa64/MATLAB       20MiB |\n",
      "|    0   N/A  N/A      5600      G   ...features=BackForwardCache        2MiB |\n",
      "|    0   N/A  N/A      6175      G   ...bexec/kscreenlocker_greet      276MiB |\n",
      "|    0   N/A  N/A      6281      G   /usr/lib/xorg/Xorg                106MiB |\n",
      "|    0   N/A  N/A      6441      G   /usr/lib/xorg/Xorg                314MiB |\n",
      "|    0   N/A  N/A      6511      G   kwin_x11                           43MiB |\n",
      "|    0   N/A  N/A      6517      G   /usr/bin/krunner                   24MiB |\n",
      "|    0   N/A  N/A      6519      G   /usr/bin/plasmashell               56MiB |\n",
      "|    0   N/A  N/A      6662      G   kwin_x11                           30MiB |\n",
      "|    0   N/A  N/A      6670      G   /usr/bin/krunner                   28MiB |\n",
      "|    0   N/A  N/A      6672      G   /usr/bin/plasmashell               53MiB |\n",
      "|    0   N/A  N/A     17864      C   ...roup/anaconda3/bin/python      545MiB |\n",
      "|    0   N/A  N/A     19624      G   ...bexec/kscreenlocker_greet       28MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4BsYPkY7gYGx"
   },
   "outputs": [],
   "source": [
    "# Multiclass classification\n",
    "#Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iklamKYlNjh"
   },
   "source": [
    "\n",
    "`os`: This module provides functions for interacting with the operating system. It's commonly used for tasks such as file manipulation and directory operations.<br>\n",
    "`sklearn.preprocessing`: This module from scikit-learn provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables.<br>\n",
    "`sklearn.metrics`: This module contains functions for evaluating model performance, such as computing confusion matrices, recall scores, and precision scores.<br>\n",
    "`multiclass_model_w1_30.h5`:The .h5 extension indicates that the model will be saved in the Hierarchical Data Format version 5 (HDF5) format, which is commonly used for storing large numerical datasets. The model will be saved with the filename **multiclass_model_w1_30.h5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "\n",
    "\n",
    "# define path to save model\n",
    "#model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "from data_preprocessor import DataPreprocessor\n",
    "\n",
    "# Initialize the preprocessor with the path to your training data file\n",
    "preprocessor = DataPreprocessor('PM_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 28)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpsNnInsevi"
   },
   "source": [
    "`train_df.sort_values(['id','cycle'])`: This line sorts the DataFrame **train_df** first by the 'id' column and then by the 'cycle' column. It ensures that the data is ordered by engine ID and cycle number, which may be necessary for certain analyses or modeling tasks. The sorted DataFrame is then assigned back to the variable **train_df**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing\n",
    "data preprocessing step, particularly for labeling the data for training purposes. Let's break down what each part of the code does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12138, 50, 25),\n",
       " (12138, 3),\n",
       " (1742, 50, 25),\n",
       " (1742, 3),\n",
       " (1751, 50, 25),\n",
       " (1751, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_array,dummy_label_array,seq_array_validation, dummy_label_array_validation,seq_array_test,dummy_label_array_test, sequence_cols = preprocessor.preprocess()\n",
    "seq_array.shape, dummy_label_array.shape, seq_array_validation.shape, dummy_label_array_validation.shape,seq_array_test.shape, dummy_label_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setting1', 'setting2', 'setting3', 'cycle_norm', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n"
     ]
    }
   ],
   "source": [
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2251, 29), (16138, 29), (2242, 29))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df,test_df,train_df = preprocessor.test_data_pdmPolicy()\n",
    "test_df.shape, train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4493, 29),\n",
       " Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
       "        's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
       "        's15', 's16', 's17', 's18', 's19', 's20', 's21', 'RUL', 'label1',\n",
       "        'label2'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df = pd.concat([validation_df, test_df], ignore_index=True)\n",
    "complete_df = pd.concat([train_df, pdm_df], ignore_index=True)\n",
    "pdm_df.shape, pdm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0  81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91  1406.63   \n",
       "1  81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25  1407.88   \n",
       "2  81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42  1396.40   \n",
       "3  81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89  1404.86   \n",
       "4  81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49  1409.58   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4455  0.03  391  2388  100.0  38.87  23.3365  239       0   \n",
       "1  14.62  ...  8.4573  0.03  392  2388  100.0  38.91  23.3452  238       0   \n",
       "2  14.62  ...  8.4522  0.03  394  2388  100.0  39.04  23.3610  237       0   \n",
       "3  14.62  ...  8.4403  0.03  392  2388  100.0  38.77  23.4206  236       0   \n",
       "4  14.62  ...  8.3971  0.03  392  2388  100.0  39.04  23.3311  235       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cycle_norm', 's1', 's10', 's11', 's12', 's13', 's14', 's15', 's16',\n",
      "       's17', 's18', 's19', 's2', 's20', 's21', 's3', 's4', 's5', 's6', 's7',\n",
      "       's8', 's9', 'setting1', 'setting2', 'setting3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols_normalize_train,cols_normalize_validation,cols_normalize_test = preprocessor.normalize_data_pdmPolicy()\n",
    "print(cols_normalize_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0  81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91  1406.63   \n",
       "1  81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25  1407.88   \n",
       "2  81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42  1396.40   \n",
       "3  81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89  1404.86   \n",
       "4  81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49  1409.58   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4455  0.03  391  2388  100.0  38.87  23.3365  239       0   \n",
       "1  14.62  ...  8.4573  0.03  392  2388  100.0  38.91  23.3452  238       0   \n",
       "2  14.62  ...  8.4522  0.03  394  2388  100.0  39.04  23.3610  237       0   \n",
       "3  14.62  ...  8.4403  0.03  392  2388  100.0  38.77  23.4206  236       0   \n",
       "4  14.62  ...  8.3971  0.03  392  2388  100.0  39.04  23.3311  235       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-_dxq60Nf4"
   },
   "source": [
    ">`Data Labeling`: This part calculates the Remaining Useful Life (RUL) or Time to Failure for each engine by finding the maximum cycle number (cycle) for each engine ID (id). The result is stored in a DataFrame rul with columns 'id' and 'max'.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZubPMTD0T9z"
   },
   "source": [
    ">`Merge RUL with Training Data`:the RUL information is merged back into the original training DataFrame **train_df** based on the engine ID. This allows each row in train_df to have the corresponding maximum cycle number as well.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-eouxGv0gpG"
   },
   "source": [
    ">`Calculate RUL`: This line calculates the RUL by subtracting the current cycle number ('cycle') from the maximum cycle number ('max') for each engine. This represents how many more cycles the engine is expected to operate before failure.<br>\n",
    ">`Drop Unnecessary Columns`: After calculating RUL, the 'max' column, which was used temporarily to calculate RUL, is dropped from the DataFrame as it's no longer needed.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFd2pLr0odt"
   },
   "source": [
    "> `Labeling for Classification`: This part assigns labels to each data point based on the calculated RUL. It defines thresholds `w1` and `w0`, and assigns:\n",
    ">> * Label 1 ('label1') as 1 if RUL is less than or equal to 'w1', and 0 otherwise.\n",
    ">> * Label2 ('label2') as 1 if RUL is less than or equal to 'w1', 2 if RUL is less than or equal to 'w0', and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_8MNc2Z6Ptu"
   },
   "source": [
    " Now I want to separate the train_df set into a training/validation/test set. I will use 80% training sets for the training and 10% training sets as validation sets for hyperparameter tuning and the remaining 10% as test set for the PdM policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13rqadVW6Ptv"
   },
   "source": [
    "I separate into training and validation and test set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7siRw6Ptx"
   },
   "source": [
    "Perform the min max scaling on the training data and validation dataset\n",
    "use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYkFz5FqQplh"
   },
   "source": [
    ">`Create a copy of the cycle column`: This line creates a new column named 'cycle_norm' in the train_df DataFrame and initializes it with the values from the original 'cycle' column. This column will be normalized later.<br>\n",
    "> `Select columns for normalization`: This line selects all columns from **train_df** except 'id', 'cycle', 'RUL', 'label1', and 'label2'. These columns are the ones that will undergo normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AHbVv4RgN-"
   },
   "source": [
    "> `Initialize MinMaxScaler`: This line initializes a MinMaxScaler object from the scikit-learn preprocessing module. This scaler will be used to perform Min-Max normalization.<br>\n",
    "> `Perform Min-Max normalization`: This line applies Min-Max normalization to the selected columns (`cols_normalize`) of the `train_df` DataFrame.<br>\n",
    "> `min_max_scaler.fit_transform(train_df[cols_normalize])` computes the Min-Max normalization for the selected columns.<br>\n",
    "> The resulting normalized values are stored in a new DataFrame called `norm_train_df`, with the same index as `train_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEf7s4DS3jR"
   },
   "source": [
    "> `Join normalized DataFrame with the original DataFrame`: This line joins the normalized DataFrame (`norm_train_df`) with the original DataFrame (`train_df`) excluding the columns that were normalized.<br>\n",
    "> The resulting DataFrame `join_df` contains both the normalized columns and the original columns that were not normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF2-ITehT1_P"
   },
   "source": [
    "`Reorder columns`:\n",
    "> * This line reorders the columns of `join_df` to match the original order of columns in `train_df`.\n",
    "> * The reordered DataFrame is then assigned back to `train_df`, effectively replacing the original DataFrame with the normalized version.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## Vanilla Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `sequence_cols.extend(sensor_cols)`, it adds each element of `sensor_cols` to the end of `sequence_cols`.<br>\n",
    "After this operation, `sequence_cols` will contain 25 elements: 4 operational settings followed by 21 sensor readings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEV0vj7AkJOl"
   },
   "source": [
    "## generate sequences for each engine\n",
    "> * This creates a generator expression that iterates over unique engine IDs in the training data.<br>\n",
    "> * For each engine, it generates sequences using the `gen_sequence` function defined earlier.<br>\n",
    "> * Each sequence is a list of sensor data, and multiple sequences are generated for each engine.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Fc99j1rPQa"
   },
   "source": [
    "> * This concatenates all the generated sequences into a single numpy array.\n",
    "> * It converts the array to `float32` data type.\n",
    "> * The resulting `seq_array` contains the sequences of sensor data, with shape `(num_sequences, sequence_length, num_features)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782949253,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "An8AqEX_6Pty"
   },
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu8ocOhxkaf"
   },
   "source": [
    "`Function Signature:` This function efficiently generates labels for each sequence of sensor data. It ensures that the labels are correctly aligned with the sequences and handles the special case where the first sequence uses the last label as its target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This function takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (id).<br>\n",
    ">> * `seq_length`: Length of the sequence window.<br>\n",
    ">> * `label`: List of column names representing the labels.\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[label].values:`\n",
    ">> * This line extracts the columns specified by label from the DataFrame id_df and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant label(s) needed for generating sequences.<br>\n",
    "\n",
    "`Label Generation:`\n",
    "> * `num_elements:`This line calculates the number of rows (elements) in the data matrix, which corresponds to the number of labels.<br>\n",
    "> * `return data_matrix[seq_length:num_elements, :]:`\n",
    ">> * This line returns the labels associated with each sequence.<br>\n",
    ">> * It removes the first `seq_length` labels because, for each engine (`id`), the first sequence of size `seq_length` uses the last label as its target. The previous labels are discarded.<br>\n",
    ">> * All subsequent sequences for the same engine (`id`) will have one label associated with them step by step.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When modeling multi-class classification problems using neural networks,<br>\n",
    "it is good practice to reshape the output attribute from a vector that contains values for each class value to be<br>\n",
    "a matrix with a boolean for each class value and whether or not a given instance has that class value or not.<br>\n",
    "This is called one hot encoding or creating dummy variables from a categorical variable.<br>\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qBrRZdYZiHb"
   },
   "source": [
    "`to_categorical` is a utility function in Keras that converts class vectors (integers) to binary class matrices.<br>\n",
    "`dummy_label_array = to_categorical(label_array):`This line applies one-hot encoding to the `label_array`.<br>\n",
    "`label_array` contains the labels associated with each sequence, where each label represents a class or category.<br>\n",
    "> * One-hot encoding converts these integer labels into binary vectors, where each vector has a length equal to the number of classes and contains a 1 in the position corresponding to the class and 0s elsewhere.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782965252,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "XnwyiGj06Pt0",
    "outputId": "b319666d-4293-4940-843e-a3bce535053c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1718782966954,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "qbAJg5KqY-In",
    "outputId": "22d56de8-00e6-4671-b0cd-d47c0e24276c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1742, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782969359,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "M40CNdn7krRw",
    "outputId": "9df2380d-c7b1-43df-cb41-ec51fb81c98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782971301,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "dWhWIVl56Pt1",
    "outputId": "a9a57d55-f589-4306-954f-0d36aed732c2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "nb_features, nb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0h3i_FJg7pJ"
   },
   "source": [
    "`Extracting Feature and Output Dimensions:`\n",
    "> `nb_features:`Determines the number of features in the input sequence data.<br>\n",
    "> `nb_out:`Determines the number of output classes. It's extracted from the shape of the label array.<br>\n",
    "\n",
    "`Defining the Model Architecture:` describe in the code below.\n",
    "`Compiling the Model:` `model.compile(...)` Here, `categorical_crossentropy` is used as the loss function for multi-class classification.\n",
    "\n",
    "`Model Summary:`Prints a summary of the model architecture, including the layers and their parameters.\n",
    "\n",
    "`Training the Model:` `model.fit(...):` Trains the model on the training data. It specifies the input data (`seq_array`) and the corresponding labels (`dummy_label_array`). Other parameters include the number of epochs, batch size, validation split, verbosity, and callbacks.<br>\n",
    "\n",
    "\n",
    "`history.history.keys():` After training, this prints the keys of the history object, which contains information about training and validation metrics over each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOmuAimTDi5p"
   },
   "source": [
    "### Define the Dataset:\n",
    "Create a custom dataset class to handle your multivariate time series data with labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1718791179567,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "C5EsCykBKvsT",
    "outputId": "ebb36239-3bab-4478-9e21-ea72b12ac2ba"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#os.chdir('/content/drive/MyDrive/Colab Notebooks/LSTM_Antonis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6baBqY-Oli3"
   },
   "source": [
    "`Shuffling Batches:` By setting `shuffle=True` in the `DataLoader` for the training set, you ensure that the order of batches is shuffled each epoch. This maintains the temporal structure within each batch while still introducing variability in the order in which batches are processed.<br>\n",
    "\n",
    "`DataLoader for Validation:` Ensure `shuffle=False` for the validation set to maintain the sequence order during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1718791182861,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "8SbqCLucDib9"
   },
   "outputs": [],
   "source": [
    "# Import custom classes\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from dataset import MultivariateTimeSeriesDataset\n",
    "from model import TransformerTimeSeriesModel\n",
    "\n",
    "# Load initial parameters from params.json\n",
    "with open('params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Load hyperparameter search space from hyperparameters.json\n",
    "with open('hyperparameters.json', 'r') as f:\n",
    "    hp_config = json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage with seq_array and dummy_var\n",
    "#seq_array --> (12138, 50, 25)  # use this as your actual data\n",
    "#dummy_var --->  (12138, 3) # use this as your actual dummy variable (3 classes)\n",
    "\n",
    "#train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "#val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "# Shuffle batches by setting shuffle=True in DataLoader\n",
    "#train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.environ[\\'WANDB_API_KEY\\'] = \\'f05e720396a77affdb7e82b525ef1a912da7aaf4\\'\\n\\ntry:\\n    wandb.login()\\n    print(f\"Successfully logged in as: {wandb.api.viewer()[\\'entity\\']}\")\\nexcept wandb.errors.AuthenticationError:\\n    print(\"Failed to authenticate. Please check your API key.\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "os.environ['WANDB_API_KEY'] = 'f05e720396a77affdb7e82b525ef1a912da7aaf4'\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(f\"Successfully logged in as: {wandb.api.viewer()['entity']}\")\n",
    "except wandb.errors.AuthenticationError:\n",
    "    print(\"Failed to authenticate. Please check your API key.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_entity = wandb.api.viewer()[\\'entity\\']\\nprint(f\"Currently logged in as: {current_entity}\")\\nif current_entity != \"jitendratiwari11\":\\n    raise ValueError(f\"Logged in as {current_entity}, but expected to be logged in as jitendra\")\\n\\nprint(wandb.api.viewer()[\\'entity\\'])\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "current_entity = wandb.api.viewer()['entity']\n",
    "print(f\"Currently logged in as: {current_entity}\")\n",
    "if current_entity != \"jitendratiwari11\":\n",
    "    raise ValueError(f\"Logged in as {current_entity}, but expected to be logged in as jitendra\")\n",
    "\n",
    "print(wandb.api.viewer()['entity'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wandb.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQmBGIESY4qE"
   },
   "source": [
    "### Define the Transformer Model:\n",
    "Implement the vanilla Transformer architecture, ensuring it takes the dummy variable as an input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV8tK1SNV2e-"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Define the training loop with the loss function and optimizer, and include the dummy variable in the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_and_evaluate(config, num_epochs,use_early_stopping=True):\\n    model = TransformerTimeSeriesModel(\\n        input_dim=params[\\'input_dim\\'],\\n        seq_length=params[\\'seq_length\\'],\\n        num_classes=params[\\'num_classes\\'],\\n        model_dim=config[\\'model_dim\\'],\\n        num_heads=config[\\'num_heads\\'],\\n        num_layers=config[\\'num_layers\\'],\\n        dropout_rate=config[\\'dropout_rate\\']\\n    )\\n\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    model.to(device)\\n    \\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.AdamW(model.parameters(), lr=config[\\'learning_rate\\'], weight_decay=config[\\'weight_decay\\'])\\n    scaler = GradScaler()\\n\\n    # Set up datasets\\n    train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params[\\'seq_length\\'])\\n    val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params[\\'seq_length\\'])\\n\\n    # Set up data loaders\\n    train_loader = DataLoader(train_dataset, batch_size=config[\\'batch_size\\'], shuffle=True)\\n    val_loader = DataLoader(val_dataset, batch_size=config[\\'batch_size\\'], shuffle=False)\\n    \\n    train_losses = []\\n    val_losses = []\\n    val_accuracies = []\\n\\n    best_val_loss = float(\\'inf\\')\\n    best_val_accuracy = 0\\n    best_accuracy = 0\\n    best_model = None\\n    epochs_without_improvement = 0\\n\\n    for epoch in range(num_epochs):\\n        model.train()\\n        total_train_loss = 0.0\\n        train_correct = 0\\n        train_total = 0\\n        \\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\\n        for i, batch in enumerate(train_pbar):\\n            inputs, labels = batch\\n            inputs, labels = inputs.float().to(device), labels.float().to(device)\\n            \\n            with autocast():\\n                outputs_class = model(inputs)\\n                outputs_reshaped = outputs_class.view(-1, model.num_classes)\\n                labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\\n                loss = criterion(outputs_reshaped, labels_reshaped) / config[\\'gradient_accumulation_steps\\']\\n\\n            scaler.scale(loss).backward()\\n            \\n            if (i + 1) % config[\\'gradient_accumulation_steps\\'] == 0:\\n                scaler.step(optimizer)\\n                scaler.update()\\n                optimizer.zero_grad()\\n            \\n            total_train_loss += loss.item() * config[\\'gradient_accumulation_steps\\']\\n            \\n            \\n            current_train_loss = total_train_loss / (i + 1)\\n            train_pbar.set_postfix({\\'train loss\\': current_train_loss})\\n            \\n            if i % 100 == 0:\\n                torch.cuda.empty_cache()\\n                gc.collect()\\n\\n        avg_train_loss = total_train_loss / len(train_loader)\\n        train_losses.append(avg_train_loss)\\n\\n        \\n        #Validation loop\\n        model.eval()\\n        total_val_loss = 0.0\\n        all_preds = []\\n        all_labels = []\\n\\n        \\n        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\\n        with torch.no_grad():\\n            for batch in val_pbar:\\n                inputs, labels = batch\\n                inputs, labels = inputs.float().to(device), labels.float().to(device)\\n                \\n                with autocast():\\n                    outputs_class = model(inputs)\\n                    outputs_reshaped = outputs_class.view(-1, model.num_classes)\\n                    labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\\n                    val_loss = criterion(outputs_reshaped, labels_reshaped)\\n                \\n                total_val_loss += val_loss.item()\\n\\n                _, preds = torch.max(outputs_class, dim=2)\\n                all_preds.extend(preds.cpu().numpy().reshape(-1))\\n                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\\n\\n                current_val_loss = total_val_loss / (val_pbar.n + 1)\\n                val_pbar.set_postfix({\\'val loss\\': current_val_loss})\\n\\n        avg_val_loss = total_val_loss / len(val_loader)\\n        val_losses.append(avg_val_loss)\\n\\n        accuracy = accuracy_score(all_labels, all_preds)\\n        val_accuracies.append(accuracy)\\n        \\n        print(f\"Epoch {epoch+1}/{num_epochs}, \"\\n              f\"Train Loss: {avg_train_loss:.4f}, \"\\n              f\"Val Loss: {avg_val_loss:.4f}, \"\\n              f\"Val Accuracy: {accuracy:.2f}%\")\\n        \\n        wandb.log({\\n            \"epoch\": epoch,\\n            \"train_loss\": avg_train_loss,\\n            \"val_loss\": avg_val_loss,\\n            \"val_accuracy\": accuracy\\n        })\\n        if use_early_stopping:\\n            # Early stopping check\\n            if accuracy > best_val_accuracy:\\n                best_val_accuracy = accuracy\\n                best_val_loss = avg_val_loss\\n                best_model = model.state_dict()\\n                epochs_without_improvement = 0\\n            else:\\n                epochs_without_improvement += 1\\n\\n            if epochs_without_improvement >= params[\\'patience\\']:\\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\\n                break\\n        else:\\n            # If not using early stopping, always update best model\\n            best_val_accuracy = accuracy\\n            best_val_loss = avg_val_loss\\n            best_model = model.state_dict()\\n            \\n\\n        \\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n    #Load the best model before returning\\n    model.load_state_dict(best_model)\\n\\n    return model, train_losses, val_losses, val_accuracies\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def train_and_evaluate(config, num_epochs,use_early_stopping=True):\n",
    "    model = TransformerTimeSeriesModel(\n",
    "        input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=config['model_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout_rate=config['dropout_rate']\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Set up datasets\n",
    "    train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "    val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "    # Set up data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for i, batch in enumerate(train_pbar):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs_class = model(inputs)\n",
    "                outputs_reshaped = outputs_class.view(-1, model.num_classes)\n",
    "                labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped) / config['gradient_accumulation_steps']\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_train_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "            \n",
    "            \n",
    "            current_train_loss = total_train_loss / (i + 1)\n",
    "            train_pbar.set_postfix({'train loss': current_train_loss})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        \n",
    "        #Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs_class = model(inputs)\n",
    "                    outputs_reshaped = outputs_class.view(-1, model.num_classes)\n",
    "                    labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\n",
    "                    val_loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs_class, dim=2)\n",
    "                all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "\n",
    "                current_val_loss = total_val_loss / (val_pbar.n + 1)\n",
    "                val_pbar.set_postfix({'val loss': current_val_loss})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": accuracy\n",
    "        })\n",
    "        if use_early_stopping:\n",
    "            # Early stopping check\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= params['patience']:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            # If not using early stopping, always update best model\n",
    "            best_val_accuracy = accuracy\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    #Load the best model before returning\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef objective():\\n    global trial_counter\\n    with wandb.init() as run:\\n        config = wandb.config\\n        best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(config, num_epochs=config.num_epochs)\\n        \\n        best_val_accuracy = max(val_accuracies)\\n        wandb.log({\"best_val_accuracy\": best_val_accuracy})\\n        \\n        trial_counter = run.step + 1\\n        print(f\"Trial {trial_counter}/{hp_config[\\'num_trials\\']}, \"\\n              f\"Best Val Accuracy: {best_val_accuracy:.2f}\")\\n\\n        \\n        # Save the best model\\n        torch.save(best_model.state_dict(), \\'best_model_3_class.pth\\')\\n        wandb.save(\\'best_model_3_class.pth\\')\\n        \\n        return best_val_accuracy\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def objective():\n",
    "    global trial_counter\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(config, num_epochs=config.num_epochs)\n",
    "        \n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        \n",
    "        trial_counter = run.step + 1\n",
    "        print(f\"Trial {trial_counter}/{hp_config['num_trials']}, \"\n",
    "              f\"Best Val Accuracy: {best_val_accuracy:.2f}\")\n",
    "\n",
    "        \n",
    "        # Save the best model\n",
    "        torch.save(best_model.state_dict(), 'best_model_3_class.pth')\n",
    "        wandb.save('best_model_3_class.pth')\n",
    "        \n",
    "        return best_val_accuracy\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the hyperparameter search space\\nsweep_config = {\\n    \\'method\\': hp_config[\\'method\\'],\\n    \\'metric\\': {\\'name\\': hp_config[\\'metric_name\\'], \\'goal\\': hp_config[\\'metric_goal\\']},\\n    \\'parameters\\': {\\n        \\'model_dim\\': {\\'values\\': hp_config[\\'model_dim\\']},\\n        \\'num_heads\\': {\\'values\\': hp_config[\\'num_heads\\']},\\n        \\'num_layers\\': {\\'values\\': hp_config[\\'num_layers\\']},\\n        \\'dropout_rate\\': {\\'distribution\\': \\'uniform\\', \\'min\\': hp_config[\\'dropout_rate\\'][\\'min\\'], \\'max\\': hp_config[\\'dropout_rate\\'][\\'max\\']},\\n        \\'learning_rate\\': {\\'distribution\\': \\'log_uniform_values\\', \\'min\\': hp_config[\\'learning_rate\\'][\\'min\\'], \\'max\\': hp_config[\\'learning_rate\\'][\\'max\\']},\\n        \\'batch_size\\': {\\'values\\': hp_config[\\'batch_size\\']},\\n        \\'num_epochs\\': {\\'values\\': hp_config[\\'num_epochs\\']},\\n        \\'weight_decay\\': {\\'distribution\\': \\'log_uniform_values\\', \\'min\\': hp_config[\\'weight_decay\\'][\\'min\\'], \\'max\\': hp_config[\\'weight_decay\\'][\\'max\\']},\\n        \\'gradient_accumulation_steps\\': {\\'values\\': hp_config[\\'gradient_accumulation_steps\\']}\\n    }\\n}\\n\\n\\n#sweep_id = wandb.sweep(sweep_config, project=\"transformer_time_series\", entity=\"jitendratiwari11\")\\nsweep_id= \\'c3hgzsib\\'\\n# Start the sweep\\n#wandb.agent(sweep_id, function=objective, count=hp_config[\\'num_trials\\'])\\nwandb.agent(sweep_id, function=objective, count=hp_config[\\'num_trials\\'], project=\"transformer_time_series\", entity=\"jitendratiwari11\")\\n\\n# After all trials, find the best run\\napi = wandb.Api()\\nruns = api.runs(\"jitendratiwari11/transformer_time_series\")\\nbest_run = max(runs, key=lambda run: run.summary.get(\\'best_val_accuracy\\', 0))\\n\\nprint(f\"Best run: {best_run.name}\")\\nprint(f\"Best validation accuracy: {best_run.summary[\\'best_val_accuracy\\']}\")\\nprint(\"Best parameters:\")\\nprint(json.dumps(best_run.config, indent=4))\\n\\n# Update params.json with best parameters\\nparams.update(best_run.config)\\nwith open(\\'params.json\\', \\'w\\') as f:\\n    json.dump(params, f, indent=4)\\n\\nprint(\"Best parameters saved to params.json\")\\n\\n# Run the best model for longer epochs\\nprint(\"Running best model for longer epochs...\")\\nwandb.init(project=\"transformer_time_series\", name=\"best_model_long_run\", config=params, reinit=True)\\n\\nbest_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=100)\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the hyperparameter search space\n",
    "sweep_config = {\n",
    "    'method': hp_config['method'],\n",
    "    'metric': {'name': hp_config['metric_name'], 'goal': hp_config['metric_goal']},\n",
    "    'parameters': {\n",
    "        'model_dim': {'values': hp_config['model_dim']},\n",
    "        'num_heads': {'values': hp_config['num_heads']},\n",
    "        'num_layers': {'values': hp_config['num_layers']},\n",
    "        'dropout_rate': {'distribution': 'uniform', 'min': hp_config['dropout_rate']['min'], 'max': hp_config['dropout_rate']['max']},\n",
    "        'learning_rate': {'distribution': 'log_uniform_values', 'min': hp_config['learning_rate']['min'], 'max': hp_config['learning_rate']['max']},\n",
    "        'batch_size': {'values': hp_config['batch_size']},\n",
    "        'num_epochs': {'values': hp_config['num_epochs']},\n",
    "        'weight_decay': {'distribution': 'log_uniform_values', 'min': hp_config['weight_decay']['min'], 'max': hp_config['weight_decay']['max']},\n",
    "        'gradient_accumulation_steps': {'values': hp_config['gradient_accumulation_steps']}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep_config, project=\"transformer_time_series\", entity=\"jitendratiwari11\")\n",
    "sweep_id= 'c3hgzsib'\n",
    "# Start the sweep\n",
    "#wandb.agent(sweep_id, function=objective, count=hp_config['num_trials'])\n",
    "wandb.agent(sweep_id, function=objective, count=hp_config['num_trials'], project=\"transformer_time_series\", entity=\"jitendratiwari11\")\n",
    "\n",
    "# After all trials, find the best run\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"jitendratiwari11/transformer_time_series\")\n",
    "best_run = max(runs, key=lambda run: run.summary.get('best_val_accuracy', 0))\n",
    "\n",
    "print(f\"Best run: {best_run.name}\")\n",
    "print(f\"Best validation accuracy: {best_run.summary['best_val_accuracy']}\")\n",
    "print(\"Best parameters:\")\n",
    "print(json.dumps(best_run.config, indent=4))\n",
    "\n",
    "# Update params.json with best parameters\n",
    "params.update(best_run.config)\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"Best parameters saved to params.json\")\n",
    "\n",
    "# Run the best model for longer epochs\n",
    "print(\"Running best model for longer epochs...\")\n",
    "wandb.init(project=\"transformer_time_series\", name=\"best_model_long_run\", config=params, reinit=True)\n",
    "\n",
    "best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=100)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Run the best model for longer epochs\\nprint(\"Running best model for longer epochs...\")\\nwandb.init(project=\"transformer_time_series\", name=\"experiment2\", config=params, reinit=True)\\n\\nbest_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=100, use_early_stopping=False)\\n\\n# Save the model after 100 epochs\\ntorch.save(best_model.state_dict(), \\'best_model_after_100_epochs.pth\\')\\nprint(\"Model after 100 epochs saved to best_model_after_100_epochs.pth\")\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Run the best model for longer epochs\n",
    "print(\"Running best model for longer epochs...\")\n",
    "wandb.init(project=\"transformer_time_series\", name=\"experiment2\", config=params, reinit=True)\n",
    "\n",
    "best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=100, use_early_stopping=False)\n",
    "\n",
    "# Save the model after 100 epochs\n",
    "torch.save(best_model.state_dict(), 'best_model_after_100_epochs.pth')\n",
    "print(\"Model after 100 epochs saved to best_model_after_100_epochs.pth\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Validation set created during preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Plot the losses and accuracy\\nplt.figure(figsize=(12, 4))\\nplt.subplot(1, 2, 1)\\nplt.plot(train_losses, label=\\'Train Loss\\')\\nplt.plot(val_losses, label=\\'Validation Loss\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Loss\\')\\nplt.legend()\\nplt.title(\\'Training and Validation Loss\\')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(val_accuracies, label=\\'Validation Accuracy\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Accuracy (%)\\')\\nplt.legend()\\nplt.title(\\'Validation Accuracy\\')\\n\\nplt.tight_layout()\\nplt.savefig(\\'learning_curves_100_epochs.png\\')\\nplt.close()\\n\\nwandb.log({\\n    \"final_train_loss\": train_losses[-1],\\n    \"final_val_loss\": val_losses[-1],\\n    \"final_val_accuracy\": val_accuracies[-1],\\n    \"training_curves\": wandb.Image(\\'training_curves.png\\')\\n})\\n\\nprint(f\"Final train loss: {train_losses[-1]:.4f}\")\\nprint(f\"Final validation loss: {val_losses[-1]:.4f}\")\\nprint(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\\n\\nwandb.finish()\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plot the losses and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves_100_epochs.png')\n",
    "plt.close()\n",
    "\n",
    "wandb.log({\n",
    "    \"final_train_loss\": train_losses[-1],\n",
    "    \"final_val_loss\": val_losses[-1],\n",
    "    \"final_val_accuracy\": val_accuracies[-1],\n",
    "    \"training_curves\": wandb.Image('training_curves.png')\n",
    "})\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "wandb.finish()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\" total nr of parameters as per best model: {sum(p.numel() for p in best_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Confusion matricx and Classification report\n",
    "\n",
    "For each test set, I need to give the on-line sensor data as input to the trained Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "k_GqqVKV6Pt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best_model_after_100_epochs.pth\n",
      " total nr of parameters as per best model: 32051\n",
      "Confusion Matrix:\n",
      "[[67150  4900     0]\n",
      " [    0  6849  3151]\n",
      " [    0     0  5500]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     72050\n",
      "           1       0.58      0.68      0.63     10000\n",
      "           2       0.64      1.00      0.78      5500\n",
      "\n",
      "    accuracy                           0.91     87550\n",
      "   macro avg       0.74      0.87      0.79     87550\n",
      "weighted avg       0.93      0.91      0.91     87550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import TransformerTimeSeriesModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# define path to save model\n",
    "#model_path = 'best_model_3_class.pth'# This file then contains the already trained network, so that you don't have to retrain every time\n",
    "model_path = 'best_model_after_100_epochs.pth'# This file then contains the already trained network, so that you don't have to retrain every time\n",
    "\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = TransformerTimeSeriesModel(\n",
    "    input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=params['model_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Load the state dict\n",
    "    estimator.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\" total nr of parameters as per best model: {sum(p.numel() for p in estimator.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    \n",
    "    # Move the model to the appropriate device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    estimator = estimator.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    estimator.eval()\n",
    "\n",
    "    test_dataset = MultivariateTimeSeriesDataset(seq_array_test, dummy_label_array_test, params['seq_length'])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = estimator(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "            all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "\n",
    "            \n",
    "           # _, preds = torch.max(outputs, 1)\n",
    "            #all_preds.extend(preds.cpu().numpy())\n",
    "            #all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "            \n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Optionally, print a classification report\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "else:\n",
    "    print(f\"No saved model found at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vloF6HXQ6Pt4"
   },
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 1000\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PZ-vs1-x6Pt5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170\n",
      " 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350\n",
      " 360 370 380 390]\n"
     ]
    }
   ],
   "source": [
    "array_decisions = np.arange(0,400,DT) # decisions can only be made every DT = 10 cycles\n",
    "print(array_decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5KP5yjWw6Pt5"
   },
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PqIzk2b96Pt5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4493, 30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df['cycle_norm'] = pdm_df['cycle']\n",
    "pdm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16138, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df['cycle_norm'] = result_df['cycle']\n",
    "#result_df.head(193)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBklUcnv6Pt5"
   },
   "source": [
    "## PdM policy evaluation on a the whole (test data+ validation) set (ids 81 to 101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vfyabzdL6Pt_"
   },
   "outputs": [],
   "source": [
    "#costs_rep_array   = np.zeros(10) # An array to store costs related to replacements.\n",
    "\n",
    "#costs_delay_array = np.zeros(10) # An array to store costs related to delays.\n",
    "#costs_stock_array = np.zeros(10) # An array to store costs related to stock.\n",
    "\n",
    "#t_LC_array        = np.zeros(10) # An array to store lead time.\n",
    "#t_order_array     = np.zeros(10) # An array to store order time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsWT1ebGbOdz"
   },
   "source": [
    "> 1. Initializes a counter variable to 0.\n",
    "> 2. Iterates over unique IDs in the `test_df` DataFrame.\n",
    "> 3. For each ID:\n",
    ">> * Sets flags for preventive replacement and ordering to False.<br>\n",
    ">> * Iterates over cycles within the range of the DataFrame.<br>\n",
    ">> * Checks if the current cycle is in the `array_decisions`.<br>\n",
    ">> * If it is, preprocesses the validation data for the Transformer model.<br>\n",
    ">> * Predicts the probability of RUL being smaller than w1 and DT (decision time) using the trained model.<br>\n",
    ">> * Evaluates decision heuristics:\n",
    ">>> * If no order has been placed yet and the cost of preventive replacement is less than or equal to the cost of waiting until `w1`, orders the component and sets the order time.<br>\n",
    ">>> * If the cost of preventive replacement is less than or equal to the cost of waiting until `DT`, performs preventive replacement, calculates related costs, and breaks the loop.<br>\n",
    ">> If preventive replacement is not performed:\n",
    ">>> * Sets the component failure time to the last cycle in the ID's data.<br>\n",
    ">>> * Sets replacement costs to `C_c`.<br>\n",
    ">>> * Calculates delay costs based on whether an order has been placed.<br>\n",
    ">> * Prints diagnostic information for each iteration.\n",
    ">> * Increments the counter.\n",
    "\n",
    "\n",
    "This code essentially simulates a decision-making process for component maintenance based on predictive models and cost considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jG8Lip5w6Pt_",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn import preprocessing\\n\\ncounter = 0\\nfor id in test_df['id'].unique():\\n    print('ID:', id)\\n    preventive_replacement = False\\n    order                  = False\\n\\n    for cycle in range(test_df[test_df['id']==id].shape[0]-params['seq_length']+1):\\n\\n        if cycle in array_decisions:\\n            min_max_scaler = preprocessing.MinMaxScaler()\\n            norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[test_df['id']==id][cols_normalize_train][:params['seq_length']+cycle]),\\n                 columns=cols_normalize_train,\\n                 index=test_df[test_df['id']==id][:params['seq_length']+cycle].index)\\n\\n            join_df = test_df[test_df['id']==id][:params['seq_length']+cycle][test_df[test_df['id']==id][:params['seq_length']+cycle].columns.difference(cols_normalize_train)].join(norm_test_df)\\n            test_df_eval_online = join_df.reindex(columns = test_df[test_df['id']==id][cycle:params['seq_length']+cycle].columns)\\n\\n            seq_array_test_k = test_df_eval_online[sequence_cols].values[cycle:params['seq_length']+cycle]\\n            seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1,params['seq_length'], nb_features)\\n            #prob_RUL_smaller_DT    = estimator.predict(seq_array_test_k).reshape(3)[2]\\n            #prob_RUL_smaller_w1    = estimator.predict(seq_array_test_k).reshape(3)[1]\\n            \\n            prob_RUL_smaller_DT    = estimator(seq_array_test_k).reshape(3)[2]\\n            prob_RUL_smaller_w1    = estimator(seq_array_test_k).reshape(3)[1]\\n\\n            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\\n            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n\\n\\n            # evaluate decision heuristics\\n            if order == False:\\n                if C_p <= prob_RUL_smaller_w1*C_c:\\n                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\\n                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n                    t_order_array[counter] = params['seq_length']+cycle\\n                    order = True\\n                    print('component ordering at cycle:', t_order_array[counter])\\n\\n            if C_p <= prob_RUL_smaller_DT*C_c:\\n                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n\\n                t_LC_array[counter] = params['seq_length']+cycle\\n                costs_rep_array[counter] = C_p\\n                print('preventive replacement informed at cycle:', t_LC_array[counter])\\n                # print('component lifecycle:', t_LC)\\n                preventive_replacement = True\\n                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\\n\\n                costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\\n                # print('delay time', max(t_order+L-t_LC, 0))\\n                # print('cost_delay_id:',cost_delay_id)\\n                # print('cost of stock:', cost_stock_id)\\n                break\\n\\n    if preventive_replacement == False:\\n        t_LC_array[counter] = test_df[test_df['id']==id]['cycle'].iloc[-1]\\n        print('Component failure at t:', t_LC_array[counter])\\n        costs_rep_array[counter] = C_c\\n\\n        if order == False:\\n            costs_delay_array[counter] = L * C_unav\\n        else:\\n            costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\\n            costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\\n\\n    print('True failure:', test_df[test_df['id']==id]['cycle'].iloc[-1])\\n    print('-----------------------------------------')\\n    counter+=1\\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "\n",
    "counter = 0\n",
    "for id in test_df['id'].unique():\n",
    "    print('ID:', id)\n",
    "    preventive_replacement = False\n",
    "    order                  = False\n",
    "\n",
    "    for cycle in range(test_df[test_df['id']==id].shape[0]-params['seq_length']+1):\n",
    "\n",
    "        if cycle in array_decisions:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[test_df['id']==id][cols_normalize_train][:params['seq_length']+cycle]),\n",
    "                 columns=cols_normalize_train,\n",
    "                 index=test_df[test_df['id']==id][:params['seq_length']+cycle].index)\n",
    "\n",
    "            join_df = test_df[test_df['id']==id][:params['seq_length']+cycle][test_df[test_df['id']==id][:params['seq_length']+cycle].columns.difference(cols_normalize_train)].join(norm_test_df)\n",
    "            test_df_eval_online = join_df.reindex(columns = test_df[test_df['id']==id][cycle:params['seq_length']+cycle].columns)\n",
    "\n",
    "            seq_array_test_k = test_df_eval_online[sequence_cols].values[cycle:params['seq_length']+cycle]\n",
    "            seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1,params['seq_length'], nb_features)\n",
    "            #prob_RUL_smaller_DT    = estimator.predict(seq_array_test_k).reshape(3)[2]\n",
    "            #prob_RUL_smaller_w1    = estimator.predict(seq_array_test_k).reshape(3)[1]\n",
    "            \n",
    "            prob_RUL_smaller_DT    = estimator(seq_array_test_k).reshape(3)[2]\n",
    "            prob_RUL_smaller_w1    = estimator(seq_array_test_k).reshape(3)[1]\n",
    "\n",
    "            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "\n",
    "            # evaluate decision heuristics\n",
    "            if order == False:\n",
    "                if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                    t_order_array[counter] = params['seq_length']+cycle\n",
    "                    order = True\n",
    "                    print('component ordering at cycle:', t_order_array[counter])\n",
    "\n",
    "            if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                t_LC_array[counter] = params['seq_length']+cycle\n",
    "                costs_rep_array[counter] = C_p\n",
    "                print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                # print('component lifecycle:', t_LC)\n",
    "                preventive_replacement = True\n",
    "                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "\n",
    "                costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "                # print('delay time', max(t_order+L-t_LC, 0))\n",
    "                # print('cost_delay_id:',cost_delay_id)\n",
    "                # print('cost of stock:', cost_stock_id)\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        t_LC_array[counter] = test_df[test_df['id']==id]['cycle'].iloc[-1]\n",
    "        print('Component failure at t:', t_LC_array[counter])\n",
    "        costs_rep_array[counter] = C_c\n",
    "\n",
    "        if order == False:\n",
    "            costs_delay_array[counter] = L * C_unav\n",
    "        else:\n",
    "            costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "            costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "\n",
    "    print('True failure:', test_df[test_df['id']==id]['cycle'].iloc[-1])\n",
    "    print('-----------------------------------------')\n",
    "    counter+=1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDM Policy1\n",
    "\n",
    "1. `Policy Overview:` We first consider the simple dynamic PdM decision setting, in which one determines at each time step\n",
    "    tk whether a component should be preventively replaced or not. The assumption here is that the new\n",
    "    component is readily available when a preventive replacement is decided or a corrective replacement is\n",
    "    imposed. <br>\n",
    "2. `Decision Making Process:`\n",
    "> At each time step tk = k * ΔT, the policy decides whether to take action or not.<br>\n",
    "> The action arep,k can be either:\n",
    ">> a) DN (Do Nothing) <br>\n",
    ">> b) PR (Preventive Replacement)<br>\n",
    "\n",
    "3. `Decision Rule:`\n",
    "> If Pr(RULpred,k ≤ ΔT) < pthres, then Do Nothing (DN) <br>\n",
    "> Otherwise, perform Preventive Replacement (PR) <br>\n",
    "where:\n",
    "> RULpred,k is the predicted Remaining Useful Life at time tk <br>\n",
    "> ΔT is the time step <br>\n",
    "> pthres is a variable heuristic threshold <br>\n",
    "4. `Threshold Determination:`\n",
    "> Initially, pthres is set to cp/cc <br>\n",
    "> cp is the cost of preventive replacement <br>\n",
    "> cc is the cost of component failure <br>\n",
    "5. `Cost Considerations:`\n",
    "> PR action costs cp <br>\n",
    "> DN action risks a potential cost of Pr(RULpred,k ≤ ΔT) * cc <br>\n",
    "6. `Rationale:`\n",
    "> PR is performed only when its cost is less than the predicted risk of failure in the next time step. <br>\n",
    "7. `Input Requirements:`\n",
    ">  `current_cycle:` variable represents the current time step or cycle within the sequence of data for a specific engine or component. It is used to iterate through the sequence of cycles for each unique engine ID in the dataset. <br>\n",
    ">  The policy needs Pr(RULpred,k ≤ ΔT) from the prognostic algorithm. <br>\n",
    "8. `Outcome:`\n",
    "> The policy informs replacement decisions for each component. <br>\n",
    "> It determines C_rep(i) (replacement cost) and Tlc(i) (lifecycle time) for each component i. <br>\n",
    "\n",
    ">> 1. `t_LC_array(Lifecycle Time) :`\n",
    ">>> * This array represents `Tlc(i) = min[T_R(i), T_F(i)]` for each component. <br>\n",
    ">>> * In the code, it's set in two cases:\n",
    ">>> a) When a preventive replacement is decided: `t_LC_array[counter] = params['seq_length'] + current_cycle`\n",
    ">>> b) When no preventive replacement occurred (implying failure): `t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]`\n",
    ">>> * This aligns with the definition of `Tlc(i)` being the minimum of preventive replacement time or failure time.\n",
    ">> 2. `costs_rep_array (Replacement Cost):`\n",
    ">>> * This array represents C_rep(i) for each component.\n",
    ">>> * In the code, it's set as follows:\n",
    ">>> a) For preventive replacement: `costs_rep_array[counter] = C_p` <br>\n",
    ">>> b) For corrective replacement (when no preventive replacement occurred): `costs_rep_array[counter] = C_c` <br>\n",
    ">>> This directly implements the condition as mentioned in equation 8: `C_rep(i) = (cp, if T_R(i) < T_F(i), cc, else.`\n",
    "\n",
    "\n",
    "9. `t_order_array:`\n",
    "> * Meaning: This array stores the cycle times at which components are ordered. <br>\n",
    "> * Significance: It helps track when preventive maintenance actions are initiated, allowing for analysis of the timing of maintenance decisions. <br>\n",
    "\n",
    "10. `t_LC_array:`\n",
    "> Meaning: This array likely stores the lifecycle times for each component. <br>\n",
    "> Significance: It represents either the time of preventive replacement or the time of failure for each component, which is crucial for evaluating the effectiveness of the maintenance policy. <br>\n",
    "\n",
    "11. `costs_rep_array:`\n",
    "> * Meaning: This array stores the replacement costs for each component. <br>\n",
    "> * Significance: It captures the financial impact of replacements, whether they are preventive (C_p) or corrective (C_c). This is essential for assessing the cost-effectiveness of the maintenance strategy. <br>\n",
    "\n",
    "12. `costs_delay_array:`\n",
    "> * Meaning: This array stores the costs associated with delays in component replacement. <br>\n",
    "> * Significance: It represents the financial penalties incurred when a component fails before a replacement arrives, helping to quantify the impact of maintenance timing on overall costs. <br>\n",
    "\n",
    "13. `costs_stock_array:`\n",
    "> Meaning: This array stores the costs related to holding replacement components in stock. <br>\n",
    "> Significance: It captures the inventory holding costs when components are ordered too early, balancing the trade-off between early ordering to prevent failures and the costs of storing components. <bR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>81</td>\n",
       "      <td>132</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.54</td>\n",
       "      <td>1595.33</td>\n",
       "      <td>1413.15</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.82</td>\n",
       "      <td>23.2471</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>81</td>\n",
       "      <td>133</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1592.20</td>\n",
       "      <td>1406.65</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.89</td>\n",
       "      <td>23.3028</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>81</td>\n",
       "      <td>134</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.62</td>\n",
       "      <td>1589.66</td>\n",
       "      <td>1405.98</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.03</td>\n",
       "      <td>23.2205</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>81</td>\n",
       "      <td>135</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.60</td>\n",
       "      <td>1590.30</td>\n",
       "      <td>1407.55</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.92</td>\n",
       "      <td>23.3995</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>81</td>\n",
       "      <td>136</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.73</td>\n",
       "      <td>1589.50</td>\n",
       "      <td>1415.36</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.74</td>\n",
       "      <td>23.2573</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0    81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91   \n",
       "1    81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25   \n",
       "2    81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42   \n",
       "3    81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89   \n",
       "4    81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49   \n",
       "..   ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "131  81    132    0.0018    0.0002     100.0  518.67  642.54  1595.33   \n",
       "132  81    133    0.0024    0.0005     100.0  518.67  642.55  1592.20   \n",
       "133  81    134    0.0008   -0.0004     100.0  518.67  642.62  1589.66   \n",
       "134  81    135    0.0009    0.0002     100.0  518.67  642.60  1590.30   \n",
       "135  81    136    0.0018   -0.0001     100.0  518.67  642.73  1589.50   \n",
       "\n",
       "          s4     s5  ...   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0    1406.63  14.62  ...  0.03  391  2388  100.0  38.87  23.3365  239       0   \n",
       "1    1407.88  14.62  ...  0.03  392  2388  100.0  38.91  23.3452  238       0   \n",
       "2    1396.40  14.62  ...  0.03  394  2388  100.0  39.04  23.3610  237       0   \n",
       "3    1404.86  14.62  ...  0.03  392  2388  100.0  38.77  23.4206  236       0   \n",
       "4    1409.58  14.62  ...  0.03  392  2388  100.0  39.04  23.3311  235       0   \n",
       "..       ...    ...  ...   ...  ...   ...    ...    ...      ...  ...     ...   \n",
       "131  1413.15  14.62  ...  0.03  393  2388  100.0  38.82  23.2471  108       0   \n",
       "132  1406.65  14.62  ...  0.03  393  2388  100.0  38.89  23.3028  107       0   \n",
       "133  1405.98  14.62  ...  0.03  395  2388  100.0  39.03  23.2205  106       0   \n",
       "134  1407.55  14.62  ...  0.03  394  2388  100.0  38.92  23.3995  105       0   \n",
       "135  1415.36  14.62  ...  0.03  395  2388  100.0  38.74  23.2573  104       0   \n",
       "\n",
       "     label2  cycle_norm  \n",
       "0         0           1  \n",
       "1         0           2  \n",
       "2         0           3  \n",
       "3         0           4  \n",
       "4         0           5  \n",
       "..      ...         ...  \n",
       "131       0         132  \n",
       "132       0         133  \n",
       "133       0         134  \n",
       "134       0         135  \n",
       "135       0         136  \n",
       "\n",
       "[136 rows x 30 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df.head(136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4493, 30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Sequence Data Preparation: `<br>\n",
    " * When you prepare a sequence for prediction, you're using data from `current_cycle` to `current_cycle + params['seq_length']`. <br>\n",
    " * This means that the sequence includes data points from the `current_cycle` up to `params['seq_length']` cycles into the future. <br>\n",
    " \n",
    "\n",
    "2. `Model Prediction: ` <br>\n",
    " * When your model makes a prediction based on this sequence, it's essentially predicting the state at the end of the sequence.<br>\n",
    " * The prediction is about the condition at cycle `current_cycle + params['seq_length']`, not at `current_cycle`. <br>\n",
    " \n",
    "3. `Decision Point: `<br>\n",
    " * The decision to perform a preventive replacement (PR) is made based on the prediction for the end of the sequence.<br>\n",
    " * This decision point is effectively at `current_cycle + params['seq_length']`.<br>\n",
    "4. `Realistic Timing: `<br>\n",
    " * In a real-world scenario, you would make the decision to replace a component based on its predicted future state, not its current state.<br>\n",
    " * Setting `T_R_i = current_cycle + params['seq_length']` reflects this reality - you're deciding to replace at the future point for which you've made the prediction.\n",
    " \n",
    "5. `Consistency with Model Output: `<br>\n",
    " * Your model is trained to predict the state at the end of each input sequence.<br>\n",
    " * By setting `T_R_i` to the end of the sequence, you're aligning the replacement decision with what the model is actually predicting.<br>\n",
    " \n",
    "6. `Avoiding Premature Decisions: `<br>\n",
    " * If you set `T_R_i = current_cycle`, you might be making decisions based on incomplete information.<br>\n",
    " * The full sequence might show trends or patterns that only become clear by the end of the `params['seq_length']` cycles.<br>\n",
    "\n",
    "Example: <br>\n",
    "Let's say `current_cycle = 100` and `params['seq_length'] = 50`:\n",
    " * Your sequence data covers cycles 100 to 150.<br>\n",
    " * The model predicts the state at cycle 150.<br>\n",
    " * If the prediction indicates a need for replacement, it's logical to set `T_R_i = 150` (which is `current_cycle + params['seq_length']`), not 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM policy 1 without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef prepare_sequence_data(pdm_df, id, current_cycle, params, cols_normalize_train, sequence_cols):\\n    min_max_scaler = preprocessing.MinMaxScaler()\\n    norm_pdm_df = pd.DataFrame(min_max_scaler.fit_transform(pdm_df[pdm_df[\\'id\\'] == id][cols_normalize_train][:params[\\'seq_length\\'] + current_cycle]),\\n                                columns=cols_normalize_train,\\n                                index=pdm_df[pdm_df[\\'id\\'] == id][:params[\\'seq_length\\'] + current_cycle].index)\\n    #print(\"shape of norm_pdm_df: \", norm_pdm_df.shape)\\n\\n    join_df = pdm_df[pdm_df[\\'id\\'] == id][:params[\\'seq_length\\'] + current_cycle][pdm_df[pdm_df[\\'id\\'] == id][:params[\\'seq_length\\'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\\n    #print(\"shape of joint_df: \",join_df.shape)\\n    pdm_df_eval_online = join_df.reindex(columns=pdm_df[pdm_df[\\'id\\'] == id][current_cycle:params[\\'seq_length\\'] + current_cycle].columns)\\n    \\n    #print(\"shape of pdm_df_eval_online: \",pdm_df_eval_online.shape)\\n    seq_array_test_k = pdm_df_eval_online[sequence_cols].values[current_cycle:params[\\'seq_length\\'] + current_cycle]\\n    #print(\"shape of seq_array_test_k before reshaping: \",seq_array_test_k.shape)\\n    seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params[\\'seq_length\\'], len(sequence_cols))\\n    #print(\"shape of seq_array_test_k after reshaping: \",seq_array_test_k.shape)\\n    \\n    return seq_array_test_k\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def prepare_sequence_data(pdm_df, id, current_cycle, params, cols_normalize_train, sequence_cols):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    norm_pdm_df = pd.DataFrame(min_max_scaler.fit_transform(pdm_df[pdm_df['id'] == id][cols_normalize_train][:params['seq_length'] + current_cycle]),\n",
    "                                columns=cols_normalize_train,\n",
    "                                index=pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle].index)\n",
    "    #print(\"shape of norm_pdm_df: \", norm_pdm_df.shape)\n",
    "\n",
    "    join_df = pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle][pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "    #print(\"shape of joint_df: \",join_df.shape)\n",
    "    pdm_df_eval_online = join_df.reindex(columns=pdm_df[pdm_df['id'] == id][current_cycle:params['seq_length'] + current_cycle].columns)\n",
    "    \n",
    "    #print(\"shape of pdm_df_eval_online: \",pdm_df_eval_online.shape)\n",
    "    seq_array_test_k = pdm_df_eval_online[sequence_cols].values[current_cycle:params['seq_length'] + current_cycle]\n",
    "    #print(\"shape of seq_array_test_k before reshaping: \",seq_array_test_k.shape)\n",
    "    seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], len(sequence_cols))\n",
    "    #print(\"shape of seq_array_test_k after reshaping: \",seq_array_test_k.shape)\n",
    "    \n",
    "    return seq_array_test_k\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[cols_normalize_train])\n",
    "\n",
    "\n",
    "def prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols):\n",
    "    \n",
    "    # In prepare_sequence_data\n",
    "    norm_pdm_df = pd.DataFrame(scaler.transform(pdm_df[pdm_df['id'] == id][cols_normalize_train][:params['seq_length'] + current_cycle]),\n",
    "                               columns=cols_normalize_train,\n",
    "                               index=pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle].index)\n",
    "\n",
    "    join_df = pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle][pdm_df[pdm_df['id'] == id][:params['seq_length'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "    #print(\"shape of joint_df: \",join_df.shape)\n",
    "    pdm_df_eval_online = join_df.reindex(columns=pdm_df[pdm_df['id'] == id][current_cycle:params['seq_length'] + current_cycle].columns)\n",
    "    \n",
    "    #print(\"shape of pdm_df_eval_online: \",pdm_df_eval_online.shape)\n",
    "    seq_array_test_k = pdm_df_eval_online[sequence_cols].values[current_cycle:params['seq_length'] + current_cycle]\n",
    "    #print(\"shape of seq_array_test_k before reshaping: \",seq_array_test_k.shape)\n",
    "    seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], len(sequence_cols))\n",
    "    #print(\"shape of seq_array_test_k after reshaping: \",seq_array_test_k.shape)\n",
    "    \n",
    "    return seq_array_test_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "                                                           \n",
    "def calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler,params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT,C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        preventive_replacement = False\n",
    "        \n",
    "        \n",
    "        for current_cycle in range(pdm_df[pdm_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data                           \n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "                #print(\"probabilities of last sequence: \", probabilities[-1,:])\n",
    "                #print(\"shape of probabilities: \", probabilities.shape)\n",
    "                # Calculate probabilities\n",
    "                prob_RUL_smaller_w1 = (probabilities[-1, 1] + probabilities[-1, 2])\n",
    "                prob_RUL_smaller_DT = probabilities[-1, 2]\n",
    "                #prob_RUL_smaller_w1 = np.mean(probabilities[:, 1] + probabilities[:, 2])\n",
    "                #prob_RUL_smaller_DT = np.mean(probabilities[:, 2])\n",
    "\n",
    "\n",
    "                # Apply PdM policy 1\n",
    "                pthres = C_p / C_c  # Heuristic threshold\n",
    "\n",
    "                if prob_RUL_smaller_DT < pthres:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                else:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                if action == \"PR\":\n",
    "                    T_R_i =  current_cycle + params['seq_length']\n",
    "                    #T_R_i =  current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i) # length of life cycle of ith component\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            \n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "# Usage\n",
    "\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 200., 280., 260., 180., 270., 170., 200., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 190., 150., 180., 190.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,\n",
       "        100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,\n",
       "        100.,  100.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220., 180., 260., 240., 160., 250., 150., 180., 190., 130., 110.,\n",
       "       320., 130., 230., 260., 310., 170., 130., 160., 170.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM Policy 1 with Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "                   \n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler,params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(pdm_df[pdm_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "            #print(\"current_cycle: \", current_cycle)\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                \n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                #print(\"probabilities of last sequence: \", probabilities[-1,:])\n",
    "                #print(\"shape of probabilities: \", probabilities.shape)\n",
    "                # Calculate probabilities\n",
    "                prob_RUL_smaller_w1 = (probabilities[-1, 1] + probabilities[-1, 2])\n",
    "                prob_RUL_smaller_DT = probabilities[-1, 2]\n",
    "                #prob_RUL_smaller_w1 = np.mean(probabilities[:, 1] + probabilities[:, 2])\n",
    "                #prob_RUL_smaller_DT = np.mean(probabilities[:, 2])\n",
    "\n",
    "\n",
    "                # Apply PdM policy 1\n",
    "                p_order_thres = C_p / C_c  # Heuristic threshold for ordering\n",
    "                p_rep_thres = C_p / C_c  # Heuristic threshold for replacement\n",
    "\n",
    "                #w = (L // params['seq_length']) * params['seq_length']  # Adjusted lead time\n",
    "\n",
    "                # Ordering decision\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    \n",
    "                    t_order_array[counter] = params['seq_length'] + current_cycle + w\n",
    "                    #t_order_array[counter] =  current_cycle + w\n",
    "                    order = True\n",
    "                    #print(f\"Component ordered at cycle: {t_order_array[counter]}\")\n",
    "\n",
    "                # Apply PdM policy 1\n",
    "                \n",
    "                if prob_RUL_smaller_DT < p_rep_thres:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                else:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = params['seq_length'] + current_cycle\n",
    "                    #T_R_i =  current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "                                                                                               \n",
    "# Usage\n",
    "\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230., 210., 290., 260., 180., 270., 160., 200., 200., 150., 130.,\n",
       "       340., 140., 260., 280., 330., 200., 140., 180., 180.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 200., 280., 260., 180., 270., 170., 200., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 190., 150., 180., 190.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,\n",
       "        100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,  100.,\n",
       "        100.,  100.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220., 180., 260., 240., 160., 250., 150., 180., 190., 130., 110.,\n",
       "       320., 130., 230., 260., 310., 170., 130., 160., 170.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM policy 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Objective:` PDM policy 2 aims to find the optimal time for preventive replacement (T_{R,k}*) by minimizing the long-run expected maintenance cost per unit time. <br>\n",
    "2. `Decision Rule:` At each time step t_k, the policy decides:\n",
    "> * Perform Preventive Replacement (PR) if t_k + ΔT ≥ T_{R,k}*\n",
    "> * Do Nothing (DN) otherwise\n",
    "3. `Optimization Problem:` The policy solves an optimization problem at each time step to find T_{R,k}* by minimizing the objective function f(T_{R,k}) given in Equation 12. <br>\n",
    "4. `Components of the Objective Function (Equation 12):`\n",
    "> $f\\left(T_{\\mathrm{R}, k}\\right)=\\frac{\\mathrm{E}\\left[C_{\\mathrm{rep}}\\left(T_{\\mathrm{R}, k}\\right)\\right]}{\\mathrm{E}\\left[T_{\\mathrm{lc}}\\left(T_{\\mathrm{R}, k}\\right)\\right]}=\\frac{P_{\\mathrm{PR}} \\cdot c_{\\mathrm{p}}+\\left(1-P_{\\mathrm{PR}}\\right) \\cdot c_{\\mathrm{c}}}{P_{\\mathrm{PR}} \\cdot\\left(T_{\\mathrm{R}, k}\\right)+\\int_t^{T_{\\mathrm{R}, k}} t f_{R U L_{\\mathrm{Pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t}$\n",
    "> * E[C_{rep}(T_{R,k})]: Expected cost of replacement\n",
    "> * E[T_{lc}(T_{R,k})]: Expected lifecycle time\n",
    "> * P_{PR}: Probability of preventive replacement (defined in Equation 13)\n",
    "> * c_p: Cost of preventive replacement\n",
    "> * c_c: Cost of corrective replacement (failure)\n",
    "> * f_{RUL_{pred,k}}(t): Full distribution of the RUL prediction at time t_k\n",
    "> * The find_optimal_replacement_time function implements the objective function from PDM policy 2. It finds the optimal replacement time T_R_k by minimizing the expected cost per unit time. <br>\n",
    "\n",
    "5. Interpretation of Equation 12:\n",
    "> * Numerator: Represents the expected cost, considering both preventive and corrective replacements.\n",
    "> * Denominator: Represents the expected lifecycle time.\n",
    "> * By minimizing this ratio, the policy aims to find the optimal balance between cost and component lifetime.\n",
    "\n",
    "\n",
    "6. Probability of Preventive Replacement(Equation 13): \n",
    "> P_{PR} represents the probability that the component will be preventively replaced at T_{R,k}.\n",
    "> It's calculated by integrating the RUL prediction distribution from T_{R,k} to infinity.\n",
    "\n",
    "> $$\n",
    "P_{\\mathrm{PR}}=\\int_{T_{\\mathrm{R}, k}}^{\\infty} f_{R U L_{\\mathrm{Pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t\n",
    "$$       \n",
    "\n",
    "7. Full RUL Distribution: Unlike PDM policy 1, this policy uses the full distribution of the Remaining Useful Life (RUL) prediction, allowing for more nuanced decision-making.<br>\n",
    "\n",
    "8. The function `find_optimal_replacement_time` and its Parameters: function is designed to determine the optimal time for preventive replacement in a Predictive Maintenance (PdM) system. It uses a lognormal distribution fitted to the predicted probabilities of component failure to calculate the expected cost per unit time and find the optimal replacement time.<br>\n",
    " , representing the probability of needing a preventive replacement after T_R_K. <br>\n",
    "> `probabilities:` The predicted probabilities of class1 and class2 from the model. <br>\n",
    "> C_p, C_c, current_cycle, seq_length <br>\n",
    "> `Fitting the Lognormal Distribution:` `mu, sigma = fit_lognormal_cdf(probabilities):` This fits a lognormal distribution to the given probabilities, returning the parameters mu and sigma..\n",
    "> * `Probability of Preventive Replacement (P_PR):` This is the probability that the component survives until T_R_k. <br>\n",
    "> * `Expected Replacement Cost (E_C_rep) :` It's a weighted sum of preventive (C_p) and corrective (C_c) replacement costs.<br> \n",
    "> * `Expected Lifecycle Time (E_T_lc) :` The first term (P_PR * T_R_k) is the expected time if preventive replacement occurs\n",
    "and The sum calculates the expected time if failure occurs before T_R_k.<br>\n",
    "\n",
    "9. `Optimization:`\n",
    "> * Uses scipy's `minimize_scalar` function to find the T_R_k that minimizes the objective function.<br>\n",
    "> * The search is bounded between the current cycle and the end of the sequence.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  250\n",
      "current_cycle:  250 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  260\n",
      "current_cycle:  260 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  270\n",
      "current_cycle:  270 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  280\n",
      "current_cycle:  280 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  290\n",
      "current_cycle:  290 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  250\n",
      "current_cycle:  250 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  260\n",
      "current_cycle:  260 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  270\n",
      "current_cycle:  270 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  280\n",
      "current_cycle:  280 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999493320217\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.stats import lognorm\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.integrate import quad\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_2_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT,C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        \n",
    "        preventive_replacement = False\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            \n",
    "            if current_cycle in array_decisions:\n",
    "                #print('current_cycle: ', current_cycle)\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "                #print(\"probabilities: \", probabilities, \"shape of probability: \", probabilities.shape)\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle,DT, params)\n",
    "                \n",
    "                #sequence_end = current_cycle + params['seq_length']\n",
    "                #print(\"current_cycle: \", current_cycle,\"|| DT: \", DT, \"||T_R_k_optimal: \", T_R_k_optimal)\n",
    "\n",
    "                # Apply PDM policy 2\n",
    "                if DT >= T_R_k_optimal:\n",
    "                #if sequence_end + DT >= T_R_k_optimal:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "\n",
    "\n",
    "                # Evaluate decision heuristics\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    #T_R_i = params['seq_length'] + current_cycle\n",
    "                    T_R_i =  current_cycle\n",
    "                    print(\"T_R_i: \", T_R_i)\n",
    "                    #T_R_i =  T_R_k_optimal\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    print(\"T_F_i: \", T_F_i)\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            \n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expected_time_to_failure(T_R_k, current_cycle, mu, sigma):\n",
    "    \"\"\"Calculate the expected time to failure.\"\"\"\n",
    "    # Define the integrand for E[T_lc]\n",
    "    def integrand(t):\n",
    "        return t * lognorm.pdf(t - current_cycle, mu, sigma)\n",
    "\n",
    "    #Calculate the integral from current_cycle + params['seq_length'] to T_R_k\n",
    "    integral_value, error = quad(integrand, current_cycle , T_R_k, limit=200)\n",
    "        \n",
    "    return integral_value\n",
    "\n",
    "\n",
    "\n",
    "def lognorm_cdf(x, mu, sigma):\n",
    "    return lognorm.cdf(x, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "\n",
    "def fit_lognormal_cdf(probabilities):\n",
    "    # Extract probabilities for class 1 and 2 from the last sequence\n",
    "    p1 = probabilities[-1, 1]  # Probability of class 1\n",
    "    p2 = probabilities[-1, 2]  # Probability of class 2\n",
    "\n",
    "    # Estimate parameters of lognormal distribution\n",
    "    # We'll use two points: (1, p1) and (2, p1+p2)\n",
    "    x = np.array([1, 2])\n",
    "    y = np.array([p1, p1+p2])\n",
    "\n",
    "    # Use scipy.stats to fit the lognormal CDF\n",
    "    shape, loc, scale = stats.lognorm.fit(x, y, floc=0)\n",
    "    \n",
    "    # Convert lognorm parameters to mu and sigma\n",
    "    sigma = shape\n",
    "    mu = np.log(scale)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle,DT, params):\n",
    "    \"\"\"Find the optimal replacement time for PdM policy 2.\"\"\"\n",
    "    #sequence_end = current_cycle + params['seq_length']\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "    \n",
    "    def objective_function(T_R_k):\n",
    "        # P_PR: Probability that the component will survive beyond time T_R_k\n",
    "        #P_PR = 1 - lognorm.cdf(T_R_k - sequence_end, s=sigma, scale=np.exp(mu))\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "        \n",
    "        # Calculate expected cost of replacement\n",
    "        E_C_rep = P_PR * C_p + (1 - P_PR) * C_c\n",
    "        \n",
    "        # Calculate expected time to failure using the fitted lognormal distribution\n",
    "        #E_T_lc = P_PR * T_R_k + expected_time_to_failure(T_R_k, sequence_end, mu, sigma)\n",
    "        E_T_lc = P_PR * T_R_k + expected_time_to_failure(T_R_k, current_cycle, mu, sigma)\n",
    "        \n",
    "        return E_C_rep / E_T_lc\n",
    "\n",
    "    # Use minimize_scalar to find the optimal replacement time\n",
    "    #result = minimize_scalar(objective_function, bounds=(sequence_end, sequence_end + DT), method='bounded')\n",
    "    result = minimize_scalar(objective_function, bounds=(0, 2*DT), method='bounded')\n",
    "    \n",
    "    return result.x\n",
    "# Usage\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 214., 293., 267., 188., 278., 178., 213., 217., 154., 135.,\n",
       "       341., 155., 258., 283., 336., 202., 156., 185., 200.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "       1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "       1000., 1000.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    #ΔT = params['seq_length']  # Time interval between decision points\n",
    "    #w = (L // ΔT) * ΔT  # Adjusted lead time\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(pdm_df[pdm_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "            #print(\"current_cycle: \", current_cycle)\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle, DT, params)\n",
    "                #print(\"current_cycle: \", current_cycle,\"||current_cycle + params['seq_length']: \", current_cycle + params['seq_length'], \"||T_R_k_optimal: \", T_R_k_optimal)\n",
    "                # Apply PDM policy 2\n",
    "                if DT >= T_R_k_optimal:\n",
    "                #if sequence_end + DT >= T_R_k_optimal:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                # Ordering decision\n",
    "                #prob_RUL_smaller_w1 = np.mean(probabilities[:, 1] + probabilities[:, 2])\n",
    "                prob_RUL_smaller_w1 = probabilities[-1, 1] + probabilities[-1, 2]\n",
    "                p_order_thres = C_p / C_c  # Heuristic threshold for ordering\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    t_order_array[counter] = params['seq_length'] + current_cycle + w\n",
    "                    \n",
    "                    order = True\n",
    "                    #print(f\"Component ordered at cycle: {t_order_array[counter]}\")\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = params['seq_length'] + current_cycle\n",
    "                    #T_R_i =  T_R_k_optimal\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "def expected_time_to_failure(T_R_k, current_cycle, mu, sigma):\n",
    "    \"\"\"Calculate the expected time to failure.\"\"\"\n",
    "    # Define the integrand for E[T_lc]\n",
    "    def integrand(t):\n",
    "        return t * lognorm.pdf(t - current_cycle, mu, sigma)\n",
    "\n",
    "    #Calculate the integral from current_cycle + params['seq_length'] to T_R_k\n",
    "    integral_value, error = quad(integrand, current_cycle , T_R_k, limit=200)\n",
    "    \n",
    "    return integral_value\n",
    "\n",
    "\n",
    "\n",
    "def lognorm_cdf(x, mu, sigma):\n",
    "    return lognorm.cdf(x, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "\n",
    "def fit_lognormal_cdf(probabilities):\n",
    "    # Extract probabilities for class 1 and 2 from the last sequence\n",
    "    p1 = probabilities[-1, 1]  # Probability of class 1\n",
    "    p2 = probabilities[-1, 2]  # Probability of class 2\n",
    "\n",
    "    # Estimate parameters of lognormal distribution\n",
    "    # We'll use two points: (1, p1) and (2, p1+p2)\n",
    "    x = np.array([1, 2])\n",
    "    y = np.array([p1, p1+p2])\n",
    "\n",
    "    # Use scipy.stats to fit the lognormal CDF\n",
    "    shape, loc, scale = stats.lognorm.fit(x, y, floc=0)\n",
    "    \n",
    "    # Convert lognorm parameters to mu and sigma\n",
    "    sigma = shape  \n",
    "    mu = np.log(scale) \n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle,DT, params):\n",
    "    \"\"\"Find the optimal replacement time for PdM policy 2.\"\"\"\n",
    "    sequence_end = current_cycle + params['seq_length']\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "    \n",
    "    def objective_function(T_R_k):\n",
    "        # P_PR: Probability that the component will survive beyond time T_R_k\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - sequence_end, s=sigma, scale=np.exp(mu))\n",
    "        \n",
    "        # Calculate expected cost of replacement\n",
    "        E_C_rep = P_PR * C_p + (1 - P_PR) * C_c\n",
    "        \n",
    "        # Calculate expected time to failure using the fitted lognormal distribution\n",
    "        E_T_lc = P_PR * T_R_k + expected_time_to_failure(T_R_k, sequence_end, mu, sigma)\n",
    "        \n",
    "        return E_C_rep / E_T_lc\n",
    "\n",
    "    # Use minimize_scalar to find the optimal replacement time\n",
    "    #result = minimize_scalar(objective_function, bounds=(sequence_end, sequence_end + DT), method='bounded')\n",
    "    result = minimize_scalar(objective_function, bounds=(0, 2*DT), method='bounded')\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "# Usage\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230., 210., 290., 260., 180., 270., 160., 200., 200., 150., 130.,\n",
       "       340., 140., 260., 280., 330., 200., 140., 180., 180.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 214., 293., 267., 188., 278., 178., 213., 217., 154., 135.,\n",
       "       341., 155., 258., 283., 336., 202., 156., 185., 200.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "       1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "       1000., 1000.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 160., 170., 130., 120., 120.,  20.,  70.,  30., 160., 150.,\n",
       "       190.,  50., 220., 170., 140., 180.,  40., 150.,   0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDM Policy 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Objective: `PDM Policy 3 represents a significant advancement in predictive maintenance by utilizing the full distribution of RUL data to inform maintenance decisions. The objective function captures both the expected costs of preventive and corrective actions and the additional costs associated with early replacements, providing a comprehensive framework for optimizing maintenance strategies. <br>\n",
    "2. `Explanation of Equation(14): ` The equation for the objective function in PDM Policy 3 is given as:\n",
    "> $f\\left(T_{\\mathrm{R}, k}\\right)=P_{\\mathrm{PR}} \\cdot C_{\\mathrm{p}}+\\left(1-P_{\\mathrm{PR}}\\right) \\cdot C_{\\mathrm{c}}+\\int_{T_{\\mathrm{R}, k}}^{\\infty}\\left(t-T_{\\mathrm{R}, k}\\right) \\cdot \\frac{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[C_{\\mathrm{rep}}\\right]}{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[T_{l \\mathrm{c}}\\right]} f_{R U L_{\\mathrm{pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t$\n",
    "\n",
    "3. `Components of the equation: `\n",
    "\n",
    ">  1. `Expected Replacement Cost: `\n",
    ">> * The first two terms, $P_{\\mathrm{PR}} \\cdot C_{\\mathrm{p}}$ and $\\left(1-P_{\\mathrm{PR}}\\right) \\cdot C_{\\mathrm{c}}$ represent the expected costs associated with preventive and corrective replacements, respectively. <br>\n",
    ">> * $P_{\\mathrm{PR}}$ is the probability that the components will survive until the replacement time $T_{\\mathrm{R}, k}$ <br>\n",
    ">> * $ C_{\\mathrm{p}}$ is the cost of preventive replacement, and $ C_{\\mathrm{c}}$ is the cost of corrective replacement. <br>\n",
    "\n",
    "> 2. `Integral Term: `\n",
    ">> * The integral term quantifies the additional expected maintenance cost associated with an \"early\" replacement at $T_{\\mathrm{R}, k}$ <br>\n",
    ">> * The expression $ \\left(t-T_{\\mathrm{R}, k}\\right) $ represents the time lost due to an early replacement.\n",
    ">> * The term $ \\frac{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[C_{\\mathrm{rep}}\\right]}{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[T_{l \\mathrm{c}}\\right]} $  is the long-run expected maintenance cost per unit time concerning the distribution of the population of components. It provides a scaling factor for the additional cost incurred by replacing the component early. <br> \n",
    ">> * $ f_{R U L_{\\mathrm{pred}, k}}\\left(t-t_k\\right) $is the predicted probability density function of the RUL, representing the likelihood of failure occurring at time t after the current cycle $t_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad, simps, romberg\n",
    "from scipy.stats import lognorm\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "#def calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "def calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df,scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique())) \n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "\n",
    "        for current_cycle in range(pdm_df[pdm_df['id'] == id].shape[0] - params['seq_length'] + 1):      \n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c, train_df, current_cycle,DT, params)\n",
    "\n",
    "                #print(\"current_cycle: \", current_cycle,\"||current_cycle + DT: \", current_cycle + DT, \"||T_R_k_optimal: \", T_R_k_optimal)\n",
    "                sequence_end = current_cycle + params['seq_length']\n",
    "                # Apply PDM policy 2 but calculate T_R_k_optimal using PDM Policy 3\n",
    "                if DT >= T_R_k_optimal:\n",
    "                #if sequence_end + DT >= T_R_k_optimal:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                # Evaluate decision heuristics\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = params['seq_length'] + current_cycle\n",
    "                    #T_R_i =  int(T_R_k_optimal)\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            \n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "\n",
    "#def lognorm_cdf(x, mu, sigma):\n",
    "#    return lognorm.cdf(x, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "\n",
    "def fit_lognormal_cdf(probabilities):\n",
    "    # Extract probabilities for class 1 and 2 from the last sequence\n",
    "    p1 = probabilities[-1, 1]  # Probability of class 1\n",
    "    p2 = probabilities[-1, 2]  # Probability of class 2\n",
    "\n",
    "    # Estimate parameters of lognormal distribution\n",
    "    # We'll use two points: (1, p1) and (2, p1+p2)\n",
    "    x = np.array([1, 2])\n",
    "    y = np.array([p1, p1+p2])\n",
    "\n",
    "    # Use scipy.stats to fit the lognormal CDF\n",
    "    shape, loc, scale = stats.lognorm.fit(x, y, floc=0)\n",
    "    \n",
    "    # Convert lognorm parameters to mu and sigma\n",
    "    sigma = shape\n",
    "    mu = np.log(scale)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\"\"\"\n",
    "def fit_lognormal_cdf(probabilities):\n",
    "    # Extract probabilities for class 1 and 2 from the last sequence\n",
    "    p1 = probabilities[-1, 1]  # Probability of class 1\n",
    "    p2 = probabilities[-1, 2]  # Probability of class 2\n",
    "\n",
    "    # Estimate parameters of lognormal distribution\n",
    "    # We'll use two points: (1, p1) and (2, p1+p2)\n",
    "    x = np.array([1, 2])\n",
    "    y = np.array([p1, p1+p2])\n",
    "\n",
    "    # Use scipy.stats to fit the lognormal CDF\n",
    "    shape, loc, scale = stats.lognorm.fit(x, y, floc=0)\n",
    "    \n",
    "    # Convert lognorm parameters to mu and sigma\n",
    "    #sigma = shape\n",
    "    #mu = np.log(scale)\n",
    "    \n",
    "    return shape, loc, scale\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_expected_values(train_df):\n",
    "    mu_TF = 0\n",
    "    unique_ids = train_df['id'].unique()\n",
    "    for id in unique_ids:\n",
    "        mu_TF += train_df[train_df['id'] == id]['cycle'].iloc[-1]\n",
    "    return  mu_TF / len(unique_ids)\n",
    "\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c, train_df, current_cycle, DT, params):\n",
    "    sequence_end = current_cycle + params['seq_length']\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "    #shape, loc, scale = fit_lognormal_cdf(probabilities)\n",
    "    mu_TF = calculate_expected_values(train_df)\n",
    "    #print('mu_TF: ',mu_TF)\n",
    "    #mu_TF = calculate_expected_values(pdm_df)\n",
    "    \n",
    "    def objective_function(T_R_k):\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - sequence_end, mu, sigma)\n",
    "        #P_PR = 1 - lognorm.cdf(T_R_k - sequence_end, s=sigma, scale=np.exp(mu))\n",
    "        #P_PR = 1 - lognorm.cdf(T_R_k - sequence_end, shape, loc, scale)\n",
    "        \n",
    "        def integral_term(t):\n",
    "            return (t - T_R_k) * lognorm.pdf(t - sequence_end, mu, sigma)\n",
    "            #return (t - T_R_k) * lognorm.pdf(t - sequence_end, s=sigma, scale=np.exp(mu))\n",
    "            #return (t - T_R_k) * lognorm.pdf(t - sequence_end, shape, loc, scale)\n",
    "        additional_cost, _ = quad(integral_term, T_R_k, np.inf, limit=200)\n",
    "        additional_cost *= C_p / mu_TF\n",
    "        \n",
    "        return P_PR * C_p + (1 - P_PR) * C_c + additional_cost\n",
    "    \n",
    "    # Find the optimal replacement time using bounded optimization\n",
    "    #result = minimize_scalar(objective_function, bounds=(sequence_end, sequence_end + 2*DT), method='bounded')\n",
    "    result = minimize_scalar(objective_function, bounds=(0,2*DT), method='bounded')\n",
    "    T_R_k_optimal = result.x\n",
    "    \n",
    "    return T_R_k_optimal\n",
    "\n",
    "# Usage                                                                                                                               \n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df,scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "#t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80.,\n",
       "       80., 80., 80., 80., 80., 80., 80.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60.,\n",
       "       60., 60., 60., 60., 60., 60., 60.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    #ΔT = params['seq_length']  # Time interval between decision points\n",
    "    #w = (L // ΔT) * ΔT  # Adjusted lead time\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(pdm_df[pdm_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c, train_df, current_cycle,DT, params)\n",
    "                #print(\"current_cycle: \", current_cycle,\"||current_cycle + DT : \", current_cycle + DT, \"||T_R_k_optimal: \", T_R_k_optimal)\n",
    "                \n",
    "                # Apply PDM policy 2 but calculate T_R_k_optimal using PDM Policy 3\n",
    "                if DT >= T_R_k_optimal:\n",
    "                #if sequence_end + DT >= T_R_k_optimal:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "\n",
    "                # Ordering decision\n",
    "                #prob_RUL_smaller_w1 = np.mean(probabilities[:, 1] + probabilities[:, 2])\n",
    "                prob_RUL_smaller_w1 = probabilities[-1, 1] + probabilities[-1, 2]\n",
    "                p_order_thres = C_p / C_c  # Heuristic threshold for ordering\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    t_order_array[counter] = params['seq_length'] + current_cycle+ w\n",
    "                    #t_order_array[counter] = current_cycle+ w\n",
    "                    order = True\n",
    "                    #print(f\"Component ordered at cycle: {t_order_array[counter]}\")\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = params['seq_length'] + current_cycle\n",
    "                    #T_R_i =  T_R_k_optimal\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "# Usage\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df, scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80.,\n",
       "       80., 80., 80., 80., 80., 80., 80.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60.,\n",
       "       60., 60., 60., 60., 60., 60., 60.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  250\n",
      "current_cycle:  250 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  260\n",
      "current_cycle:  260 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  270\n",
      "current_cycle:  270 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  280\n",
      "current_cycle:  280 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  290\n",
      "current_cycle:  290 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n",
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  160\n",
      "current_cycle:  160 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_cycle:  170 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  180\n",
      "current_cycle:  180 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  190\n",
      "current_cycle:  190 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  200\n",
      "current_cycle:  200 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  210\n",
      "current_cycle:  210 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  220\n",
      "current_cycle:  220 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  230\n",
      "current_cycle:  230 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  240\n",
      "current_cycle:  240 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  250\n",
      "current_cycle:  250 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  260\n",
      "current_cycle:  260 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  270\n",
      "current_cycle:  270 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  280\n",
      "current_cycle:  280 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  0\n",
      "current_cycle:  0 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  10\n",
      "current_cycle:  10 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  20\n",
      "current_cycle:  20 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  30\n",
      "current_cycle:  30 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  40\n",
      "current_cycle:  40 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  50\n",
      "current_cycle:  50 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  60\n",
      "current_cycle:  60 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  70\n",
      "current_cycle:  70 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  80\n",
      "current_cycle:  80 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  90\n",
      "current_cycle:  90 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  100\n",
      "current_cycle:  100 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  110\n",
      "current_cycle:  110 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  120\n",
      "current_cycle:  120 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  130\n",
      "current_cycle:  130 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  140\n",
      "current_cycle:  140 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n",
      "current_cycle:  150\n",
      "current_cycle:  150 || DT:  10 ||T_R_k_optimal:  19.99999335625205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17864/2172916391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Call the function to get updated arrays for policy3 with ordering                calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df, scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mt_order_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_LC_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts_rep_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts_delay_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts_stock_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_probabilities_for_pdm_policy_3_With_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdm_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_normalize_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_unav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Calculate M_hat using the updated arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17864/3116091228.py\u001b[0m in \u001b[0;36mcalculate_probabilities_for_pdm_policy_3_With_ordering\u001b[0;34m(estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# Calculate optimal replacement time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mT_R_k_optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal_replacement_time_for_PDM_Policy3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;31m#print(\"current_cycle: \", current_cycle,\"||current_cycle + DT : \", current_cycle + DT, \"||T_R_k_optimal: \", T_R_k_optimal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17864/283128450.py\u001b[0m in \u001b[0;36mfind_optimal_replacement_time_for_PDM_Policy3\u001b[0;34m(probabilities, C_p, C_c, train_df, current_cycle, DT, params)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Find the optimal replacement time using bounded optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m#result = minimize_scalar(objective_function, bounds=(sequence_end, sequence_end + 2*DT), method='bounded')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mDT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bounded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mT_R_k_optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize_scalar\u001b[0;34m(fun, bracket, bounds, args, method, tol, options)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'disp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_scalar_bounded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'golden'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_scalar_golden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbracket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_scalar_bounded\u001b[0;34m(func, bounds, args, xatol, maxiter, disp, **unknown_options)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \u001b[0msi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mfu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0mfmin_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17864/283128450.py\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(T_R_k)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m#return (t - T_R_k) * lognorm.pdf(t - sequence_end, s=sigma, scale=np.exp(mu))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m#return (t - T_R_k) * lognorm.pdf(t - sequence_end, shape, loc, scale)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0madditional_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegral_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_R_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0madditional_cost\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mC_p\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmu_TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36mquad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0;32m--> 352\u001b[0;31m                        points)\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36m_quad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfbounds\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17864/283128450.py\u001b[0m in \u001b[0;36mintegral_term\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mintegral_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mT_R_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlognorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;31m#return (t - T_R_k) * lognorm.pdf(t - sequence_end, s=sigma, scale=np.exp(mu))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m#return (t - T_R_k) * lognorm.pdf(t - sequence_end, shape, loc, scale)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mpdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0mdtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m         \u001b[0mcond0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m         \u001b[0mcond1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond0\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcond1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m_argcheck\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \"\"\"\n\u001b[1;32m    965\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def calculate_T_R_perfect(pdm_df, id, DT=10):\n",
    "    # Retrieve the perfect failure time for the ith component\n",
    "    T_F_perfect = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "    \n",
    "    # Calculate k as the largest integer such that k * DT < T_F_perfect\n",
    "    k = int(T_F_perfect // DT)  # Use floor division\n",
    "    T_R_perfect = k * DT  # Calculate T_R_perfect\n",
    "    \n",
    "    return T_R_perfect\n",
    "\n",
    "def calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df):\n",
    "    #n = len(costs_rep_array)  # Number of components\n",
    "\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = (np.mean(costs_rep_array) + np.mean(costs_delay_array) + np.mean(costs_stock_array))\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric\n",
    "    M_hat = numerator / denominator if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "def calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df):\n",
    "    #n = len(costs_rep_array)  # Number of components\n",
    "\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = np.mean(costs_rep_array)\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric\n",
    "    M_hat = numerator / denominator if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "\n",
    "# Range of C_p values\n",
    "C_p_values = np.array([0,100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "C_c = 1000  # Constant corrective replacement cost\n",
    "M_hat_Policy1_With_ordering = []\n",
    "M_hat_Policy1_Without_ordering = []\n",
    "\n",
    "M_hat_Policy2_With_ordering = []\n",
    "M_hat_Policy2_Without_ordering = []\n",
    "\n",
    "M_hat_Policy3_With_ordering = []\n",
    "M_hat_Policy3_Without_ordering = []\n",
    "# Calculate M_hat for each C_p\n",
    "for C_p in C_p_values:\n",
    "    #Call the function to get updated arrays for policy1 with ordering  \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat1 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_With_ordering.append(M_hat1)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy1 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat2 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_Without_ordering.append(M_hat2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 with ordering\n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat3 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_With_ordering.append(M_hat3)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat4 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_Without_ordering.append(M_hat4)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 with ordering                calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df, scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)   \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat5 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_With_ordering.append(M_hat5)\n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df,scaler,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat6 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_Without_ordering.append(M_hat6)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate C_p/C_c ratios\n",
    "C_p_C_c_ratios = C_p_values / C_c\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy1_With_ordering, marker='o', label='Policy 1 With Ordering')\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy1_Without_ordering, marker='o', label='Policy 1 Without Ordering')\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy2_With_ordering, marker='o', label='Policy 2 With Ordering')\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy2_Without_ordering, marker='o', label='Policy 2 Without Ordering')\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy3_With_ordering, marker='o', label='Policy 3 With Ordering')\n",
    "plt.plot(C_p_C_c_ratios, M_hat_Policy3_Without_ordering, marker='o', label='Policy 3 Without Ordering')\n",
    "\n",
    "plt.title('Decision Metric M_hat vs C_p/C_c Ratio')\n",
    "plt.xlabel('C_p/C_c Ratio')\n",
    "plt.ylabel('Decision Metric M_hat')\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.xticks(C_p_C_c_ratios)  # Set x-ticks to the calculated ratios\n",
    "plt.axhline(0, color='black', lw=0.5, ls='--')  # Add a horizontal line at y=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn import preprocessing\\n\\ncounter = 0\\nfor id in test_df['id'].unique():\\n    print('ID:', id)\\n    preventive_replacement = False\\n    order = False\\n\\n    for current_cycle in range(test_df[test_df['id'] == id].shape[0] - params['seq_length'] + 1):\\n\\n        if current_cycle in array_decisions:\\n            min_max_scaler = preprocessing.MinMaxScaler()\\n            norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[test_df['id'] == id][cols_normalize_train][:params['seq_length'] + current_cycle]),\\n                                        columns=cols_normalize_train,\\n                                        index=test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].index)\\n\\n            join_df = test_df[test_df['id'] == id][:params['seq_length'] + current_cycle][test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_test_df)\\n            test_df_eval_online = join_df.reindex(columns=test_df[test_df['id'] == id][current_cycle:params['seq_length'] + current_cycle].columns)\\n\\n            seq_array_test_k = test_df_eval_online[sequence_cols].values[current_cycle:params['seq_length'] + current_cycle]\\n            seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], nb_features)\\n\\n            # Convert to tensor\\n            seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\\n\\n            # Predict\\n            outputs = estimator(seq_tensor).squeeze()\\n            #print('outputs: ',outputs )\\n            probabilities = torch.softmax(outputs, dim=1).cpu().detach().numpy() #shape: [batch_size, sequence_length,num_classes]\\n            #prob_RUL_smaller_DT = probabilities[-1,2] # class2 probability for last time steps\\n            #prob_RUL_smaller_w1 = probabilities[-1,1] + probabilities[-1,2] # (class1+class2) for last time steps\\n            \\n            # Assuming 'probabilities' is your (1, 50, 3) tensor\\n            prob_RUL_smaller_w1 = probabilities[:, 1] + probabilities[:, 2]\\n            prob_RUL_smaller_DT = probabilities[:, 2]\\n\\n            \\n            #print('Collective probabilities: ', probabilities)\\n            #print('shape of Collective probabilities: ', probabilities.shape)\\n            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\\n            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n\\n            # Evaluate decision heuristics\\n            if not order:#\\n                # Order component\\n                if C_p <= ( * C_c):\\n                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\\n                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n                    t_order_array[counter] = params['seq_length'] + current_cycle\\n                    order = True\\n                    print('component ordering at cycle:', t_order_array[counter])\\n            \\n            # Perform preventive replacement\\n            if C_p <= (prob_RUL_smaller_DT * C_c):\\n                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\\n\\n                t_LC_array[counter] = params['seq_length'] + current_cycle\\n                costs_rep_array[counter] = C_p\\n                print('preventive replacement informed at cycle:', t_LC_array[counter])\\n\\n                preventive_replacement = True\\n                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\\n                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\\n                break\\n\\n    if not preventive_replacement:\\n        t_LC_array[counter] = test_df[test_df['id'] == id]['cycle'].iloc[-1]\\n        print('Component failure at t:', t_LC_array[counter])\\n        costs_rep_array[counter] = C_c\\n\\n        if not order:\\n            costs_delay_array[counter] = L * C_unav\\n        else:\\n            costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\\n            costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\\n\\n    print('True failure:', test_df[test_df['id'] == id]['cycle'].iloc[-1])\\n    print('-----------------------------------------')\\n    counter += 1\\n\\n\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "\n",
    "counter = 0\n",
    "for id in test_df['id'].unique():\n",
    "    print('ID:', id)\n",
    "    preventive_replacement = False\n",
    "    order = False\n",
    "\n",
    "    for current_cycle in range(test_df[test_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "\n",
    "        if current_cycle in array_decisions:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[test_df['id'] == id][cols_normalize_train][:params['seq_length'] + current_cycle]),\n",
    "                                        columns=cols_normalize_train,\n",
    "                                        index=test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].index)\n",
    "\n",
    "            join_df = test_df[test_df['id'] == id][:params['seq_length'] + current_cycle][test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_test_df)\n",
    "            test_df_eval_online = join_df.reindex(columns=test_df[test_df['id'] == id][current_cycle:params['seq_length'] + current_cycle].columns)\n",
    "\n",
    "            seq_array_test_k = test_df_eval_online[sequence_cols].values[current_cycle:params['seq_length'] + current_cycle]\n",
    "            seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], nb_features)\n",
    "\n",
    "            # Convert to tensor\n",
    "            seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Predict\n",
    "            outputs = estimator(seq_tensor).squeeze()\n",
    "            #print('outputs: ',outputs )\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().detach().numpy() #shape: [batch_size, sequence_length,num_classes]\n",
    "            #prob_RUL_smaller_DT = probabilities[-1,2] # class2 probability for last time steps\n",
    "            #prob_RUL_smaller_w1 = probabilities[-1,1] + probabilities[-1,2] # (class1+class2) for last time steps\n",
    "            \n",
    "            # Assuming 'probabilities' is your (1, 50, 3) tensor\n",
    "            prob_RUL_smaller_w1 = probabilities[:, 1] + probabilities[:, 2]\n",
    "            prob_RUL_smaller_DT = probabilities[:, 2]\n",
    "\n",
    "            \n",
    "            #print('Collective probabilities: ', probabilities)\n",
    "            #print('shape of Collective probabilities: ', probabilities.shape)\n",
    "            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "            # Evaluate decision heuristics\n",
    "            if not order:#\n",
    "                # Order component\n",
    "                if C_p <= ( * C_c):\n",
    "                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                    t_order_array[counter] = params['seq_length'] + current_cycle\n",
    "                    order = True\n",
    "                    print('component ordering at cycle:', t_order_array[counter])\n",
    "            \n",
    "            # Perform preventive replacement\n",
    "            if C_p <= (prob_RUL_smaller_DT * C_c):\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                t_LC_array[counter] = params['seq_length'] + current_cycle\n",
    "                costs_rep_array[counter] = C_p\n",
    "                print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "\n",
    "                preventive_replacement = True\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "                break\n",
    "\n",
    "    if not preventive_replacement:\n",
    "        t_LC_array[counter] = test_df[test_df['id'] == id]['cycle'].iloc[-1]\n",
    "        print('Component failure at t:', t_LC_array[counter])\n",
    "        costs_rep_array[counter] = C_c\n",
    "\n",
    "        if not order:\n",
    "            costs_delay_array[counter] = L * C_unav\n",
    "        else:\n",
    "            costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "            costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "    print('True failure:', test_df[test_df['id'] == id]['cycle'].iloc[-1])\n",
    "    print('-----------------------------------------')\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Lp4AF1KW6PuB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'costs_tot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17864/2812186258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosts_tot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'costs_tot' is not defined"
     ]
    }
   ],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzXo4Qlw6PuB"
   },
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNUa2J36PuB"
   },
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6g7k8Yrftya"
   },
   "source": [
    "### This code calculates the expected cost per unit time using the LSTM model. It computes the mean of the total costs divided by the mean of the time to component failure (t_LC_array). This metric gives an estimate of the average cost incurred per unit time in the system, considering both maintenance and operational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3_N4kNM6PuC"
   },
   "outputs": [],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgoTz57gC_P"
   },
   "source": [
    "This code segment calculates the expected cost per unit time assuming perfect prognostics.\n",
    "`1. Perfect Prognostics Calculation:`\n",
    "> * It initializes an array `t_LC_perfect_array` to store the time of component failure for each unit in the validation dataset. This is calculated by dividing the last observed cycle number by the decision interval DT and then flooring the result to get the last decision cycle before failure.\n",
    "> * The loop iterates over each unique ID in the validation dataset, calculates the time of component failure for each unit, and stores it in\n",
    "`t_LC_perfect_array`.<br>\n",
    "> * `math.floor()` is used to round down the result to the nearest multiple of `DT`.\n",
    "> * Finally, the loop increments the counter for each unit.<br>\n",
    "\n",
    "`2. Cost Calculation:`\n",
    "> * `costs_perfect_array` is initialized with a value of `C_p`, representing the cost of preventive replacements. In a perfect scenario, only preventive replacements are made.\n",
    "> * This array holds the same cost value for each unit in the validation dataset.\n",
    "\n",
    "`3. Expected Cost Calculation:`\n",
    "> * `expected_cost_perfect` is calculated by taking the mean of `costs_perfect_array` and dividing it by the mean of `t_LC_perfect_array`.\n",
    "> * This calculation provides an estimate of the average cost per unit time assuming perfect prognostics, where components are replaced preventively at regular intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOD6NDfH6PuC"
   },
   "outputs": [],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(10)\n",
    "DT=10\n",
    "counter=0\n",
    "for id in test_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(test_df[test_df['id']==id]['cycle'].iloc[-1] /DT) * DT\n",
    "    counter+=1\n",
    "\n",
    "costs_perfect_array = np.ones(10)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-vK-DBi6PuC"
   },
   "outputs": [],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7_LJce6PuC"
   },
   "outputs": [],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KofjvavY6PuD"
   },
   "outputs": [],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42Nu-Rg6PuE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
