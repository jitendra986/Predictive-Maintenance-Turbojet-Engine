{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70-XsHkDGUKu",
    "outputId": "4a5f97ed-c7cc-44c4-fc13-65ae479147b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/eragroup/anaconda3/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: matplotlib in /home/eragroup/anaconda3/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (62.3.2)\n",
      "Requirement already satisfied: wheel in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.33.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0)\n",
      "Requirement already satisfied: requests in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /home/eragroup/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (1.24.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.0.12)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmWTwJyIYWDo"
   },
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Yeksw_AMYVaV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3wOjKm186Pth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available GPU:  1\n",
      "Available devices:\n",
      "- <torch.cuda.device object at 0x7f96feb34cd0>\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#device_lib.list_local_devices()\n",
    "\n",
    "# Get the available devices\n",
    "devices = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print('number of available GPU: ',torch.cuda.device_count())  # Should print the number of visible devices\n",
    "\n",
    "# Print the available devices\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(f\"- {device}\")\n",
    "device = torch.device(\"cuda:0\")  # Use the first GPU\n",
    "\n",
    "# Print the current device\n",
    "#print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4BsYPkY7gYGx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952041472, 8512602112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 28 10:51:54 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P4000        Off  | 00000000:65:00.0  On |                  N/A |\n",
      "| 50%   49C    P0    30W / 105W |   7210MiB /  8118MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1168      G   ...roup/anaconda3/bin/python        1MiB |\n",
      "|    0   N/A  N/A      1240      G   /usr/lib/xorg/Xorg                 12MiB |\n",
      "|    0   N/A  N/A      1754      G   kwin_x11                          104MiB |\n",
      "|    0   N/A  N/A      1761      G   /usr/bin/krunner                    1MiB |\n",
      "|    0   N/A  N/A      1816      G   /usr/lib/firefox/firefox            2MiB |\n",
      "|    0   N/A  N/A      2273      G   ...s/spyder-5.5.1/bin/python       19MiB |\n",
      "|    0   N/A  N/A      2895      C   ...s/lag_llama/bin/python3.1     6719MiB |\n",
      "|    0   N/A  N/A      3842      C   ...roup/anaconda3/bin/python       77MiB |\n",
      "|    0   N/A  N/A     15451      G   /usr/lib/xorg/Xorg                171MiB |\n",
      "|    0   N/A  N/A     15724      G   kwin_x11                            5MiB |\n",
      "|    0   N/A  N/A     15732      G   /usr/bin/krunner                   19MiB |\n",
      "|    0   N/A  N/A     15734      G   /usr/bin/plasmashell               54MiB |\n",
      "|    0   N/A  N/A     18871      G   ...mviewer/tv_bin/TeamViewer        6MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass classification\n",
    "#Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iklamKYlNjh"
   },
   "source": [
    "\n",
    "`os`: This module provides functions for interacting with the operating system. It's commonly used for tasks such as file manipulation and directory operations.<br>\n",
    "`sklearn.preprocessing`: This module from scikit-learn provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables.<br>\n",
    "`sklearn.metrics`: This module contains functions for evaluating model performance, such as computing confusion matrices, recall scores, and precision scores.<br>\n",
    "`multiclass_model_w1_30.h5`:The .h5 extension indicates that the model will be saved in the Hierarchical Data Format version 5 (HDF5) format, which is commonly used for storing large numerical datasets. The model will be saved with the filename **multiclass_model_w1_30.h5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "#from tensorflow.keras.models import Sequential,load_model\n",
    "#from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "#model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1       2       3      4       5       6        7        8      9   \\\n",
       "0   1   1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60  14.62   \n",
       "1   1   2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14  14.62   \n",
       "2   1   3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20  14.62   \n",
       "3   1   4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87  14.62   \n",
       "4   1   5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22  14.62   \n",
       "\n",
       "   ...       18      19    20   21    22     23     24       25  26  27  \n",
       "0  ...  8138.62  8.4195  0.03  392  2388  100.0  39.06  23.4190 NaN NaN  \n",
       "1  ...  8131.49  8.4318  0.03  392  2388  100.0  39.00  23.4236 NaN NaN  \n",
       "2  ...  8133.23  8.4178  0.03  390  2388  100.0  38.95  23.3442 NaN NaN  \n",
       "3  ...  8133.83  8.3682  0.03  392  2388  100.0  38.88  23.3739 NaN NaN  \n",
       "4  ...  8133.80  8.4294  0.03  393  2388  100.0  38.90  23.4044 NaN NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n0PdH-gOLmrl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpsNnInsevi"
   },
   "source": [
    "`train_df.sort_values(['id','cycle'])`: This line sorts the DataFrame **train_df** first by the 'id' column and then by the 'cycle' column. It ensures that the data is ordered by engine ID and cycle number, which may be necessary for certain analyses or modeling tasks. The sorted DataFrame is then assigned back to the variable **train_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mo0v9RsVriPz"
   },
   "outputs": [],
   "source": [
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "E7x-rVZz6Ptq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20626</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.49</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20627</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.68</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20628</td>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>520.01</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20629</td>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.67</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20630</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.30</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
       "0      1400.60  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1      1403.14  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2      1404.20  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3      1401.87  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4      1406.22  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "...        ...    ...  ...     ...      ...      ...     ...   ...  ...   ...   \n",
       "20626  1428.63  14.62  ...  519.49  2388.26  8137.60  8.4956  0.03  397  2388   \n",
       "20627  1433.58  14.62  ...  519.68  2388.22  8136.50  8.5139  0.03  395  2388   \n",
       "20628  1428.18  14.62  ...  520.01  2388.24  8141.05  8.5646  0.03  398  2388   \n",
       "20629  1426.53  14.62  ...  519.67  2388.23  8139.29  8.5389  0.03  395  2388   \n",
       "20630  1432.14  14.62  ...  519.30  2388.26  8137.33  8.5036  0.03  396  2388   \n",
       "\n",
       "         s19    s20      s21  \n",
       "0      100.0  39.06  23.4190  \n",
       "1      100.0  39.00  23.4236  \n",
       "2      100.0  38.95  23.3442  \n",
       "3      100.0  38.88  23.3739  \n",
       "4      100.0  38.90  23.4044  \n",
       "...      ...    ...      ...  \n",
       "20626  100.0  38.49  22.9735  \n",
       "20627  100.0  38.30  23.1594  \n",
       "20628  100.0  38.44  22.9333  \n",
       "20629  100.0  38.29  23.0640  \n",
       "20630  100.0  38.37  23.0522  \n",
       "\n",
       "[20631 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing\n",
    "data preprocessing step, particularly for labeling the data for training purposes. Let's break down what each part of the code does:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-_dxq60Nf4"
   },
   "source": [
    ">`Data Labeling`: This part calculates the Remaining Useful Life (RUL) or Time to Failure for each engine by finding the maximum cycle number (cycle) for each engine ID (id). The result is stored in a DataFrame rul with columns 'id' and 'max'.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BnqCery8yupl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  cycle\n",
       "0     1    192\n",
       "1     2    287\n",
       "2     3    179\n",
       "3     4    189\n",
       "4     5    269\n",
       "..  ...    ...\n",
       "95   96    336\n",
       "96   97    202\n",
       "97   98    156\n",
       "98   99    185\n",
       "99  100    200\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZubPMTD0T9z"
   },
   "source": [
    ">`Merge RUL with Training Data`:the RUL information is merged back into the original training DataFrame **train_df** based on the engine ID. This allows each row in train_df to have the corresponding maximum cycle number as well.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6Ycs7i7Qyu32"
   },
   "outputs": [],
   "source": [
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AroVzMsuzHhp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...      s13      s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
       "1  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
       "2  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
       "3  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
       "4  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
       "\n",
       "       s21  max  \n",
       "0  23.4190  192  \n",
       "1  23.4236  192  \n",
       "2  23.3442  192  \n",
       "3  23.3739  192  \n",
       "4  23.4044  192  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-eouxGv0gpG"
   },
   "source": [
    ">`Calculate RUL`: This line calculates the RUL by subtracting the current cycle number ('cycle') from the maximum cycle number ('max') for each engine. This represents how many more cycles the engine is expected to operate before failure.<br>\n",
    ">`Drop Unnecessary Columns`: After calculating RUL, the 'max' column, which was used temporarily to calculate RUL, is dropped from the DataFrame as it's no longer needed.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vulNLE0CztxT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.19</td>\n",
       "      <td>1584.07</td>\n",
       "      <td>1395.16</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.69</td>\n",
       "      <td>8.4311</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3255</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.07</td>\n",
       "      <td>1595.77</td>\n",
       "      <td>1407.81</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8128.74</td>\n",
       "      <td>8.4105</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.01</td>\n",
       "      <td>23.2963</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.00</td>\n",
       "      <td>1591.11</td>\n",
       "      <td>1404.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8127.89</td>\n",
       "      <td>8.4012</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.2554</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1592.73</td>\n",
       "      <td>1406.13</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>8131.77</td>\n",
       "      <td>8.4481</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.82</td>\n",
       "      <td>23.2323</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.22</td>\n",
       "      <td>1589.63</td>\n",
       "      <td>1411.35</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8132.49</td>\n",
       "      <td>8.4241</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.93</td>\n",
       "      <td>23.4090</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0    1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1    1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2    1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3    1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4    1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "..  ..    ...       ...       ...       ...     ...     ...      ...      ...   \n",
       "95   1     96   -0.0034    0.0001     100.0  518.67  642.19  1584.07  1395.16   \n",
       "96   1     97    0.0035   -0.0003     100.0  518.67  642.07  1595.77  1407.81   \n",
       "97   1     98    0.0006    0.0004     100.0  518.67  642.00  1591.11  1404.56   \n",
       "98   1     99   -0.0005   -0.0000     100.0  518.67  642.46  1592.73  1406.13   \n",
       "99   1    100   -0.0021   -0.0003     100.0  518.67  642.22  1589.63  1411.35   \n",
       "\n",
       "       s5  ...      s13      s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0   14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
       "1   14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
       "2   14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
       "3   14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
       "4   14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
       "..    ...  ...      ...      ...     ...   ...  ...   ...    ...    ...   \n",
       "95  14.62  ...  2388.06  8130.69  8.4311  0.03  392  2388  100.0  38.88   \n",
       "96  14.62  ...  2388.06  8128.74  8.4105  0.03  392  2388  100.0  39.01   \n",
       "97  14.62  ...  2388.06  8127.89  8.4012  0.03  391  2388  100.0  38.96   \n",
       "98  14.62  ...  2388.10  8131.77  8.4481  0.03  393  2388  100.0  38.82   \n",
       "99  14.62  ...  2388.08  8132.49  8.4241  0.03  392  2388  100.0  38.93   \n",
       "\n",
       "        s21  RUL  \n",
       "0   23.4190  191  \n",
       "1   23.4236  190  \n",
       "2   23.3442  189  \n",
       "3   23.3739  188  \n",
       "4   23.4044  187  \n",
       "..      ...  ...  \n",
       "95  23.3255   96  \n",
       "96  23.2963   95  \n",
       "97  23.2554   94  \n",
       "98  23.2323   93  \n",
       "99  23.4090   92  \n",
       "\n",
       "[100 rows x 27 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFd2pLr0odt"
   },
   "source": [
    "> `Labeling for Classification`: This part assigns labels to each data point based on the calculated RUL. It defines thresholds `w1` and `w0`, and assigns:\n",
    ">> * Label 1 ('label1') as 1 if RUL is less than or equal to 'w1', and 0 otherwise.\n",
    ">> * Label2 ('label2') as 1 if RUL is less than or equal to 'w1', 2 if RUL is less than or equal to 'w0', and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "K6wpAOpjyvGl"
   },
   "outputs": [],
   "source": [
    "w1 = 30\n",
    "w0 = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6nJhPwEw1RgE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4195  0.03  392  2388  100.0  39.06  23.4190  191       0   \n",
       "1  14.62  ...  8.4318  0.03  392  2388  100.0  39.00  23.4236  190       0   \n",
       "2  14.62  ...  8.4178  0.03  390  2388  100.0  38.95  23.3442  189       0   \n",
       "3  14.62  ...  8.3682  0.03  392  2388  100.0  38.88  23.3739  188       0   \n",
       "4  14.62  ...  8.4294  0.03  393  2388  100.0  38.90  23.4044  187       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qDzP1TlcGy9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 29)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_8MNc2Z6Ptu"
   },
   "source": [
    "## Now I want to separate the train_df set into a training/validation/test set. I will use 80% training sets for the training and 10% training sets as validation sets for hyperparameter tuning and the remaining 10% as test set for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1718782937087,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Sdm_ZG3E6Ptv"
   },
   "outputs": [],
   "source": [
    "list_ID1 = np.arange(81,91,1) # I take the 20 last #TODO: make this random\n",
    "list_ID2 = np.arange(91,101,1) # I take the 20 last #TODO: make this random\n",
    "list_ID3 = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13rqadVW6Ptv"
   },
   "source": [
    "## I separate into training and validation and test set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1718782937087,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "2nwuKyvt6Ptw"
   },
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID1)]\n",
    "test_df = train_df.loc[train_df['id'].isin(list_ID2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1718782937088,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "X1ncv7M-oe0v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2251, 29)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1718782937088,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "pM4-dDoC6Ptw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2242, 29)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1718782937088,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Q6aoJ3v76Ptx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16138, 29)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID3)]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1718782938033,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "xii2VCBDpOmm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16133</td>\n",
       "      <td>80</td>\n",
       "      <td>181</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.00</td>\n",
       "      <td>1604.26</td>\n",
       "      <td>1428.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5194</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.39</td>\n",
       "      <td>23.1678</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16134</td>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.0014</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.81</td>\n",
       "      <td>1598.32</td>\n",
       "      <td>1426.31</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5493</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.34</td>\n",
       "      <td>23.0142</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16135</td>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1603.37</td>\n",
       "      <td>1434.30</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5106</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.34</td>\n",
       "      <td>22.9337</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16136</td>\n",
       "      <td>80</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.83</td>\n",
       "      <td>1603.01</td>\n",
       "      <td>1430.23</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4983</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.53</td>\n",
       "      <td>22.9971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16137</td>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.88</td>\n",
       "      <td>1606.38</td>\n",
       "      <td>1421.41</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5250</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.35</td>\n",
       "      <td>23.0323</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16138 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0       1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1       1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2       1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3       1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4       1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "16133  80    181    0.0042    0.0002     100.0  518.67  644.00  1604.26   \n",
       "16134  80    182   -0.0014    0.0004     100.0  518.67  643.81  1598.32   \n",
       "16135  80    183    0.0018   -0.0000     100.0  518.67  643.49  1603.37   \n",
       "16136  80    184   -0.0024    0.0002     100.0  518.67  643.83  1603.01   \n",
       "16137  80    185    0.0015   -0.0000     100.0  518.67  643.88  1606.38   \n",
       "\n",
       "            s4     s5  ...     s15   s16  s17   s18    s19    s20      s21  \\\n",
       "0      1400.60  14.62  ...  8.4195  0.03  392  2388  100.0  39.06  23.4190   \n",
       "1      1403.14  14.62  ...  8.4318  0.03  392  2388  100.0  39.00  23.4236   \n",
       "2      1404.20  14.62  ...  8.4178  0.03  390  2388  100.0  38.95  23.3442   \n",
       "3      1401.87  14.62  ...  8.3682  0.03  392  2388  100.0  38.88  23.3739   \n",
       "4      1406.22  14.62  ...  8.4294  0.03  393  2388  100.0  38.90  23.4044   \n",
       "...        ...    ...  ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "16133  1428.92  14.62  ...  8.5194  0.03  397  2388  100.0  38.39  23.1678   \n",
       "16134  1426.31  14.62  ...  8.5493  0.03  396  2388  100.0  38.34  23.0142   \n",
       "16135  1434.30  14.62  ...  8.5106  0.03  397  2388  100.0  38.34  22.9337   \n",
       "16136  1430.23  14.62  ...  8.4983  0.03  397  2388  100.0  38.53  22.9971   \n",
       "16137  1421.41  14.62  ...  8.5250  0.03  397  2388  100.0  38.35  23.0323   \n",
       "\n",
       "       RUL  label1  label2  \n",
       "0      191       0       0  \n",
       "1      190       0       0  \n",
       "2      189       0       0  \n",
       "3      188       0       0  \n",
       "4      187       0       0  \n",
       "...    ...     ...     ...  \n",
       "16133    4       1       2  \n",
       "16134    3       1       2  \n",
       "16135    2       1       2  \n",
       "16136    1       1       2  \n",
       "16137    0       1       2  \n",
       "\n",
       "[16138 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7siRw6Ptx"
   },
   "source": [
    "# Perform the min max scaling on the training data and validation dataset\n",
    "# use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYkFz5FqQplh"
   },
   "source": [
    ">`Create a copy of the cycle column`: This line creates a new column named 'cycle_norm' in the train_df DataFrame and initializes it with the values from the original 'cycle' column. This column will be normalized later.<br>\n",
    "> `Select columns for normalization`: This line selects all columns from **train_df** except 'id', 'cycle', 'RUL', 'label1', and 'label2'. These columns are the ones that will undergo normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1718782938034,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "n_loHDxEQnHc"
   },
   "outputs": [],
   "source": [
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "validation_df['cycle_norm'] = validation_df['cycle']\n",
    "cols_normalize_train = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "cols_normalize_validation = validation_df.columns.difference(['id','cycle','RUL','label1','label2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AHbVv4RgN-"
   },
   "source": [
    "> `Initialize MinMaxScaler`: This line initializes a MinMaxScaler object from the scikit-learn preprocessing module. This scaler will be used to perform Min-Max normalization.<br>\n",
    "> `Perform Min-Max normalization`: This line applies Min-Max normalization to the selected columns (`cols_normalize`) of the `train_df` DataFrame.<br>\n",
    "> `min_max_scaler.fit_transform(train_df[cols_normalize])` computes the Min-Max normalization for the selected columns.<br>\n",
    "> The resulting normalized values are stored in a new DataFrame called `norm_train_df`, with the same index as `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1718782938035,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "yA4WFngxQm4Z"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize_train]),\n",
    "                             columns=cols_normalize_train,\n",
    "                             index=train_df.index)\n",
    "\n",
    "norm_validation_df = pd.DataFrame(min_max_scaler.fit_transform(validation_df[cols_normalize_validation]),\n",
    "                             columns=cols_normalize_validation,\n",
    "                             index=validation_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEf7s4DS3jR"
   },
   "source": [
    "> `Join normalized DataFrame with the original DataFrame`: This line joins the normalized DataFrame (`norm_train_df`) with the original DataFrame (`train_df`) excluding the columns that were normalized.<br>\n",
    "> The resulting DataFrame `join_df` contains both the normalized columns and the original columns that were not normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1718782938036,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "ThKcf5Qf6Ptx"
   },
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "join_df_train = train_df[train_df.columns.difference(cols_normalize_train)].join(norm_train_df)\n",
    "join_df_validation = validation_df[validation_df.columns.difference(cols_normalize_validation)].join(norm_validation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF2-ITehT1_P"
   },
   "source": [
    "`Reorder columns`:\n",
    "> * This line reorders the columns of `join_df` to match the original order of columns in `train_df`.\n",
    "> * The reordered DataFrame is then assigned back to `train_df`, effectively replacing the original DataFrame with the normalized version.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1718782938036,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "BWuYsg8eTyoJ"
   },
   "outputs": [],
   "source": [
    "train_df = join_df_train.reindex(columns = train_df.columns)\n",
    "validation_df = join_df_validation.reindex(columns = validation_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1718782938037,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "MrXBeYUg6Ptx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.425154</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.473456</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.732001</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.386193</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.661565</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387283</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.269082</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.704790</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16133</td>\n",
       "      <td>80</td>\n",
       "      <td>181</td>\n",
       "      <td>0.739884</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840361</td>\n",
       "      <td>0.756892</td>\n",
       "      <td>0.787812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>0.369473</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.498615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16134</td>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16135</td>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.736614</td>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.504155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16136</td>\n",
       "      <td>80</td>\n",
       "      <td>184</td>\n",
       "      <td>0.358382</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.728412</td>\n",
       "      <td>0.809926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291339</td>\n",
       "      <td>0.127551</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.506925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16137</td>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0.583815</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.661040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>0.177438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16138 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0       1      1  0.456647  0.166667       0.0  0.0  0.183735  0.425154   \n",
       "1       1      2  0.606936  0.250000       0.0  0.0  0.283133  0.473456   \n",
       "2       1      3  0.248555  0.750000       0.0  0.0  0.343373  0.386193   \n",
       "3       1      4  0.537572  0.500000       0.0  0.0  0.343373  0.267715   \n",
       "4       1      5  0.387283  0.333333       0.0  0.0  0.349398  0.269082   \n",
       "...    ..    ...       ...       ...       ...  ...       ...       ...   \n",
       "16133  80    181  0.739884  0.666667       0.0  0.0  0.840361  0.756892   \n",
       "16134  80    182  0.416185  0.833333       0.0  0.0  0.783133  0.621554   \n",
       "16135  80    183  0.601156  0.500000       0.0  0.0  0.686747  0.736614   \n",
       "16136  80    184  0.358382  0.666667       0.0  0.0  0.789157  0.728412   \n",
       "16137  80    185  0.583815  0.500000       0.0  0.0  0.804217  0.805195   \n",
       "\n",
       "             s4   s5  ...  s16       s17  s18  s19       s20       s21  RUL  \\\n",
       "0      0.309757  0.0  ...  0.0  0.363636  0.0  0.0  0.708661  0.725482  191   \n",
       "1      0.352633  0.0  ...  0.0  0.363636  0.0  0.0  0.661417  0.732001  190   \n",
       "2      0.370527  0.0  ...  0.0  0.181818  0.0  0.0  0.622047  0.619473  189   \n",
       "3      0.331195  0.0  ...  0.0  0.363636  0.0  0.0  0.566929  0.661565  188   \n",
       "4      0.404625  0.0  ...  0.0  0.454545  0.0  0.0  0.582677  0.704790  187   \n",
       "...         ...  ...  ...  ...       ...  ...  ...       ...       ...  ...   \n",
       "16133  0.787812  0.0  ...  0.0  0.818182  0.0  0.0  0.181102  0.369473    4   \n",
       "16134  0.743754  0.0  ...  0.0  0.727273  0.0  0.0  0.141732  0.151786    3   \n",
       "16135  0.878629  0.0  ...  0.0  0.818182  0.0  0.0  0.141732  0.037698    2   \n",
       "16136  0.809926  0.0  ...  0.0  0.818182  0.0  0.0  0.291339  0.127551    1   \n",
       "16137  0.661040  0.0  ...  0.0  0.818182  0.0  0.0  0.149606  0.177438    0   \n",
       "\n",
       "       label1  label2  cycle_norm  \n",
       "0           0       0    0.000000  \n",
       "1           0       0    0.002770  \n",
       "2           0       0    0.005540  \n",
       "3           0       0    0.008310  \n",
       "4           0       0    0.011080  \n",
       "...       ...     ...         ...  \n",
       "16133       1       2    0.498615  \n",
       "16134       1       2    0.501385  \n",
       "16135       1       2    0.504155  \n",
       "16136       1       2    0.506925  \n",
       "16137       1       2    0.509695  \n",
       "\n",
       "[16138 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1718782938037,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "2FSIVjf6QTOO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16138</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232704</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255663</td>\n",
       "      <td>0.465317</td>\n",
       "      <td>0.402390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.577044</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16139</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453074</td>\n",
       "      <td>0.374969</td>\n",
       "      <td>0.426479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.590098</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16140</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420712</td>\n",
       "      <td>0.379166</td>\n",
       "      <td>0.205242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.613803</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16141</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>0.588250</td>\n",
       "      <td>0.368279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.703226</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16142</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.479635</td>\n",
       "      <td>0.459241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.568942</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18375</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.732905</td>\n",
       "      <td>0.671035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.354989</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.510274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18376</td>\n",
       "      <td>90</td>\n",
       "      <td>151</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>0.587262</td>\n",
       "      <td>0.784159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.219355</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.513699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18377</td>\n",
       "      <td>90</td>\n",
       "      <td>152</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889968</td>\n",
       "      <td>0.780548</td>\n",
       "      <td>0.705338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.217554</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.517123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18378</td>\n",
       "      <td>90</td>\n",
       "      <td>153</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708738</td>\n",
       "      <td>0.944705</td>\n",
       "      <td>0.843130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117117</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.520548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18379</td>\n",
       "      <td>90</td>\n",
       "      <td>154</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.684522</td>\n",
       "      <td>0.930237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225225</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.523973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2242 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "16138  81      1  0.232704  0.750000       0.0  0.0  0.255663  0.465317   \n",
       "16139  81      2  0.691824  0.666667       0.0  0.0  0.453074  0.374969   \n",
       "16140  81      3  0.515723  0.916667       0.0  0.0  0.420712  0.379166   \n",
       "16141  81      4  0.540881  0.500000       0.0  0.0  0.375405  0.588250   \n",
       "16142  81      5  0.698113  0.666667       0.0  0.0  0.699029  0.479635   \n",
       "...    ..    ...       ...       ...       ...  ...       ...       ...   \n",
       "18375  90    150  0.735849  0.416667       0.0  0.0  0.760518  0.732905   \n",
       "18376  90    151  0.666667  0.583333       0.0  0.0  0.773463  0.587262   \n",
       "18377  90    152  0.515723  0.500000       0.0  0.0  0.889968  0.780548   \n",
       "18378  90    153  0.691824  0.666667       0.0  0.0  0.708738  0.944705   \n",
       "18379  90    154  0.484277  0.333333       0.0  0.0  0.760518  0.684522   \n",
       "\n",
       "             s4   s5  ...  s16  s17  s18  s19       s20       s21  RUL  \\\n",
       "16138  0.402390  0.0  ...  0.0  0.2  0.0  0.0  0.612613  0.577044  239   \n",
       "16139  0.426479  0.0  ...  0.0  0.3  0.0  0.0  0.648649  0.590098  238   \n",
       "16140  0.205242  0.0  ...  0.0  0.5  0.0  0.0  0.765766  0.613803  237   \n",
       "16141  0.368279  0.0  ...  0.0  0.3  0.0  0.0  0.522523  0.703226  236   \n",
       "16142  0.459241  0.0  ...  0.0  0.3  0.0  0.0  0.765766  0.568942  235   \n",
       "...         ...  ...  ...  ...  ...  ...  ...       ...       ...  ...   \n",
       "18375  0.671035  0.0  ...  0.0  0.6  0.0  0.0  0.153153  0.354989    4   \n",
       "18376  0.784159  0.0  ...  0.0  0.8  0.0  0.0  0.360360  0.219355    3   \n",
       "18377  0.705338  0.0  ...  0.0  0.6  0.0  0.0  0.189189  0.217554    2   \n",
       "18378  0.843130  0.0  ...  0.0  0.7  0.0  0.0  0.117117  0.023856    1   \n",
       "18379  0.930237  0.0  ...  0.0  0.6  0.0  0.0  0.225225  0.181245    0   \n",
       "\n",
       "       label1  label2  cycle_norm  \n",
       "16138       0       0    0.000000  \n",
       "16139       0       0    0.003425  \n",
       "16140       0       0    0.006849  \n",
       "16141       0       0    0.010274  \n",
       "16142       0       0    0.013699  \n",
       "...       ...     ...         ...  \n",
       "18375       1       2    0.510274  \n",
       "18376       1       2    0.513699  \n",
       "18377       1       2    0.517123  \n",
       "18378       1       2    0.520548  \n",
       "18379       1       2    0.523973  \n",
       "\n",
       "[2242 rows x 30 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1718782938038,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "T5AtglSI1t99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cycle'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1718782938038,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "0F6EWxhp0Qgf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16138, 30)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## Vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1718782938039,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "zSInZu-EkFtf"
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles. This sets the length of the sequence window to 50 cycles.\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1718782938039,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "aAWprb25jV4h"
   },
   "outputs": [],
   "source": [
    "# pick the feature columns, This selects the columns to be included in the sequences.\n",
    "# sensor_cols contains the sensor data columns (s1 to s21).\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "#sequence_cols initially contains the operational settings columns (setting1, setting2, setting3, cycle_norm).\n",
    "# Then, it's extended to include the sensor data columns as well.\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEV0vj7AkJOl"
   },
   "source": [
    "## generate sequences for each engine\n",
    "> * This creates a generator expression that iterates over unique engine IDs in the training data.<br>\n",
    "> * For each engine, it generates sequences using the `gen_sequence` function defined earlier.<br>\n",
    "> * Each sequence is a list of sensor data, and multiple sequences are generated for each engine.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1718782943605,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "gAsGvlwBjVpe"
   },
   "outputs": [],
   "source": [
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "seq_gen_validation = (list(gen_sequence(validation_df[validation_df['id']==id], sequence_length, sequence_cols))\n",
    "           for id in validation_df['id'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1718782945408,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "tVitGXG9LMMa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7f96f81ab550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Fc99j1rPQa"
   },
   "source": [
    "> * This concatenates all the generated sequences into a single numpy array.\n",
    "> * It converts the array to `float32` data type.\n",
    "> * The resulting `seq_array` contains the sequences of sensor data, with shape `(num_sequences, sequence_length, num_features)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1718782945409,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "6C8e0SuHjVbh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 50, 25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782946742,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "TL1mtjFEVvdN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1742, 50, 25)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences and convert to numpy array\n",
    "seq_array_validation = np.concatenate(list(seq_gen_validation)).astype(np.float32)\n",
    "seq_array_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718782947175,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "TR4RCn98H8Fu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seq_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782947475,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "RUUWP-h9IAnU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5833333 , 0.9166667 , 0.5833333 , 0.9166667 , 0.41666666],\n",
       "       [0.9166667 , 0.5833333 , 0.9166667 , 0.41666666, 0.75      ],\n",
       "       [0.5833333 , 0.9166667 , 0.41666666, 0.75      , 0.41666666],\n",
       "       ...,\n",
       "       [0.5       , 0.25      , 0.33333334, 0.6666667 , 0.8333333 ],\n",
       "       [0.25      , 0.33333334, 0.6666667 , 0.8333333 , 0.5       ],\n",
       "       [0.33333334, 0.6666667 , 0.8333333 , 0.5       , 0.6666667 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_array[:,-5:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782949253,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "An8AqEX_6Pty"
   },
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu8ocOhxkaf"
   },
   "source": [
    "`Function Signature:` This function efficiently generates labels for each sequence of sensor data. It ensures that the labels are correctly aligned with the sequences and handles the special case where the first sequence uses the last label as its target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This function takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (id).<br>\n",
    ">> * `seq_length`: Length of the sequence window.<br>\n",
    ">> * `label`: List of column names representing the labels.\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[label].values:`\n",
    ">> * This line extracts the columns specified by label from the DataFrame id_df and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant label(s) needed for generating sequences.<br>\n",
    "\n",
    "`Label Generation:`\n",
    "> * `num_elements:`This line calculates the number of rows (elements) in the data matrix, which corresponds to the number of labels.<br>\n",
    "> * `return data_matrix[seq_length:num_elements, :]:`\n",
    ">> * This line returns the labels associated with each sequence.<br>\n",
    ">> * It removes the first `seq_length` labels because, for each engine (`id`), the first sequence of size `seq_length` uses the last label as its target. The previous labels are discarded.<br>\n",
    ">> * All subsequent sequences for the same engine (`id`) will have one label associated with them step by step.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1718782953828,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "xIDbZQrJ6Pty",
    "outputId": "9cbbcb2a-9316-45e5-fa12-cd6bcfd79402"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12138, 1), (1742, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels for training and validation set\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label2'])\n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "label_gen_validation = [gen_labels(validation_df[validation_df['id']==id], sequence_length, ['label2'])\n",
    "             for id in validation_df['id'].unique()]\n",
    "label_array_validation = np.concatenate(label_gen_validation).astype(np.float32)\n",
    "label_array.shape, label_array_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782953829,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "pjniT37X6Ptz"
   },
   "outputs": [],
   "source": [
    "# When modeling multi-class classification problems using neural networks,\n",
    "# it is good practice to reshape the output attribute from a vector that contains values for each class value to be\n",
    "# a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "# This is called one hot encoding or creating dummy variables from a categorical variable.\n",
    "\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming your array is a numpy array named `data`\n",
    "data = np.array([label_array]).reshape(-1)  # Ensure it is a 1D array of shape (12138,)\n",
    "data_validation = np.array([label_array_validation]).reshape(-1)  # Ensure it is a 1D array of shape (12138,)\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "data_tensor_validation = torch.tensor(data_validation, dtype=torch.long)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "dummy_label_array = torch.nn.functional.one_hot(data_tensor)\n",
    "dummy_label_array_validation = torch.nn.functional.one_hot(data_tensor_validation)\n",
    "\n",
    "dummy_label_array=np.array(dummy_label_array, dtype=np.int64)  # Convert input to NumPy array\n",
    "dummy_label_array_validation=np.array(dummy_label_array_validation, dtype=np.int64)  # Convert input to NumPy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qBrRZdYZiHb"
   },
   "source": [
    "`to_categorical` is a utility function in Keras that converts class vectors (integers) to binary class matrices.<br>\n",
    "`dummy_label_array = to_categorical(label_array):`This line applies one-hot encoding to the `label_array`.<br>\n",
    "`label_array` contains the labels associated with each sequence, where each label represents a class or category.<br>\n",
    "> * One-hot encoding converts these integer labels into binary vectors, where each vector has a length equal to the number of classes and contains a 1 in the position corresponding to the class and 0s elsewhere.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1718782957915,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "HEpXlgujjc11",
    "outputId": "9176232f-c60c-4315-8502-6b5c692c559a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782961167,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "5Z61CGUr4-po",
    "outputId": "3746dfcd-9fa9-4b3b-f86a-ee202d88fcbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782963095,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "ST9VHStw5SYv",
    "outputId": "b70726c4-2712-4d13-ea88-240b09bcb9c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782965252,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "XnwyiGj06Pt0",
    "outputId": "b319666d-4293-4940-843e-a3bce535053c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1718782966954,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "qbAJg5KqY-In",
    "outputId": "22d56de8-00e6-4671-b0cd-d47c0e24276c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782969359,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "M40CNdn7krRw",
    "outputId": "9df2380d-c7b1-43df-cb41-ec51fb81c98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782971301,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "dWhWIVl56Pt1",
    "outputId": "a9a57d55-f589-4306-954f-0d36aed732c2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "nb_features, nb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0h3i_FJg7pJ"
   },
   "source": [
    "`Extracting Feature and Output Dimensions:`\n",
    "> `nb_features:`Determines the number of features in the input sequence data.<br>\n",
    "> `nb_out:`Determines the number of output classes. It's extracted from the shape of the label array.<br>\n",
    "\n",
    "`Defining the Model Architecture:` describe in the code below.\n",
    "`Compiling the Model:` `model.compile(...)` Here, `categorical_crossentropy` is used as the loss function for multi-class classification.\n",
    "\n",
    "`Model Summary:`Prints a summary of the model architecture, including the layers and their parameters.\n",
    "\n",
    "`Training the Model:` `model.fit(...):` Trains the model on the training data. It specifies the input data (`seq_array`) and the corresponding labels (`dummy_label_array`). Other parameters include the number of epochs, batch size, validation split, verbosity, and callbacks.<br>\n",
    "\n",
    "\n",
    "`history.history.keys():` After training, this prints the keys of the history object, which contains information about training and validation metrics over each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOmuAimTDi5p"
   },
   "source": [
    "### Define the Dataset:\n",
    "Create a custom dataset class to handle your multivariate time series data with labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1718791179567,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "C5EsCykBKvsT",
    "outputId": "ebb36239-3bab-4478-9e21-ea72b12ac2ba"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#os.chdir('/content/drive/MyDrive/Colab Notebooks/LSTM_Antonis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6baBqY-Oli3"
   },
   "source": [
    "`Shuffling Batches:` By setting `shuffle=True` in the `DataLoader` for the training set, you ensure that the order of batches is shuffled each epoch. This maintains the temporal structure within each batch while still introducing variability in the order in which batches are processed.<br>\n",
    "\n",
    "`DataLoader for Validation:` Ensure `shuffle=False` for the validation set to maintain the sequence order during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1718791182861,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "8SbqCLucDib9"
   },
   "outputs": [],
   "source": [
    "# Import custom classes\n",
    "from dataset import MultivariateTimeSeriesDataset\n",
    "from model import TransformerTimeSeriesModel\n",
    "\n",
    "# Load parameters from JSON file\n",
    "with open('params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "# Example usage with seq_array and dummy_var\n",
    "#seq_array --> (12138, 50, 25)  # use this as your actual data\n",
    "#dummy_var --->  (12138, 3) # use this as your actual dummy variable (3 classes)\n",
    "\n",
    "train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "# Shuffle batches by setting shuffle=True in DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQmBGIESY4qE"
   },
   "source": [
    "### Define the Transformer Model:\n",
    "Implement the vanilla Transformer architecture, ensuring it takes the dummy variable as an input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_dummy_expanded shape: (1742, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# Expand val_dummy to match the sequence length of val_data\n",
    "val_dummy_expanded = np.repeat(dummy_label_array_validation[:, np.newaxis, :], params['seq_length'], axis=1)\n",
    "\n",
    "print(f'val_dummy_expanded shape: {val_dummy_expanded.shape}')  # Should print: val_dummy_expanded shape: (1742, 50, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model using parameters from the JSON file\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim=params['input_dim'],\n",
    "    model_dim=params['model_dim'],\n",
    "    num_heads=params['num_heads'],\n",
    "    num_layers=params['num_layers'],\n",
    "    seq_length=params['seq_length'],\n",
    "    num_classes=params['num_classes'],\n",
    "    dropout_rate=params['dropout_rate']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeriesModel(\n",
       "  (input_embedding): Linear(in_features=25, out_features=16, bias=True)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_output): Linear(in_features=16, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47411"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV8tK1SNV2e-"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Define the training loop with the loss function and optimizer, and include the dummy variable in the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruction_criterion = nn.MSELoss()\n",
    "torch.optim \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "# Learning rate scheduler with ReduceLROnPlateau for early stopping\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=params['patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 0.6528, Val Loss: 0.4284, Val Accuracy: 0.8404\n",
      "Epoch 2/25, Train Loss: 0.3505, Val Loss: 0.3156, Val Accuracy: 0.8722\n",
      "Epoch 3/25, Train Loss: 0.2545, Val Loss: 0.2329, Val Accuracy: 0.9104\n",
      "Epoch 4/25, Train Loss: 0.1938, Val Loss: 0.6657, Val Accuracy: 0.8065\n",
      "Epoch 5/25, Train Loss: 0.1749, Val Loss: 0.2728, Val Accuracy: 0.8869\n",
      "Epoch 6/25, Train Loss: 0.1380, Val Loss: 0.1257, Val Accuracy: 0.9426\n",
      "Epoch 7/25, Train Loss: 0.1429, Val Loss: 0.1672, Val Accuracy: 0.9150\n",
      "Epoch 8/25, Train Loss: 0.1388, Val Loss: 0.1423, Val Accuracy: 0.9300\n",
      "Epoch 9/25, Train Loss: 0.1279, Val Loss: 0.4079, Val Accuracy: 0.8536\n",
      "Epoch 10/25, Train Loss: 0.1537, Val Loss: 0.1208, Val Accuracy: 0.9432\n",
      "Epoch 11/25, Train Loss: 0.1312, Val Loss: 0.3890, Val Accuracy: 0.8657\n",
      "Epoch 12/25, Train Loss: 0.1270, Val Loss: 0.2641, Val Accuracy: 0.8972\n",
      "Epoch 13/25, Train Loss: 0.1364, Val Loss: 0.1831, Val Accuracy: 0.9214\n",
      "Epoch 14/25, Train Loss: 0.1216, Val Loss: 0.3113, Val Accuracy: 0.8949\n",
      "Epoch 15/25, Train Loss: 0.1222, Val Loss: 0.1021, Val Accuracy: 0.9518\n",
      "Epoch 16/25, Train Loss: 0.1172, Val Loss: 0.2676, Val Accuracy: 0.8984\n",
      "Epoch 17/25, Train Loss: 0.1266, Val Loss: 0.2107, Val Accuracy: 0.9150\n",
      "Epoch 18/25, Train Loss: 0.1260, Val Loss: 0.1799, Val Accuracy: 0.9242\n",
      "Epoch 19/25, Train Loss: 0.1246, Val Loss: 0.1211, Val Accuracy: 0.9403\n",
      "Epoch 20/25, Train Loss: 0.1174, Val Loss: 0.1808, Val Accuracy: 0.9139\n",
      "Epoch 21/25, Train Loss: 0.1126, Val Loss: 0.2530, Val Accuracy: 0.9018\n",
      "Epoch 22/25, Train Loss: 0.1185, Val Loss: 0.3059, Val Accuracy: 0.8944\n",
      "Epoch 23/25, Train Loss: 0.1200, Val Loss: 0.3717, Val Accuracy: 0.8611\n",
      "Epoch 24/25, Train Loss: 0.1138, Val Loss: 0.1983, Val Accuracy: 0.9214\n",
      "Epoch 25/25, Train Loss: 0.1241, Val Loss: 0.2109, Val Accuracy: 0.9099\n",
      "Trial 1/10 completed. Best Accuracy: 0.9518\n",
      "Epoch 1/16, Train Loss: 0.6574, Val Loss: 0.5875, Val Accuracy: 0.8220\n",
      "Epoch 2/16, Train Loss: 0.6472, Val Loss: 0.5936, Val Accuracy: 0.8220\n",
      "Epoch 3/16, Train Loss: 0.6420, Val Loss: 0.5850, Val Accuracy: 0.8220\n",
      "Epoch 4/16, Train Loss: 0.4396, Val Loss: 0.2591, Val Accuracy: 0.8909\n",
      "Epoch 5/16, Train Loss: 0.1950, Val Loss: 0.1755, Val Accuracy: 0.9298\n",
      "Epoch 6/16, Train Loss: 0.1551, Val Loss: 0.1095, Val Accuracy: 0.9552\n",
      "Epoch 7/16, Train Loss: 0.1438, Val Loss: 0.1684, Val Accuracy: 0.9249\n",
      "Epoch 8/16, Train Loss: 0.1400, Val Loss: 0.2732, Val Accuracy: 0.8986\n",
      "Epoch 9/16, Train Loss: 0.1216, Val Loss: 0.2773, Val Accuracy: 0.9244\n",
      "Epoch 10/16, Train Loss: 0.1230, Val Loss: 0.2951, Val Accuracy: 0.9208\n",
      "Epoch 11/16, Train Loss: 0.1116, Val Loss: 0.2082, Val Accuracy: 0.9216\n",
      "Epoch 12/16, Train Loss: 0.1118, Val Loss: 0.1891, Val Accuracy: 0.9261\n",
      "Epoch 13/16, Train Loss: 0.1141, Val Loss: 0.3213, Val Accuracy: 0.9030\n",
      "Epoch 14/16, Train Loss: 0.1055, Val Loss: 0.3472, Val Accuracy: 0.9055\n",
      "Epoch 15/16, Train Loss: 0.1078, Val Loss: 0.1315, Val Accuracy: 0.9474\n",
      "Epoch 16/16, Train Loss: 0.1102, Val Loss: 0.3514, Val Accuracy: 0.8848\n",
      "Trial 2/10 completed. Best Accuracy: 0.9552\n",
      "Epoch 1/9, Train Loss: 0.6547, Val Loss: 0.5915, Val Accuracy: 0.8220\n",
      "Epoch 2/9, Train Loss: 0.6481, Val Loss: 0.5899, Val Accuracy: 0.8220\n",
      "Epoch 3/9, Train Loss: 0.6460, Val Loss: 0.5872, Val Accuracy: 0.8220\n",
      "Epoch 4/9, Train Loss: 0.6451, Val Loss: 0.5873, Val Accuracy: 0.8220\n",
      "Epoch 5/9, Train Loss: 0.6435, Val Loss: 0.5877, Val Accuracy: 0.8220\n",
      "Epoch 6/9, Train Loss: 0.6421, Val Loss: 0.5918, Val Accuracy: 0.8220\n",
      "Epoch 7/9, Train Loss: 0.6421, Val Loss: 0.5871, Val Accuracy: 0.8220\n",
      "Epoch 8/9, Train Loss: 0.6419, Val Loss: 0.5922, Val Accuracy: 0.8220\n",
      "Epoch 9/9, Train Loss: 0.6410, Val Loss: 0.5878, Val Accuracy: 0.8220\n",
      "Trial 3/10 completed. Best Accuracy: 0.8220\n",
      "Epoch 1/24, Train Loss: 0.6029, Val Loss: 0.3136, Val Accuracy: 0.8634\n",
      "Epoch 2/24, Train Loss: 0.2227, Val Loss: 0.1787, Val Accuracy: 0.9229\n",
      "Epoch 3/24, Train Loss: 0.1625, Val Loss: 0.3407, Val Accuracy: 0.8969\n",
      "Epoch 4/24, Train Loss: 0.1410, Val Loss: 0.2604, Val Accuracy: 0.8840\n",
      "Epoch 5/24, Train Loss: 0.1347, Val Loss: 0.3520, Val Accuracy: 0.8733\n",
      "Epoch 6/24, Train Loss: 0.1383, Val Loss: 0.2061, Val Accuracy: 0.9061\n",
      "Epoch 7/24, Train Loss: 0.1232, Val Loss: 0.2794, Val Accuracy: 0.9142\n",
      "Epoch 8/24, Train Loss: 0.1269, Val Loss: 0.3310, Val Accuracy: 0.8860\n",
      "Epoch 9/24, Train Loss: 0.1244, Val Loss: 0.3494, Val Accuracy: 0.8869\n",
      "Epoch 10/24, Train Loss: 0.1232, Val Loss: 0.1180, Val Accuracy: 0.9505\n",
      "Epoch 11/24, Train Loss: 0.1298, Val Loss: 0.1702, Val Accuracy: 0.9155\n",
      "Epoch 12/24, Train Loss: 0.1157, Val Loss: 0.3090, Val Accuracy: 0.8798\n",
      "Epoch 13/24, Train Loss: 0.1238, Val Loss: 0.2526, Val Accuracy: 0.9140\n",
      "Epoch 14/24, Train Loss: 0.1096, Val Loss: 0.3241, Val Accuracy: 0.8798\n",
      "Epoch 15/24, Train Loss: 0.1091, Val Loss: 0.3704, Val Accuracy: 0.8828\n",
      "Epoch 16/24, Train Loss: 0.1214, Val Loss: 0.3237, Val Accuracy: 0.8629\n",
      "Epoch 17/24, Train Loss: 0.1068, Val Loss: 0.2425, Val Accuracy: 0.9205\n",
      "Epoch 18/24, Train Loss: 0.1133, Val Loss: 0.2193, Val Accuracy: 0.9037\n",
      "Epoch 19/24, Train Loss: 0.1183, Val Loss: 0.1116, Val Accuracy: 0.9579\n",
      "Epoch 20/24, Train Loss: 0.1222, Val Loss: 0.2378, Val Accuracy: 0.8953\n",
      "Epoch 21/24, Train Loss: 0.1076, Val Loss: 0.1221, Val Accuracy: 0.9515\n",
      "Epoch 22/24, Train Loss: 0.1062, Val Loss: 0.2571, Val Accuracy: 0.8938\n",
      "Epoch 23/24, Train Loss: 0.1181, Val Loss: 0.7406, Val Accuracy: 0.7859\n",
      "Epoch 24/24, Train Loss: 0.1149, Val Loss: 0.2578, Val Accuracy: 0.9066\n",
      "Trial 4/10 completed. Best Accuracy: 0.9579\n",
      "Epoch 1/19, Train Loss: 0.3410, Val Loss: 0.2485, Val Accuracy: 0.8990\n",
      "Epoch 2/19, Train Loss: 0.1675, Val Loss: 0.2792, Val Accuracy: 0.9092\n",
      "Epoch 3/19, Train Loss: 0.1199, Val Loss: 0.2021, Val Accuracy: 0.9233\n",
      "Epoch 4/19, Train Loss: 0.1154, Val Loss: 0.3272, Val Accuracy: 0.8869\n",
      "Epoch 5/19, Train Loss: 0.1006, Val Loss: 0.2991, Val Accuracy: 0.9059\n",
      "Epoch 6/19, Train Loss: 0.0958, Val Loss: 0.2669, Val Accuracy: 0.9036\n",
      "Epoch 7/19, Train Loss: 0.0883, Val Loss: 0.3665, Val Accuracy: 0.9035\n",
      "Epoch 8/19, Train Loss: 0.0968, Val Loss: 0.2029, Val Accuracy: 0.9085\n",
      "Epoch 9/19, Train Loss: 0.0897, Val Loss: 0.2576, Val Accuracy: 0.9059\n",
      "Epoch 10/19, Train Loss: 0.0833, Val Loss: 0.4986, Val Accuracy: 0.8897\n",
      "Epoch 11/19, Train Loss: 0.0843, Val Loss: 0.2773, Val Accuracy: 0.9115\n",
      "Epoch 12/19, Train Loss: 0.0788, Val Loss: 0.4070, Val Accuracy: 0.8909\n",
      "Epoch 13/19, Train Loss: 0.0788, Val Loss: 0.2764, Val Accuracy: 0.9225\n",
      "Epoch 14/19, Train Loss: 0.0766, Val Loss: 0.4379, Val Accuracy: 0.8919\n",
      "Epoch 15/19, Train Loss: 0.0757, Val Loss: 0.3948, Val Accuracy: 0.8994\n",
      "Epoch 16/19, Train Loss: 0.0809, Val Loss: 0.5177, Val Accuracy: 0.8812\n",
      "Epoch 17/19, Train Loss: 0.0754, Val Loss: 0.3699, Val Accuracy: 0.9070\n",
      "Epoch 18/19, Train Loss: 0.0771, Val Loss: 0.4815, Val Accuracy: 0.8840\n",
      "Epoch 19/19, Train Loss: 0.0776, Val Loss: 0.4394, Val Accuracy: 0.8877\n",
      "Trial 5/10 completed. Best Accuracy: 0.9233\n",
      "Epoch 1/14, Train Loss: 0.4006, Val Loss: 0.2371, Val Accuracy: 0.8938\n",
      "Epoch 2/14, Train Loss: 0.1818, Val Loss: 0.2547, Val Accuracy: 0.8947\n",
      "Epoch 3/14, Train Loss: 0.1358, Val Loss: 0.1255, Val Accuracy: 0.9561\n",
      "Epoch 4/14, Train Loss: 0.1319, Val Loss: 0.2344, Val Accuracy: 0.9162\n",
      "Epoch 5/14, Train Loss: 0.1316, Val Loss: 0.1735, Val Accuracy: 0.9277\n",
      "Epoch 6/14, Train Loss: 0.1208, Val Loss: 0.2111, Val Accuracy: 0.9259\n",
      "Epoch 7/14, Train Loss: 0.1259, Val Loss: 0.3347, Val Accuracy: 0.9111\n",
      "Epoch 8/14, Train Loss: 0.1111, Val Loss: 0.1909, Val Accuracy: 0.9256\n",
      "Epoch 9/14, Train Loss: 0.1318, Val Loss: 0.2609, Val Accuracy: 0.9106\n",
      "Epoch 10/14, Train Loss: 0.1070, Val Loss: 0.1760, Val Accuracy: 0.9294\n",
      "Epoch 11/14, Train Loss: 0.1208, Val Loss: 0.1805, Val Accuracy: 0.9271\n",
      "Epoch 12/14, Train Loss: 0.1126, Val Loss: 0.1663, Val Accuracy: 0.9347\n",
      "Epoch 13/14, Train Loss: 0.1169, Val Loss: 0.1805, Val Accuracy: 0.9173\n",
      "Epoch 14/14, Train Loss: 0.1181, Val Loss: 0.2340, Val Accuracy: 0.9097\n",
      "Trial 6/10 completed. Best Accuracy: 0.9561\n",
      "Epoch 1/30, Train Loss: 0.6575, Val Loss: 0.6116, Val Accuracy: 0.8220\n",
      "Epoch 2/30, Train Loss: 0.6464, Val Loss: 0.5960, Val Accuracy: 0.8220\n",
      "Epoch 3/30, Train Loss: 0.6411, Val Loss: 0.5869, Val Accuracy: 0.8220\n",
      "Epoch 4/30, Train Loss: 0.6403, Val Loss: 0.5926, Val Accuracy: 0.8220\n",
      "Epoch 5/30, Train Loss: 0.6403, Val Loss: 0.5884, Val Accuracy: 0.8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 0.6401, Val Loss: 0.5920, Val Accuracy: 0.8220\n",
      "Epoch 7/30, Train Loss: 0.6404, Val Loss: 0.5887, Val Accuracy: 0.8220\n",
      "Epoch 8/30, Train Loss: 0.6405, Val Loss: 0.5878, Val Accuracy: 0.8220\n",
      "Epoch 9/30, Train Loss: 0.6405, Val Loss: 0.5922, Val Accuracy: 0.8220\n",
      "Epoch 10/30, Train Loss: 0.6403, Val Loss: 0.5901, Val Accuracy: 0.8220\n",
      "Epoch 11/30, Train Loss: 0.6405, Val Loss: 0.5908, Val Accuracy: 0.8220\n",
      "Epoch 12/30, Train Loss: 0.6407, Val Loss: 0.5886, Val Accuracy: 0.8220\n",
      "Epoch 13/30, Train Loss: 0.6405, Val Loss: 0.5869, Val Accuracy: 0.8220\n",
      "Epoch 14/30, Train Loss: 0.6400, Val Loss: 0.5909, Val Accuracy: 0.8220\n",
      "Epoch 15/30, Train Loss: 0.6406, Val Loss: 0.5925, Val Accuracy: 0.8220\n",
      "Epoch 16/30, Train Loss: 0.6411, Val Loss: 0.5869, Val Accuracy: 0.8220\n",
      "Epoch 17/30, Train Loss: 0.6404, Val Loss: 0.5881, Val Accuracy: 0.8220\n",
      "Epoch 18/30, Train Loss: 0.6407, Val Loss: 0.5910, Val Accuracy: 0.8220\n",
      "Epoch 19/30, Train Loss: 0.6408, Val Loss: 0.5878, Val Accuracy: 0.8220\n",
      "Epoch 20/30, Train Loss: 0.6421, Val Loss: 0.5886, Val Accuracy: 0.8220\n",
      "Epoch 21/30, Train Loss: 0.6400, Val Loss: 0.5902, Val Accuracy: 0.8220\n",
      "Epoch 22/30, Train Loss: 0.6406, Val Loss: 0.5920, Val Accuracy: 0.8220\n",
      "Epoch 23/30, Train Loss: 0.6412, Val Loss: 0.5894, Val Accuracy: 0.8220\n",
      "Epoch 24/30, Train Loss: 0.6404, Val Loss: 0.5889, Val Accuracy: 0.8220\n",
      "Epoch 25/30, Train Loss: 0.6403, Val Loss: 0.5898, Val Accuracy: 0.8220\n",
      "Epoch 26/30, Train Loss: 0.6410, Val Loss: 0.5884, Val Accuracy: 0.8220\n",
      "Epoch 27/30, Train Loss: 0.6409, Val Loss: 0.5899, Val Accuracy: 0.8220\n",
      "Epoch 28/30, Train Loss: 0.6412, Val Loss: 0.5876, Val Accuracy: 0.8220\n",
      "Epoch 29/30, Train Loss: 0.6404, Val Loss: 0.5876, Val Accuracy: 0.8220\n",
      "Epoch 30/30, Train Loss: 0.6410, Val Loss: 0.5878, Val Accuracy: 0.8220\n",
      "Trial 7/10 completed. Best Accuracy: 0.8220\n",
      "Epoch 1/23, Train Loss: 0.6496, Val Loss: 0.4061, Val Accuracy: 0.8220\n",
      "Epoch 2/23, Train Loss: 0.3176, Val Loss: 0.2469, Val Accuracy: 0.8864\n",
      "Epoch 3/23, Train Loss: 0.1873, Val Loss: 0.2707, Val Accuracy: 0.8772\n",
      "Epoch 4/23, Train Loss: 0.1533, Val Loss: 0.3359, Val Accuracy: 0.8778\n",
      "Epoch 5/23, Train Loss: 0.1354, Val Loss: 0.2124, Val Accuracy: 0.9093\n",
      "Epoch 6/23, Train Loss: 0.1258, Val Loss: 0.2548, Val Accuracy: 0.8996\n",
      "Epoch 7/23, Train Loss: 0.1179, Val Loss: 0.1837, Val Accuracy: 0.9170\n",
      "Epoch 8/23, Train Loss: 0.1136, Val Loss: 0.3900, Val Accuracy: 0.8631\n",
      "Epoch 9/23, Train Loss: 0.1223, Val Loss: 0.1813, Val Accuracy: 0.9204\n",
      "Epoch 10/23, Train Loss: 0.1219, Val Loss: 0.2968, Val Accuracy: 0.9022\n",
      "Epoch 11/23, Train Loss: 0.1118, Val Loss: 0.3782, Val Accuracy: 0.8756\n",
      "Epoch 12/23, Train Loss: 0.1138, Val Loss: 0.2128, Val Accuracy: 0.9230\n",
      "Epoch 13/23, Train Loss: 0.1166, Val Loss: 0.2944, Val Accuracy: 0.8952\n",
      "Epoch 14/23, Train Loss: 0.1119, Val Loss: 0.5033, Val Accuracy: 0.8492\n",
      "Epoch 15/23, Train Loss: 0.1161, Val Loss: 0.1816, Val Accuracy: 0.9195\n",
      "Epoch 16/23, Train Loss: 0.1221, Val Loss: 0.2150, Val Accuracy: 0.8956\n",
      "Epoch 17/23, Train Loss: 0.1157, Val Loss: 0.3428, Val Accuracy: 0.8968\n",
      "Epoch 18/23, Train Loss: 0.1125, Val Loss: 0.2527, Val Accuracy: 0.8991\n",
      "Epoch 19/23, Train Loss: 0.1079, Val Loss: 0.1823, Val Accuracy: 0.9264\n",
      "Epoch 20/23, Train Loss: 0.1101, Val Loss: 0.2132, Val Accuracy: 0.9064\n",
      "Epoch 21/23, Train Loss: 0.1153, Val Loss: 0.2524, Val Accuracy: 0.9004\n",
      "Epoch 22/23, Train Loss: 0.1119, Val Loss: 0.3182, Val Accuracy: 0.8752\n",
      "Epoch 23/23, Train Loss: 0.1006, Val Loss: 0.2013, Val Accuracy: 0.9231\n",
      "Trial 8/10 completed. Best Accuracy: 0.9264\n",
      "Epoch 1/16, Train Loss: 0.4818, Val Loss: 0.2940, Val Accuracy: 0.8773\n",
      "Epoch 2/16, Train Loss: 0.2678, Val Loss: 0.2049, Val Accuracy: 0.9155\n",
      "Epoch 3/16, Train Loss: 0.1806, Val Loss: 0.2008, Val Accuracy: 0.9214\n",
      "Epoch 4/16, Train Loss: 0.1420, Val Loss: 0.2428, Val Accuracy: 0.9168\n",
      "Epoch 5/16, Train Loss: 0.1265, Val Loss: 0.3329, Val Accuracy: 0.8874\n",
      "Epoch 6/16, Train Loss: 0.1095, Val Loss: 0.3287, Val Accuracy: 0.9132\n",
      "Epoch 7/16, Train Loss: 0.1124, Val Loss: 0.1905, Val Accuracy: 0.9320\n",
      "Epoch 8/16, Train Loss: 0.1047, Val Loss: 0.5063, Val Accuracy: 0.8869\n",
      "Epoch 9/16, Train Loss: 0.1034, Val Loss: 0.2910, Val Accuracy: 0.9101\n",
      "Epoch 10/16, Train Loss: 0.0968, Val Loss: 0.3172, Val Accuracy: 0.9063\n",
      "Epoch 11/16, Train Loss: 0.0988, Val Loss: 0.2833, Val Accuracy: 0.9128\n",
      "Epoch 12/16, Train Loss: 0.0989, Val Loss: 0.4734, Val Accuracy: 0.8639\n",
      "Epoch 13/16, Train Loss: 0.1026, Val Loss: 0.2687, Val Accuracy: 0.9186\n",
      "Epoch 14/16, Train Loss: 0.0915, Val Loss: 0.2252, Val Accuracy: 0.9248\n",
      "Epoch 15/16, Train Loss: 0.0961, Val Loss: 0.4292, Val Accuracy: 0.8921\n",
      "Epoch 16/16, Train Loss: 0.0944, Val Loss: 0.3707, Val Accuracy: 0.8904\n",
      "Trial 9/10 completed. Best Accuracy: 0.9320\n",
      "Epoch 1/16, Train Loss: 0.5593, Val Loss: 0.3428, Val Accuracy: 0.8410\n",
      "Epoch 2/16, Train Loss: 0.2771, Val Loss: 0.2935, Val Accuracy: 0.8732\n",
      "Epoch 3/16, Train Loss: 0.2349, Val Loss: 0.1428, Val Accuracy: 0.9440\n",
      "Epoch 4/16, Train Loss: 0.3780, Val Loss: 0.3608, Val Accuracy: 0.8354\n",
      "Epoch 5/16, Train Loss: 0.2374, Val Loss: 0.2920, Val Accuracy: 0.9154\n",
      "Epoch 6/16, Train Loss: 0.2330, Val Loss: 0.2070, Val Accuracy: 0.9066\n",
      "Epoch 7/16, Train Loss: 0.2028, Val Loss: 0.3306, Val Accuracy: 0.8772\n",
      "Epoch 8/16, Train Loss: 0.2029, Val Loss: 0.1454, Val Accuracy: 0.9369\n",
      "Epoch 9/16, Train Loss: 0.1736, Val Loss: 0.1370, Val Accuracy: 0.9471\n",
      "Epoch 10/16, Train Loss: 0.1600, Val Loss: 0.1253, Val Accuracy: 0.9500\n",
      "Epoch 11/16, Train Loss: 0.1664, Val Loss: 0.1029, Val Accuracy: 0.9655\n",
      "Epoch 12/16, Train Loss: 0.1573, Val Loss: 0.1376, Val Accuracy: 0.9449\n",
      "Epoch 13/16, Train Loss: 0.1531, Val Loss: 0.1451, Val Accuracy: 0.9411\n",
      "Epoch 14/16, Train Loss: 0.1561, Val Loss: 0.2016, Val Accuracy: 0.9118\n",
      "Epoch 15/16, Train Loss: 0.1503, Val Loss: 0.3331, Val Accuracy: 0.8565\n",
      "Epoch 16/16, Train Loss: 0.1542, Val Loss: 0.1906, Val Accuracy: 0.9093\n",
      "Trial 10/10 completed. Best Accuracy: 0.9655\n",
      "Best Parameters:\n",
      "{\n",
      "    \"input_dim\": 25,\n",
      "    \"model_dim\": 16,\n",
      "    \"num_heads\": 2,\n",
      "    \"num_layers\": 3,\n",
      "    \"seq_length\": 50,\n",
      "    \"num_classes\": 3,\n",
      "    \"dropout_rate\": 0.2868566573951219,\n",
      "    \"learning_rate\": 0.008660297626080152,\n",
      "    \"batch_size\": 4,\n",
      "    \"num_epochs\": 16,\n",
      "    \"weight_decay\": 0.0008257506721260046\n",
      "}\n",
      "Best Accuracy: 0.9655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, accumulation_steps=4):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs_class = model(inputs)\n",
    "                outputs_reshaped = outputs_class.view(-1, params['num_classes'])\n",
    "                labels_reshaped = labels.view(-1, params['num_classes']).argmax(dim=1)\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            # Clear cache every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs_class = model(inputs)\n",
    "                    outputs_reshaped = outputs_class.view(-1, params['num_classes'])\n",
    "                    labels_reshaped = labels.view(-1, params['num_classes']).argmax(dim=1)\n",
    "                    val_loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs_class, dim=2)\n",
    "                all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Clear cache after each epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "def random_search(num_trials=10):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        # Randomly sample hyperparameters\n",
    "        params = {\n",
    "            \"input_dim\": 25,\n",
    "            \"model_dim\": random.choice([16, 32, 64]),  # Reduced max model_dim\n",
    "            \"num_heads\": random.choice([1, 2, 4]),  # Reduced max num_heads\n",
    "            \"num_layers\": random.randint(1, 4),  # Reduced max num_layers\n",
    "            \"seq_length\": 50,\n",
    "            \"num_classes\": 3,\n",
    "            \"dropout_rate\": random.uniform(0.1, 0.5),\n",
    "            \"learning_rate\": random.uniform(0.0001, 0.01),\n",
    "            \"batch_size\": random.choice([4, 8, 16]),  # Reduced batch sizes\n",
    "            \"num_epochs\": random.randint(5, 30),  # Reduced max epochs\n",
    "            \"weight_decay\": random.uniform(0.0001, 0.001)\n",
    "        }\n",
    "\n",
    "        model_params = {k: v for k, v in params.items() if k not in ['learning_rate', 'batch_size', 'num_epochs', 'weight_decay']}\n",
    "        model = TransformerTimeSeriesModel(**model_params)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "        # Create data loaders with smaller batch size\n",
    "        train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "        val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_losses, val_losses, val_accuracies = train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, params['num_epochs'], device)\n",
    "\n",
    "        best_trial_accuracy = max(val_accuracies)\n",
    "\n",
    "        if best_trial_accuracy > best_accuracy:\n",
    "            best_accuracy = best_trial_accuracy\n",
    "            best_params = params\n",
    "\n",
    "        print(f\"Trial {trial+1}/{num_trials} completed. Best Accuracy: {best_trial_accuracy:.4f}\")\n",
    "\n",
    "        # Clear cache after each trial\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Run random search\n",
    "best_params, best_accuracy = random_search()\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\")\n",
    "print(json.dumps(best_params, indent=4))\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save best parameters to file\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "i--r71JmV1LD"
   },
   "outputs": [],
   "source": [
    "# Load parameters from JSON file\n",
    "with open('params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Set up datasets\n",
    "train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# Set up model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_params = {k: v for k, v in params.items() if k not in ['learning_rate', 'batch_size', 'num_epochs', 'weight_decay']}\n",
    "model = TransformerTimeSeriesModel(**model_params).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "\n",
    "num_epochs = params['num_epochs']\n",
    "train_losses = []\n",
    "val_losses  = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, dummies = batch\n",
    "        inputs = inputs.float().to(device)\n",
    "        dummies = dummies.float().to(device)\n",
    "\n",
    "        outputs_class = model(inputs)\n",
    "\n",
    "        outputs_reshaped = outputs_class.view(-1, params['num_classes'])\n",
    "        dummies_reshaped = dummies.view(-1, params['num_classes'])\n",
    "        loss = criterion(outputs_reshaped, dummies_reshaped.argmax(dim=1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(average_epoch_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {average_epoch_loss}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, dummies = batch\n",
    "            inputs = inputs.float().to(device)\n",
    "            dummies = dummies.float().to(device)\n",
    "\n",
    "            outputs_class = model(inputs)\n",
    "\n",
    "            outputs_reshaped = outputs_class.view(-1, params['num_classes'])\n",
    "            dummies_reshaped = dummies.view(-1, params['num_classes'])\n",
    "            val_loss = criterion(outputs_reshaped, dummies_reshaped.argmax(dim=1))\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "    \n",
    "    scheduler.step(val_epoch_loss)\n",
    "\n",
    "    average_val_loss = val_epoch_loss / len(val_loader)\n",
    "    val_losses.append(average_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {average_val_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pLcEa2SD3yY-"
   },
   "outputs": [],
   "source": [
    "# Plot train and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "model = TransformerTimeSeriesModel(**params)  # Initialize your model\n",
    "model.load_state_dict(torch.load('multiclass_model_w1_30.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "r9k0p5-46Pt1"
   },
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers import TransformerEncoder, TransformerDecoder\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Next, we build a deep network.\n",
    "# The first layer is a TransformerEncoder layer followed by a TransformerDecoder layer.\n",
    "# Dropout is also applied after each layer to control overfitting.\n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "\n",
    "# Initializes a sequential model, which allows you to build a model layer by layer.\n",
    "model = Sequential()\n",
    "\n",
    "# Adds a TransformerEncoder layer to the model.\n",
    "model.add(TransformerEncoder(\n",
    "    head_size=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=256,\n",
    "    dropout=0.2,\n",
    "    input_shape=(sequence_length, nb_features)\n",
    "))\n",
    "\n",
    "# Adds a TransformerDecoder layer to the model.\n",
    "model.add(TransformerDecoder(\n",
    "    head_size=64\n",
    "    num_heads=8,\n",
    "    ff_dim=256,\n",
    "    dropout=0.2\n",
    "))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adds a dense (fully connected) layer to the model with a softmax activation function. This layer produces the output classes.\n",
    "model.add(Dense(units=nb_out, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, dummy_label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sWk2GlE6Pt2"
   },
   "source": [
    "# Every time I retrain the algorithm I get different training results, i.e., also different evaluation of the decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set created during the training (i.e., validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpVYSzXkmk5l"
   },
   "outputs": [],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u2aW0bj6Pt3"
   },
   "outputs": [],
   "source": [
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, dummy_label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK6St5XcVyd9"
   },
   "source": [
    "`y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\"):` Predicts the abels for the input sequences using the trained model. The predictions are hresholded at 0.5, meaning that any output probability greater than 0.5 is considered as class 1, otherwise class 0. The predictions are then converted to integers (0 or 1).<br>\n",
    "\n",
    "`y_true = dummy_label_array:` Sets the true labels from the dummy label array, which represents the actual labels of the data.\n",
    "then print the **confusion_matrix**\n",
    "\n",
    "`cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1)):` Computes the confusion matrix using the true labels (`y_true`) and the predicted labels `(y_pred). argmax(axis=1)` is used to convert one-hot encoded labels back to their original integer form before computing the confusion matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRwaYKfV6Pt4"
   },
   "outputs": [],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "# y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\") # this way (>0.5) the outcome goes from a probability to 0,1\n",
    "y_true = dummy_label_array\n",
    "\n",
    "# test_set = pd.DataFrame(y_pred)\n",
    "# # test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Second PdM policy evaluation on the test set.\n",
    "\n",
    "For each test set, I need to give the on-line sensor data as input to the trained Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_GqqVKV6Pt4"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vloF6HXQ6Pt4"
   },
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 1000\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ-vs1-x6Pt5"
   },
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KP5yjWw6Pt5"
   },
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqIzk2b96Pt5"
   },
   "outputs": [],
   "source": [
    "test_df['cycle_norm'] = test_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAF7B0eOY1BN"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBklUcnv6Pt5"
   },
   "source": [
    "## Second PdM policy evaluation on a the whole validation data set (ids 81 to 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfyabzdL6Pt_"
   },
   "outputs": [],
   "source": [
    "costs_rep_array   = np.zeros(20) # An array to store costs related to replacements.\n",
    "\n",
    "costs_delay_array = np.zeros(20) # An array to store costs related to delays.\n",
    "costs_stock_array = np.zeros(20) # An array to store costs related to stock.\n",
    "\n",
    "t_LC_array        = np.zeros(20) # An array to store lead time.\n",
    "t_order_array     = np.zeros(20) # An array to store order time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsWT1ebGbOdz"
   },
   "source": [
    "> 1. Initializes a counter variable to 0.\n",
    "> 2. Iterates over unique IDs in the `test_df` DataFrame.\n",
    "> 3. For each ID:\n",
    ">> * Sets flags for preventive replacement and ordering to False.<br>\n",
    ">> * Iterates over cycles within the range of the DataFrame.<br>\n",
    ">> * Checks if the current cycle is in the `array_decisions`.<br>\n",
    ">> * If it is, preprocesses the validation data for the LSTM model.<br>\n",
    ">> * Predicts the probability of RUL being smaller than w1 and DT (decision time) using the trained model.<br>\n",
    ">> * Evaluates decision heuristics:\n",
    ">>> * If no order has been placed yet and the cost of preventive replacement is less than or equal to the cost of waiting until `w1`, orders the component and sets the order time.<br>\n",
    ">>> * If the cost of preventive replacement is less than or equal to the cost of waiting until `DT`, performs preventive replacement, calculates related costs, and breaks the loop.<br>\n",
    ">> If preventive replacement is not performed:\n",
    ">>> * Sets the component failure time to the last cycle in the ID's data.<br>\n",
    ">>> * Sets replacement costs to `C_c`.<br>\n",
    ">>> * Calculates delay costs based on whether an order has been placed.<br>\n",
    ">> * Prints diagnostic information for each iteration.\n",
    ">> * Increments the counter.\n",
    "\n",
    "\n",
    "This code essentially simulates a decision-making process for component maintenance based on predictive models and cost considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG8Lip5w6Pt_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for id in test_df['id'].unique():\n",
    "    print('ID:', id)\n",
    "    preventive_replacement = False\n",
    "    order                  = False\n",
    "\n",
    "    for cycle in range(test_df[test_df['id']==id].shape[0]-sequence_length+1):\n",
    "\n",
    "        if cycle in array_decisions:\n",
    "\n",
    "            norm_validation_df = pd.DataFrame(min_max_scaler.transform(test_df[test_df['id']==id][cols_normalize][:sequence_length+cycle]),\n",
    "                 columns=cols_normalize,\n",
    "                 index=test_df[test_df['id']==id][:sequence_length+cycle].index)\n",
    "\n",
    "            join_df = test_df[test_df['id']==id][:sequence_length+cycle][test_df[test_df['id']==id][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "            validation_df_eval_online = join_df.reindex(columns = test_df[test_df['id']==id][cycle:sequence_length+cycle].columns)\n",
    "\n",
    "            seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "            seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "            prob_RUL_smaller_DT    = estimator.predict(seq_array_validation_k).reshape(3)[2]\n",
    "            prob_RUL_smaller_w1    = estimator.predict(seq_array_validation_k).reshape(3)[1]\n",
    "\n",
    "            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "\n",
    "            # evaluate decision heuristics\n",
    "            if order == False:\n",
    "                if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                    t_order_array[counter] = sequence_length+cycle\n",
    "                    order = True\n",
    "                    print('component ordering at cycle:', t_order_array[counter])\n",
    "\n",
    "            if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                t_LC_array[counter] = sequence_length+cycle\n",
    "                costs_rep_array[counter] = C_p\n",
    "                print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                # print('component lifecycle:', t_LC)\n",
    "                preventive_replacement = True\n",
    "                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "\n",
    "                costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "                # print('delay time', max(t_order+L-t_LC, 0))\n",
    "                # print('cost_delay_id:',cost_delay_id)\n",
    "                # print('cost of stock:', cost_stock_id)\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        t_LC_array[counter] = test_df[test_df['id']==id]['cycle'].iloc[-1]\n",
    "        print('Component failure at t:', t_LC_array[counter])\n",
    "        costs_rep_array[counter] = C_c\n",
    "\n",
    "        if order == False:\n",
    "            costs_delay_array[counter] = L * C_unav\n",
    "        else:\n",
    "            costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "            costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "\n",
    "    print('True failure:', test_df[test_df['id']==id]['cycle'].iloc[-1])\n",
    "    print('-----------------------------------------')\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onea6sLe6PuA"
   },
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAvJN15m6PuA"
   },
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mmc0YIA6PuA"
   },
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkJNf5d26PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot = costs_rep_array+costs_delay_array+costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp4AF1KW6PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzXo4Qlw6PuB"
   },
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNUa2J36PuB"
   },
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6g7k8Yrftya"
   },
   "source": [
    "### This code calculates the expected cost per unit time using the LSTM model. It computes the mean of the total costs divided by the mean of the time to component failure (t_LC_array). This metric gives an estimate of the average cost incurred per unit time in the system, considering both maintenance and operational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3_N4kNM6PuC"
   },
   "outputs": [],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgoTz57gC_P"
   },
   "source": [
    "This code segment calculates the expected cost per unit time assuming perfect prognostics.\n",
    "`1. Perfect Prognostics Calculation:`\n",
    "> * It initializes an array `t_LC_perfect_array` to store the time of component failure for each unit in the validation dataset. This is calculated by dividing the last observed cycle number by the decision interval DT and then flooring the result to get the last decision cycle before failure.\n",
    "> * The loop iterates over each unique ID in the validation dataset, calculates the time of component failure for each unit, and stores it in\n",
    "`t_LC_perfect_array`.<br>\n",
    "> * `math.floor()` is used to round down the result to the nearest multiple of `DT`.\n",
    "> * Finally, the loop increments the counter for each unit.<br>\n",
    "\n",
    "`2. Cost Calculation:`\n",
    "> * `costs_perfect_array` is initialized with a value of `C_p`, representing the cost of preventive replacements. In a perfect scenario, only preventive replacements are made.\n",
    "> * This array holds the same cost value for each unit in the validation dataset.\n",
    "\n",
    "`3. Expected Cost Calculation:`\n",
    "> * `expected_cost_perfect` is calculated by taking the mean of `costs_perfect_array` and dividing it by the mean of `t_LC_perfect_array`.\n",
    "> * This calculation provides an estimate of the average cost per unit time assuming perfect prognostics, where components are replaced preventively at regular intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOD6NDfH6PuC"
   },
   "outputs": [],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT\n",
    "    counter+=1\n",
    "\n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-vK-DBi6PuC"
   },
   "outputs": [],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7_LJce6PuC"
   },
   "outputs": [],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KofjvavY6PuD"
   },
   "outputs": [],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42Nu-Rg6PuE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
