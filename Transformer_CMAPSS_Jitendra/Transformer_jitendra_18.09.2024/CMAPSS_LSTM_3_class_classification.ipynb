{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26732,
     "status": "ok",
     "timestamp": 1718627582985,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "70-XsHkDGUKu",
    "outputId": "6bb24682-dbae-4c4e-be23-bdb60fe62dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.24.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.23.3)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (66.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from packaging->tensorflow-intel==2.14.0->tensorflow) (3.0.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (6.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tml5lib (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -llib3 (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\envs\\ml2023\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19128,
     "status": "ok",
     "timestamp": 1718627615090,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "3wOjKm186Pth",
    "outputId": "42183fc5-359b-4a36-ec2e-4e7ee208179f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3063099736348261380\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718627615091,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "4BsYPkY7gYGx"
   },
   "outputs": [],
   "source": [
    "# Multiclass classification\n",
    "#Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iklamKYlNjh"
   },
   "source": [
    "`tensorflow.keras`: This imports the Keras module from TensorFlow, which provides an API for building and training deep learning models.<br>\n",
    "`os`: This module provides functions for interacting with the operating system. It's commonly used for tasks such as file manipulation and directory operations.<br>\n",
    "`sklearn.preprocessing`: This module from scikit-learn provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables.<br>\n",
    "`sklearn.metrics`: This module contains functions for evaluating model performance, such as computing confusion matrices, recall scores, and precision scores.<br>\n",
    "`tensorflow.keras.models.Sequential`: This class from Keras represents a sequential model, which is a linear stack of layers. It's used for building feedforward neural networks.<br>\n",
    "`tensorflow.keras.models.load_model`: This function is used to load a pre-trained Keras model from a file.<br>\n",
    "`tensorflow.keras.layers`: This module contains various types of layers that can be added to a Keras model, such as dense (fully connected) layers, dropout layers, and LSTM layers.\n",
    "`multiclass_model_w1_30.h5`:The .h5 extension indicates that the model will be saved in the Hierarchical Data Format version 5 (HDF5) format, which is commonly used for storing large numerical datasets. The model will be saved with the filename **multiclass_model_w1_30.h5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1718627620493,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1718627651010,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "JjbfnUZGgc3C",
    "outputId": "6d96f27d-6b20-4bec-a2be-2ed504a55ca0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1       2       3      4       5       6        7        8      9   \\\n",
       "0   1   1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60  14.62   \n",
       "1   1   2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14  14.62   \n",
       "2   1   3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20  14.62   \n",
       "3   1   4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87  14.62   \n",
       "4   1   5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22  14.62   \n",
       "\n",
       "   ...       18      19    20   21    22     23     24       25  26  27  \n",
       "0  ...  8138.62  8.4195  0.03  392  2388  100.0  39.06  23.4190 NaN NaN  \n",
       "1  ...  8131.49  8.4318  0.03  392  2388  100.0  39.00  23.4236 NaN NaN  \n",
       "2  ...  8133.23  8.4178  0.03  390  2388  100.0  38.95  23.3442 NaN NaN  \n",
       "3  ...  8133.83  8.3682  0.03  392  2388  100.0  38.88  23.3739 NaN NaN  \n",
       "4  ...  8133.80  8.4294  0.03  393  2388  100.0  38.90  23.4044 NaN NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1718627653067,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "XBjr189aTPt1",
    "outputId": "4c02043a-513d-4edd-cd67-6ea664a41de7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpsNnInsevi"
   },
   "source": [
    "`train_df.sort_values(['id','cycle'])`: This line sorts the DataFrame **train_df** first by the 'id' column and then by the 'cycle' column. It ensures that the data is ordered by engine ID and cycle number, which may be necessary for certain analyses or modeling tasks. The sorted DataFrame is then assigned back to the variable **train_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1718627659356,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "mo0v9RsVriPz"
   },
   "outputs": [],
   "source": [
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1718627661504,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "E7x-rVZz6Ptq",
    "outputId": "0c2cc3b7-5974-4b39-d653-d08f95eb88bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.49</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.68</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>520.01</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.67</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.30</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
       "0      1400.60  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1      1403.14  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2      1404.20  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3      1401.87  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4      1406.22  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "...        ...    ...  ...     ...      ...      ...     ...   ...  ...   ...   \n",
       "20626  1428.63  14.62  ...  519.49  2388.26  8137.60  8.4956  0.03  397  2388   \n",
       "20627  1433.58  14.62  ...  519.68  2388.22  8136.50  8.5139  0.03  395  2388   \n",
       "20628  1428.18  14.62  ...  520.01  2388.24  8141.05  8.5646  0.03  398  2388   \n",
       "20629  1426.53  14.62  ...  519.67  2388.23  8139.29  8.5389  0.03  395  2388   \n",
       "20630  1432.14  14.62  ...  519.30  2388.26  8137.33  8.5036  0.03  396  2388   \n",
       "\n",
       "         s19    s20      s21  \n",
       "0      100.0  39.06  23.4190  \n",
       "1      100.0  39.00  23.4236  \n",
       "2      100.0  38.95  23.3442  \n",
       "3      100.0  38.88  23.3739  \n",
       "4      100.0  38.90  23.4044  \n",
       "...      ...    ...      ...  \n",
       "20626  100.0  38.49  22.9735  \n",
       "20627  100.0  38.30  23.1594  \n",
       "20628  100.0  38.44  22.9333  \n",
       "20629  100.0  38.29  23.0640  \n",
       "20630  100.0  38.37  23.0522  \n",
       "\n",
       "[20631 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing\n",
    "data preprocessing step, particularly for labeling the data for training purposes. Let's break down what each part of the code does:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-_dxq60Nf4"
   },
   "source": [
    ">`Data Labeling`: This part calculates the Remaining Useful Life (RUL) or Time to Failure for each engine by finding the maximum cycle number (cycle) for each engine ID (id). The result is stored in a DataFrame rul with columns 'id' and 'max'.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1718627665071,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718627666649,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "BnqCery8yupl",
    "outputId": "a8a61e79-462b-411c-83fb-fce3cb8f1957"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle\n",
       "0   1    192\n",
       "1   2    287\n",
       "2   3    179\n",
       "3   4    189\n",
       "4   5    269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rul.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZubPMTD0T9z"
   },
   "source": [
    ">`Merge RUL with Training Data`:the RUL information is merged back into the original training DataFrame **train_df** based on the engine ID. This allows each row in train_df to have the corresponding maximum cycle number as well.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1718627669592,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "6Ycs7i7Qyu32"
   },
   "outputs": [],
   "source": [
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718627670706,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "AroVzMsuzHhp",
    "outputId": "b0765883-fee3-4fec-d93b-305ecb7a939f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...      s13      s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
       "1  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
       "2  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
       "3  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
       "4  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
       "\n",
       "       s21  max  \n",
       "0  23.4190  192  \n",
       "1  23.4236  192  \n",
       "2  23.3442  192  \n",
       "3  23.3739  192  \n",
       "4  23.4044  192  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-eouxGv0gpG"
   },
   "source": [
    ">`Calculate RUL`: This line calculates the RUL by subtracting the current cycle number ('cycle') from the maximum cycle number ('max') for each engine. This represents how many more cycles the engine is expected to operate before failure.<br>\n",
    ">`Drop Unnecessary Columns`: After calculating RUL, the 'max' column, which was used temporarily to calculate RUL, is dropped from the DataFrame as it's no longer needed.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1718627674524,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "vulNLE0CztxT",
    "outputId": "c91c3d5a-e954-49a4-9148-e01114d896e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.19</td>\n",
       "      <td>1584.07</td>\n",
       "      <td>1395.16</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.69</td>\n",
       "      <td>8.4311</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3255</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.07</td>\n",
       "      <td>1595.77</td>\n",
       "      <td>1407.81</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8128.74</td>\n",
       "      <td>8.4105</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.01</td>\n",
       "      <td>23.2963</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.00</td>\n",
       "      <td>1591.11</td>\n",
       "      <td>1404.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8127.89</td>\n",
       "      <td>8.4012</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.2554</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1592.73</td>\n",
       "      <td>1406.13</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>8131.77</td>\n",
       "      <td>8.4481</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.82</td>\n",
       "      <td>23.2323</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.22</td>\n",
       "      <td>1589.63</td>\n",
       "      <td>1411.35</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8132.49</td>\n",
       "      <td>8.4241</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.93</td>\n",
       "      <td>23.4090</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0    1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1    1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2    1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3    1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4    1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "..  ..    ...       ...       ...       ...     ...     ...      ...      ...   \n",
       "95   1     96   -0.0034    0.0001     100.0  518.67  642.19  1584.07  1395.16   \n",
       "96   1     97    0.0035   -0.0003     100.0  518.67  642.07  1595.77  1407.81   \n",
       "97   1     98    0.0006    0.0004     100.0  518.67  642.00  1591.11  1404.56   \n",
       "98   1     99   -0.0005   -0.0000     100.0  518.67  642.46  1592.73  1406.13   \n",
       "99   1    100   -0.0021   -0.0003     100.0  518.67  642.22  1589.63  1411.35   \n",
       "\n",
       "       s5  ...      s13      s14     s15   s16  s17   s18    s19    s20  \\\n",
       "0   14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
       "1   14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
       "2   14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
       "3   14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
       "4   14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
       "..    ...  ...      ...      ...     ...   ...  ...   ...    ...    ...   \n",
       "95  14.62  ...  2388.06  8130.69  8.4311  0.03  392  2388  100.0  38.88   \n",
       "96  14.62  ...  2388.06  8128.74  8.4105  0.03  392  2388  100.0  39.01   \n",
       "97  14.62  ...  2388.06  8127.89  8.4012  0.03  391  2388  100.0  38.96   \n",
       "98  14.62  ...  2388.10  8131.77  8.4481  0.03  393  2388  100.0  38.82   \n",
       "99  14.62  ...  2388.08  8132.49  8.4241  0.03  392  2388  100.0  38.93   \n",
       "\n",
       "        s21  RUL  \n",
       "0   23.4190  191  \n",
       "1   23.4236  190  \n",
       "2   23.3442  189  \n",
       "3   23.3739  188  \n",
       "4   23.4044  187  \n",
       "..      ...  ...  \n",
       "95  23.3255   96  \n",
       "96  23.2963   95  \n",
       "97  23.2554   94  \n",
       "98  23.2323   93  \n",
       "99  23.4090   92  \n",
       "\n",
       "[100 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFd2pLr0odt"
   },
   "source": [
    "> `Labeling for Classification`: This part assigns labels to each data point based on the calculated RUL. It defines thresholds `w1` and `w0`, and assigns:\n",
    ">> * Label 1 ('label1') as 1 if RUL is less than or equal to 'w1', and 0 otherwise.\n",
    ">> * Label2 ('label2') as 1 if RUL is less than or equal to 'w1', 2 if RUL is less than or equal to 'w0', and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1718627687767,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "K6wpAOpjyvGl"
   },
   "outputs": [],
   "source": [
    "w1 = 30\n",
    "w0 = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1718627689346,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "6nJhPwEw1RgE",
    "outputId": "7f353f5c-749d-40cb-a975-21873c79143f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4195  0.03  392  2388  100.0  39.06  23.4190  191       0   \n",
       "1  14.62  ...  8.4318  0.03  392  2388  100.0  39.00  23.4236  190       0   \n",
       "2  14.62  ...  8.4178  0.03  390  2388  100.0  38.95  23.3442  189       0   \n",
       "3  14.62  ...  8.3682  0.03  392  2388  100.0  38.88  23.3739  188       0   \n",
       "4  14.62  ...  8.4294  0.03  393  2388  100.0  38.90  23.4044  187       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_8MNc2Z6Ptu"
   },
   "source": [
    "## Now I want to separate the training set into a training and validation set. I will use 80 training sets for the training and 20 training sets as evaluation sets for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1718627694382,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Sdm_ZG3E6Ptv"
   },
   "outputs": [],
   "source": [
    "list_ID = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13rqadVW6Ptv"
   },
   "source": [
    "## I separate into training and validation set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1718627811514,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "2nwuKyvt6Ptw"
   },
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1718627815227,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "pM4-dDoC6Ptw",
    "outputId": "5d16f5b8-b639-404c-f09a-ce7349ae3148"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16141</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4493 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "16138   81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91   \n",
       "16139   81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25   \n",
       "16140   81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42   \n",
       "16141   81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89   \n",
       "16142   81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s15   s16  s17   s18    s19    s20      s21  \\\n",
       "16138  1406.63  14.62  ...  8.4455  0.03  391  2388  100.0  38.87  23.3365   \n",
       "16139  1407.88  14.62  ...  8.4573  0.03  392  2388  100.0  38.91  23.3452   \n",
       "16140  1396.40  14.62  ...  8.4522  0.03  394  2388  100.0  39.04  23.3610   \n",
       "16141  1404.86  14.62  ...  8.4403  0.03  392  2388  100.0  38.77  23.4206   \n",
       "16142  1409.58  14.62  ...  8.3971  0.03  392  2388  100.0  39.04  23.3311   \n",
       "...        ...    ...  ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "20626  1428.63  14.62  ...  8.4956  0.03  397  2388  100.0  38.49  22.9735   \n",
       "20627  1433.58  14.62  ...  8.5139  0.03  395  2388  100.0  38.30  23.1594   \n",
       "20628  1428.18  14.62  ...  8.5646  0.03  398  2388  100.0  38.44  22.9333   \n",
       "20629  1426.53  14.62  ...  8.5389  0.03  395  2388  100.0  38.29  23.0640   \n",
       "20630  1432.14  14.62  ...  8.5036  0.03  396  2388  100.0  38.37  23.0522   \n",
       "\n",
       "       RUL  label1  label2  \n",
       "16138  239       0       0  \n",
       "16139  238       0       0  \n",
       "16140  237       0       0  \n",
       "16141  236       0       0  \n",
       "16142  235       0       0  \n",
       "...    ...     ...     ...  \n",
       "20626    4       1       2  \n",
       "20627    3       1       2  \n",
       "20628    2       1       2  \n",
       "20629    1       1       2  \n",
       "20630    0       1       2  \n",
       "\n",
       "[4493 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1718627818022,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Q6aoJ3v76Ptx"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1718627820336,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "PyrZZu6_uCJS",
    "outputId": "52f0d9a5-f604-48b0-e839-6c7a3b120c71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16138, 29), (4493, 29))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7siRw6Ptx"
   },
   "source": [
    "# Perform the min max scaling of the training data set\n",
    "# use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYkFz5FqQplh"
   },
   "source": [
    ">`Create a copy of the cycle column`: This line creates a new column named 'cycle_norm' in the train_df DataFrame and initializes it with the values from the original 'cycle' column. This column will be normalized later.<br>\n",
    "> `Select columns for normalization`: This line selects all columns from **train_df** except 'id', 'cycle', 'RUL', 'label1', and 'label2'. These columns are the ones that will undergo normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "n_loHDxEQnHc"
   },
   "outputs": [],
   "source": [
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cycle_norm', 's1', 's10', 's11', 's12', 's13', 's14', 's15', 's16',\n",
       "       's17', 's18', 's19', 's2', 's20', 's21', 's3', 's4', 's5', 's6', 's7',\n",
       "       's8', 's9', 'setting1', 'setting2', 'setting3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AHbVv4RgN-"
   },
   "source": [
    "> `Initialize MinMaxScaler`: This line initializes a MinMaxScaler object from the scikit-learn preprocessing module. This scaler will be used to perform Min-Max normalization.<br>\n",
    "> `Perform Min-Max normalization`: This line applies Min-Max normalization to the selected columns (`cols_normalize`) of the `train_df` DataFrame.<br>\n",
    "> `min_max_scaler.fit_transform(train_df[cols_normalize])` computes the Min-Max normalization for the selected columns.<br>\n",
    "> The resulting normalized values are stored in a new DataFrame called `norm_train_df`, with the same index as `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yA4WFngxQm4Z"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEf7s4DS3jR"
   },
   "source": [
    "> `Join normalized DataFrame with the original DataFrame`: This line joins the normalized DataFrame (`norm_train_df`) with the original DataFrame (`train_df`) excluding the columns that were normalized.<br>\n",
    "> The resulting DataFrame `join_df` contains both the normalized columns and the original columns that were not normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ThKcf5Qf6Ptx"
   },
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF2-ITehT1_P"
   },
   "source": [
    "`Reorder columns`:\n",
    "> * This line reorders the columns of `join_df` to match the original order of columns in `train_df`.\n",
    "> * The reordered DataFrame is then assigned back to `train_df`, effectively replacing the original DataFrame with the normalized version.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BWuYsg8eTyoJ"
   },
   "outputs": [],
   "source": [
    "train_df = join_df.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MrXBeYUg6Ptx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.425154</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.473456</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.732001</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.386193</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.661565</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387283</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.269082</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.704790</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>80</td>\n",
       "      <td>181</td>\n",
       "      <td>0.739884</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840361</td>\n",
       "      <td>0.756892</td>\n",
       "      <td>0.787812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>0.369473</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.498615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.736614</td>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.504155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16136</th>\n",
       "      <td>80</td>\n",
       "      <td>184</td>\n",
       "      <td>0.358382</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.728412</td>\n",
       "      <td>0.809926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291339</td>\n",
       "      <td>0.127551</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.506925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0.583815</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.661040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>0.177438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16138 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0       1      1  0.456647  0.166667       0.0  0.0  0.183735  0.425154   \n",
       "1       1      2  0.606936  0.250000       0.0  0.0  0.283133  0.473456   \n",
       "2       1      3  0.248555  0.750000       0.0  0.0  0.343373  0.386193   \n",
       "3       1      4  0.537572  0.500000       0.0  0.0  0.343373  0.267715   \n",
       "4       1      5  0.387283  0.333333       0.0  0.0  0.349398  0.269082   \n",
       "...    ..    ...       ...       ...       ...  ...       ...       ...   \n",
       "16133  80    181  0.739884  0.666667       0.0  0.0  0.840361  0.756892   \n",
       "16134  80    182  0.416185  0.833333       0.0  0.0  0.783133  0.621554   \n",
       "16135  80    183  0.601156  0.500000       0.0  0.0  0.686747  0.736614   \n",
       "16136  80    184  0.358382  0.666667       0.0  0.0  0.789157  0.728412   \n",
       "16137  80    185  0.583815  0.500000       0.0  0.0  0.804217  0.805195   \n",
       "\n",
       "             s4   s5  ...  s16       s17  s18  s19       s20       s21  RUL  \\\n",
       "0      0.309757  0.0  ...  0.0  0.363636  0.0  0.0  0.708661  0.725482  191   \n",
       "1      0.352633  0.0  ...  0.0  0.363636  0.0  0.0  0.661417  0.732001  190   \n",
       "2      0.370527  0.0  ...  0.0  0.181818  0.0  0.0  0.622047  0.619473  189   \n",
       "3      0.331195  0.0  ...  0.0  0.363636  0.0  0.0  0.566929  0.661565  188   \n",
       "4      0.404625  0.0  ...  0.0  0.454545  0.0  0.0  0.582677  0.704790  187   \n",
       "...         ...  ...  ...  ...       ...  ...  ...       ...       ...  ...   \n",
       "16133  0.787812  0.0  ...  0.0  0.818182  0.0  0.0  0.181102  0.369473    4   \n",
       "16134  0.743754  0.0  ...  0.0  0.727273  0.0  0.0  0.141732  0.151786    3   \n",
       "16135  0.878629  0.0  ...  0.0  0.818182  0.0  0.0  0.141732  0.037698    2   \n",
       "16136  0.809926  0.0  ...  0.0  0.818182  0.0  0.0  0.291339  0.127551    1   \n",
       "16137  0.661040  0.0  ...  0.0  0.818182  0.0  0.0  0.149606  0.177438    0   \n",
       "\n",
       "       label1  label2  cycle_norm  \n",
       "0           0       0    0.000000  \n",
       "1           0       0    0.002770  \n",
       "2           0       0    0.005540  \n",
       "3           0       0    0.008310  \n",
       "4           0       0    0.011080  \n",
       "...       ...     ...         ...  \n",
       "16133       1       2    0.498615  \n",
       "16134       1       2    0.501385  \n",
       "16135       1       2    0.504155  \n",
       "16136       1       2    0.506925  \n",
       "16137       1       2    0.509695  \n",
       "\n",
       "[16138 rows x 30 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T5AtglSI1t99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "74I2omLtQLLs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LaPWraKepMF"
   },
   "source": [
    "`Define a function to generate sequences:`\n",
    "> * This function generates sequences of sensor data for each engine (`id`) in the dataset.\n",
    "> * It takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (`id`).\n",
    ">> * `seq_length:` Length of the sequence window.\n",
    ">> * `seq_cols:` List of column names to include in the sequences.\n",
    "> * It iterates over the rows of `id_df`, creating sequences of length seq_length without padding.<br>\n",
    "> * It ensures that only sequences of the specified length are considered, which is important for sequence-based modeling tasks.<br>\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[seq_cols].values:`\n",
    ">> * This line extracts the columns specified by `seq_cols` from the DataFrame `id_df` and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant columns needed for creating sequences.\n",
    "\n",
    "`Sequence Generation:`\n",
    "> * `num_elements:` This line calculates the number of rows (elements) in the data_matrix.<br>\n",
    "> `for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):`\n",
    ">> * This is a loop that iterates over the rows of the data matrix to create sequences.<br>\n",
    ">> * It uses the `zip` function to iterate over two parallel lists: one list starts from 0 to `num_elements` - `seq_length`, and the other starts from `seq_length` to `num_elements`.<br>\n",
    ">> * This ensures that sequences of length `seq_length` are generated without overlap.<br>\n",
    ">> * `start` and `stop` define the start and end indices of each sequence.<br>\n",
    "> `yield data_matrix[start:stop, :]:`\n",
    ">> * This line yields (returns) each sequence as a 2D numpy array.<br>\n",
    ">> * It uses array slicing to extract the rows corresponding to the current sequence range.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zSInZu-EkFtf"
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles. This sets the length of the sequence window to 50 cycles.\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aAWprb25jV4h"
   },
   "outputs": [],
   "source": [
    "# pick the feature columns, This selects the columns to be included in the sequences.\n",
    "# sensor_cols contains the sensor data columns (s1 to s21).\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "#sequence_cols initially contains the operational settings columns (setting1, setting2, setting3, cycle_norm).\n",
    "# Then, it's extended to include the sensor data columns as well.\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEV0vj7AkJOl"
   },
   "source": [
    "## generate sequences for each engine\n",
    "> * This creates a generator expression that iterates over unique engine IDs in the training data.<br>\n",
    "> * For each engine, it generates sequences using the `gen_sequence` function defined earlier.<br>\n",
    "> * Each sequence is a list of sensor data, and multiple sequences are generated for each engine.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gAsGvlwBjVpe"
   },
   "outputs": [],
   "source": [
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Fc99j1rPQa"
   },
   "source": [
    "> * This concatenates all the generated sequences into a single numpy array.\n",
    "> * It converts the array to `float32` data type.\n",
    "> * The resulting `seq_array` contains the sequences of sensor data, with shape `(num_sequences, sequence_length, num_features)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6C8e0SuHjVbh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 50, 25)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "An8AqEX_6Pty"
   },
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu8ocOhxkaf"
   },
   "source": [
    "`Function Signature:` This function efficiently generates labels for each sequence of sensor data. It ensures that the labels are correctly aligned with the sequences and handles the special case where the first sequence uses the last label as its target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This function takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (id).<br>\n",
    ">> * `seq_length`: Length of the sequence window.<br>\n",
    ">> * `label`: List of column names representing the labels.\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[label].values:`\n",
    ">> * This line extracts the columns specified by label from the DataFrame id_df and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant label(s) needed for generating sequences.<br>\n",
    "\n",
    "`Label Generation:`\n",
    "> * `num_elements:`This line calculates the number of rows (elements) in the data matrix, which corresponds to the number of labels.<br>\n",
    "> * `return data_matrix[seq_length:num_elements, :]:`\n",
    ">> * This line returns the labels associated with each sequence.<br>\n",
    ">> * It removes the first `seq_length` labels because, for each engine (`id`), the first sequence of size `seq_length` uses the last label as its target. The previous labels are discarded.<br>\n",
    ">> * All subsequent sequences for the same engine (`id`) will have one label associated with them step by step.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xIDbZQrJ6Pty"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label2'])\n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pjniT37X6Ptz"
   },
   "outputs": [],
   "source": [
    "# When modeling multi-class classification problems using neural networks,\n",
    "# it is good practice to reshape the output attribute from a vector that contains values for each class value to be\n",
    "# a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "# This is called one hot encoding or creating dummy variables from a categorical variable.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qBrRZdYZiHb"
   },
   "source": [
    "`to_categorical` is a utility function in Keras that converts class vectors (integers) to binary class matrices.<br>\n",
    "`dummy_label_array = to_categorical(label_array):`This line applies one-hot encoding to the `label_array`.<br>\n",
    "`label_array` contains the labels associated with each sequence, where each label represents a class or category.<br>\n",
    "> * One-hot encoding converts these integer labels into binary vectors, where each vector has a length equal to the number of classes and contains a 1 in the position corresponding to the class and 0s elsewhere.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HEpXlgujjc11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "e3BMrY396Pt0"
   },
   "outputs": [],
   "source": [
    "dummy_label_array = to_categorical(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XnwyiGj06Pt0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "M40CNdn7krRw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Nj3Hmdzl9l2q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dummy_label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dWhWIVl56Pt1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "nb_features, nb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0h3i_FJg7pJ"
   },
   "source": [
    "`Extracting Feature and Output Dimensions:`\n",
    "> `nb_features:`Determines the number of features in the input sequence data.<br>\n",
    "> `nb_out:`Determines the number of output classes. It's extracted from the shape of the label array.<br>\n",
    "\n",
    "`Defining the Model Architecture:` describe in the code below.\n",
    "`Compiling the Model:` `model.compile(...)` Here, `categorical_crossentropy` is used as the loss function for multi-class classification.\n",
    "\n",
    "`Model Summary:`Prints a summary of the model architecture, including the layers and their parameters.\n",
    "\n",
    "`Training the Model:` `model.fit(...):` Trains the model on the training data. It specifies the input data (`seq_array`) and the corresponding labels (`dummy_label_array`). Other parameters include the number of epochs, batch size, validation split, verbosity, and callbacks.<br>\n",
    "\n",
    "\n",
    "`history.history.keys():` After training, this prints the keys of the history object, which contains information about training and validation metrics over each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "r9k0p5-46Pt1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50, 100)           50400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 100)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80753 (315.44 KB)\n",
      "Trainable params: 80753 (315.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "58/58 - 21s - loss: 0.4255 - accuracy: 0.8314 - val_loss: 0.2298 - val_accuracy: 0.9127 - 21s/epoch - 364ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\ML2023\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 - 13s - loss: 0.1850 - accuracy: 0.9264 - val_loss: 0.1562 - val_accuracy: 0.9357 - 13s/epoch - 216ms/step\n",
      "Epoch 3/10\n",
      "58/58 - 13s - loss: 0.1504 - accuracy: 0.9350 - val_loss: 0.1985 - val_accuracy: 0.9193 - 13s/epoch - 221ms/step\n",
      "Epoch 4/10\n",
      "58/58 - 13s - loss: 0.1279 - accuracy: 0.9481 - val_loss: 0.1324 - val_accuracy: 0.9489 - 13s/epoch - 220ms/step\n",
      "Epoch 5/10\n",
      "58/58 - 13s - loss: 0.1198 - accuracy: 0.9504 - val_loss: 0.1060 - val_accuracy: 0.9489 - 13s/epoch - 218ms/step\n",
      "Epoch 6/10\n",
      "58/58 - 13s - loss: 0.1182 - accuracy: 0.9526 - val_loss: 0.1181 - val_accuracy: 0.9473 - 13s/epoch - 221ms/step\n",
      "Epoch 7/10\n",
      "58/58 - 13s - loss: 0.1110 - accuracy: 0.9544 - val_loss: 0.1977 - val_accuracy: 0.9275 - 13s/epoch - 221ms/step\n",
      "Epoch 8/10\n",
      "58/58 - 14s - loss: 0.0953 - accuracy: 0.9612 - val_loss: 0.1021 - val_accuracy: 0.9539 - 14s/epoch - 246ms/step\n",
      "Epoch 9/10\n",
      "58/58 - 16s - loss: 0.0851 - accuracy: 0.9628 - val_loss: 0.1298 - val_accuracy: 0.9440 - 16s/epoch - 282ms/step\n",
      "Epoch 10/10\n",
      "58/58 - 16s - loss: 0.0972 - accuracy: 0.9584 - val_loss: 0.1389 - val_accuracy: 0.9390 - 16s/epoch - 269ms/step\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a deep network.\n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.\n",
    "# Dropout is also applied after each LSTM layer to control overfitting.\n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "\n",
    "# Initializes a sequential model, which allows you to build a model layer by layer.\n",
    "model = Sequential()\n",
    "\n",
    "#Adds an LSTM layer to the model. The first LSTM layer has 100 units, returns sequences, and takes input in the shape of (sequence_length, nb_features).\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adds another LSTM layer with 50 units. This layer does not return sequences, indicating it's the final LSTM layer.\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adds a dense (fully connected) layer to the model with a softmax activation function. This layer produces the output classes.\n",
    "model.add(Dense(units=nb_out, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, dummy_label_array, epochs=10, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sWk2GlE6Pt2"
   },
   "source": [
    "# Every time I retrain the algorithm I get different training results, i.e., also different evaluation of the decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set created during the training (i.e., validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FpVYSzXkmk5l"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-u2aW0bj6Pt3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 6s 100ms/step - loss: 0.0890 - accuracy: 0.9623\n",
      "Accurracy: 0.962267279624939\n"
     ]
    }
   ],
   "source": [
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, dummy_label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK6St5XcVyd9"
   },
   "source": [
    "`y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\"):` Predicts the abels for the input sequences using the trained model. The predictions are hresholded at 0.5, meaning that any output probability greater than 0.5 is considered as class 1, otherwise class 0. The predictions are then converted to integers (0 or 1).<br>\n",
    "\n",
    "`y_true = dummy_label_array:` Sets the true labels from the dummy label array, which represents the actual labels of the data.\n",
    "then print the **confusion_matrix**\n",
    "\n",
    "`cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1)):` Computes the confusion matrix using the true labels (`y_true`) and the predicted labels `(y_pred). argmax(axis=1)` is used to convert one-hot encoded labels back to their original integer form before computing the confusion matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KRwaYKfV6Pt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380/380 [==============================] - 15s 34ms/step\n",
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n",
      "[[9635   23    0]\n",
      " [ 297 1285   18]\n",
      " [   2  122  756]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "# y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\") # this way (>0.5) the outcome goes from a probability to 0,1\n",
    "y_true = dummy_label_array\n",
    "\n",
    "# test_set = pd.DataFrame(y_pred)\n",
    "# # test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n",
      "[[9635   23    0]\n",
      " [ 297 1285   18]\n",
      " [   2  122  756]]\n",
      "F1-score for each class: [0.98356472 0.84818482 0.91414752]\n",
      "F2-score for each class (beta=2.0): [0.9919491  0.82056194 0.88029809]\n",
      "R2-score: 0.888302721987315\n",
      "Precision for each class: [0.96990135 0.8986014  0.97674419]\n",
      "Recall for each class: [0.99761855 0.803125   0.85909091]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score, r2_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_pred and y_true are available\n",
    "# y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\")  # Predictions (0 or 1)\n",
    "# y_true = dummy_label_array  # True labels\n",
    "\n",
    "# Convert y_pred and y_true to one-hot encoded if they aren't already\n",
    "y_true_class = np.argmax(y_true, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "print(cm)\n",
    "\n",
    "# Function to compute F1-score, F2-score, and R2-score for each class\n",
    "def calculate_metrics(y_true_class, y_pred_class):\n",
    "    # F1-score for each class\n",
    "    f1 = f1_score(y_true_class, y_pred_class, average=None)\n",
    "    \n",
    "    # F2-score with beta=2 for each class\n",
    "    f2 = fbeta_score(y_true_class, y_pred_class, beta=2.0, average=None)\n",
    "    \n",
    "    # R2-score (optional for classification tasks, but usually used for regression tasks)\n",
    "    r2 = r2_score(y_true_class, y_pred_class)\n",
    "    \n",
    "    return f1, f2, r2\n",
    "\n",
    "# Compute F1, F2, and R2 scores for each class\n",
    "f1, f2, r2 = calculate_metrics(y_true_class, y_pred_class)\n",
    "\n",
    "# Print the results\n",
    "print(\"F1-score for each class:\", f1)\n",
    "print(\"F2-score for each class (beta=2.0):\", f2)\n",
    "print(\"R2-score:\", r2)\n",
    "\n",
    "# Additional classification metrics (optional)\n",
    "precision = precision_score(y_true_class, y_pred_class, average=None)\n",
    "recall = recall_score(y_true_class, y_pred_class, average=None)\n",
    "\n",
    "print(\"Precision for each class:\", precision)\n",
    "print(\"Recall for each class:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Second PdM policy evaluation on the validation set.\n",
    "\n",
    "For each validation set, I need to give the on-line sensor data as input to the trained LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "k_GqqVKV6Pt4"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "vloF6HXQ6Pt4"
   },
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 1000\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PZ-vs1-x6Pt5"
   },
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "5KP5yjWw6Pt5"
   },
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PqIzk2b96Pt5"
   },
   "outputs": [],
   "source": [
    "validation_df['cycle_norm'] = validation_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "pzX435pyxgxD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4493, 30)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "tAF7B0eOY1BN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16141</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>81</td>\n",
       "      <td>238</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1598.68</td>\n",
       "      <td>1428.15</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.48</td>\n",
       "      <td>23.0828</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16376</th>\n",
       "      <td>81</td>\n",
       "      <td>239</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.84</td>\n",
       "      <td>1602.45</td>\n",
       "      <td>1429.38</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.0611</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.01</td>\n",
       "      <td>1601.01</td>\n",
       "      <td>1423.61</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0289</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.73</td>\n",
       "      <td>1582.64</td>\n",
       "      <td>1401.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.84</td>\n",
       "      <td>23.4584</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16379</th>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.07</td>\n",
       "      <td>1585.10</td>\n",
       "      <td>1395.89</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.5238</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "16138  81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91   \n",
       "16139  81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25   \n",
       "16140  81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42   \n",
       "16141  81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89   \n",
       "16142  81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "16375  81    238    0.0011   -0.0004     100.0  518.67  643.54  1598.68   \n",
       "16376  81    239    0.0068    0.0005     100.0  518.67  643.84  1602.45   \n",
       "16377  81    240   -0.0026   -0.0003     100.0  518.67  644.01  1601.01   \n",
       "16378  82      1    0.0000   -0.0002     100.0  518.67  641.73  1582.64   \n",
       "16379  82      2   -0.0025    0.0001     100.0  518.67  642.07  1585.10   \n",
       "\n",
       "            s4     s5  ...   s16  s17   s18    s19    s20      s21  RUL  \\\n",
       "16138  1406.63  14.62  ...  0.03  391  2388  100.0  38.87  23.3365  239   \n",
       "16139  1407.88  14.62  ...  0.03  392  2388  100.0  38.91  23.3452  238   \n",
       "16140  1396.40  14.62  ...  0.03  394  2388  100.0  39.04  23.3610  237   \n",
       "16141  1404.86  14.62  ...  0.03  392  2388  100.0  38.77  23.4206  236   \n",
       "16142  1409.58  14.62  ...  0.03  392  2388  100.0  39.04  23.3311  235   \n",
       "...        ...    ...  ...   ...  ...   ...    ...    ...      ...  ...   \n",
       "16375  1428.15  14.62  ...  0.03  395  2388  100.0  38.48  23.0828    2   \n",
       "16376  1429.38  14.62  ...  0.03  398  2388  100.0  38.30  23.0611    1   \n",
       "16377  1423.61  14.62  ...  0.03  397  2388  100.0  38.37  23.0289    0   \n",
       "16378  1401.58  14.62  ...  0.03  391  2388  100.0  38.84  23.4584  213   \n",
       "16379  1395.89  14.62  ...  0.03  392  2388  100.0  39.10  23.5238  212   \n",
       "\n",
       "       label1  label2  cycle_norm  \n",
       "16138       0       0           1  \n",
       "16139       0       0           2  \n",
       "16140       0       0           3  \n",
       "16141       0       0           4  \n",
       "16142       0       0           5  \n",
       "...       ...     ...         ...  \n",
       "16375       1       2         238  \n",
       "16376       1       2         239  \n",
       "16377       1       2         240  \n",
       "16378       0       0           1  \n",
       "16379       0       0           2  \n",
       "\n",
       "[242 rows x 30 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head(242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBklUcnv6Pt5"
   },
   "source": [
    "## Second PdM policy evaluation on a the whole validation data set (ids 81 to 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "vfyabzdL6Pt_"
   },
   "outputs": [],
   "source": [
    "costs_rep_array   = np.zeros(20) # An array to store costs related to replacements.\n",
    "\n",
    "costs_delay_array = np.zeros(20) # An array to store costs related to delays.\n",
    "costs_stock_array = np.zeros(20) # An array to store costs related to stock.\n",
    "\n",
    "t_LC_array        = np.zeros(20) # An array to store lead time.\n",
    "t_order_array     = np.zeros(20) # An array to store order time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsWT1ebGbOdz"
   },
   "source": [
    "> 1. Initializes a counter variable to 0.\n",
    "> 2. Iterates over unique IDs in the `validation_df` DataFrame.\n",
    "> 3. For each ID:\n",
    ">> * Sets flags for preventive replacement and ordering to False.<br>\n",
    ">> * Iterates over cycles within the range of the DataFrame.<br>\n",
    ">> * Checks if the current cycle is in the `array_decisions`.<br>\n",
    ">> * If it is, preprocesses the validation data for the LSTM model.<br>\n",
    ">> * Predicts the probability of RUL being smaller than w1 and DT (decision time) using the trained model.<br>\n",
    ">> * Evaluates decision heuristics:\n",
    ">>> * If no order has been placed yet and the cost of preventive replacement is less than or equal to the cost of waiting until `w1`, orders the component and sets the order time.<br>\n",
    ">>> * If the cost of preventive replacement is less than or equal to the cost of waiting until `DT`, performs preventive replacement, calculates related costs, and breaks the loop.<br>\n",
    ">> If preventive replacement is not performed:\n",
    ">>> * Sets the component failure time to the last cycle in the ID's data.<br>\n",
    ">>> * Sets replacement costs to `C_c`.<br>\n",
    ">>> * Calculates delay costs based on whether an order has been placed.<br>\n",
    ">> * Prints diagnostic information for each iteration.\n",
    ">> * Increments the counter.\n",
    "\n",
    "\n",
    "This code essentially simulates a decision-making process for component maintenance based on predictive models and cost considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 25)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "probabilities:  [9.9907434e-01 8.3883211e-04 8.6837339e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [9.9951398e-01 4.2986262e-04 5.6077701e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.993894e-01 5.447662e-04 6.573278e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [9.9913114e-01 7.8822148e-04 8.0687161e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [9.9884629e-01 1.0590623e-03 9.4649884e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9941695e-01 5.2194064e-04 6.1112762e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9928683e-01 6.4404303e-04 6.9185109e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "probabilities:  [9.9884975e-01 1.0600629e-03 9.0152011e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9582613e-01 3.9624446e-03 2.1147281e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9760926e-01 2.2519494e-03 1.3883798e-04]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "probabilities:  [9.98206019e-01 1.68124516e-03 1.12747424e-04]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.992017e-01 7.299834e-04 6.835206e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "probabilities:  [9.9858129e-01 1.3248438e-03 9.3933799e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9580526e-01 4.0163533e-03 1.7835245e-04]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [9.8036504e-01 1.9202067e-02 4.3286884e-04]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [0.88886684 0.11007703 0.00105615]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [0.20541611 0.7910117  0.00357215]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [0.01488133 0.8792941  0.10582459]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [0.00737582 0.2950964  0.69752777]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [5.0819409e-04 1.0098097e-02 9.8939371e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9990594e-01 7.0570422e-05 2.3459790e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9990702e-01 6.9737413e-05 2.3204175e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9989772e-01 7.7195175e-05 2.5086016e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9989259e-01 8.1701044e-05 2.5801217e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9988329e-01 8.9462934e-05 2.7173797e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9988592e-01 8.7569962e-05 2.6469763e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9987650e-01 9.5422882e-05 2.8051974e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9986041e-01 1.0973507e-04 2.9836012e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9975020e-01 2.0625806e-04 4.3538072e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9958342e-01 3.5905506e-04 5.7522786e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [9.9935335e-01 5.7508366e-04 7.1627917e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9880719e-01 1.0922779e-03 1.0048836e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9247783e-01 7.2012297e-03 3.2093350e-04]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.5191112  0.47744852 0.00344016]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [0.04173983 0.9546383  0.00362186]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [0.00871683 0.49724862 0.49403453]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [2.3421925e-04 3.0619905e-03 9.9670380e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9991512e-01 6.2920633e-05 2.1939279e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9991834e-01 6.0460767e-05 2.1240956e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.9991763e-01 6.1092491e-05 2.1219643e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [9.9991727e-01 6.1332401e-05 2.1284755e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9991298e-01 6.4924185e-05 2.2051267e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "probabilities:  [9.9990344e-01 7.2543880e-05 2.3916775e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9990940e-01 6.8188347e-05 2.2451566e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "probabilities:  [9.9990237e-01 7.4153584e-05 2.3482291e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "probabilities:  [9.9989414e-01 8.0595455e-05 2.5292084e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9989176e-01 8.3108040e-05 2.5185514e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "probabilities:  [9.9986660e-01 1.0418982e-04 2.9154633e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "probabilities:  [9.9986482e-01 1.0619921e-04 2.9007177e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "probabilities:  [9.9983871e-01 1.2958543e-04 3.1692896e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "probabilities:  [9.9980503e-01 1.5977812e-04 3.5188150e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "probabilities:  [9.9976796e-01 1.9344001e-04 3.8579165e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "probabilities:  [9.996146e-01 3.333135e-04 5.202670e-05]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "probabilities:  [9.9949229e-01 4.4739782e-04 6.0239996e-05]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "probabilities:  [9.9918932e-01 7.3384406e-04 7.6853146e-05]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.992686e-01 6.639353e-04 6.741506e-05]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.9818122e-01 1.7037306e-03 1.1511060e-04]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "probabilities:  [9.6504200e-01 3.4270264e-02 6.8772869e-04]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "probabilities:  [0.5959553  0.40122256 0.00282215]\n",
      "(270, 25)\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "probabilities:  [0.1859734  0.81024605 0.00378044]\n",
      "(280, 25)\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "probabilities:  [0.016762   0.95481694 0.02842109]\n",
      "(290, 25)\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "probabilities:  [8.7502506e-04 2.1525923e-02 9.7759902e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "probabilities:  [9.9959201e-01 3.5994317e-04 4.8025617e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "probabilities:  [9.9962807e-01 3.2620682e-04 4.5733930e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9947542e-01 4.7031464e-04 5.4358828e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9956745e-01 3.8366238e-04 4.8851052e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.9958938e-01 3.6257817e-04 4.7975456e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.9953270e-01 4.1638335e-04 5.0865678e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "probabilities:  [9.9929774e-01 6.3678494e-04 6.5544911e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "probabilities:  [9.9944609e-01 4.9819745e-04 5.5756129e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "probabilities:  [9.9956650e-01 3.8562895e-04 4.7949474e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [9.9957734e-01 3.7524043e-04 4.7414462e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "probabilities:  [9.9957305e-01 3.7938805e-04 4.7522859e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "probabilities:  [9.9834573e-01 1.5483455e-03 1.0597043e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "probabilities:  [9.9840504e-01 1.4933445e-03 1.0166530e-04]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "probabilities:  [9.9908340e-01 8.4508187e-04 7.1430004e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "probabilities:  [9.9910635e-01 8.2175457e-04 7.1867587e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "probabilities:  [9.9855834e-01 1.3462357e-03 9.5413576e-05]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.9220520e-01 7.5472910e-03 2.4740904e-04]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "probabilities:  [0.79272026 0.2056323  0.00164742]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "probabilities:  [0.57699245 0.41957596 0.00343152]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "probabilities:  [0.16750439 0.8265107  0.00598486]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "probabilities:  [0.01795422 0.92143077 0.06061504]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "probabilities:  [0.00490037 0.17166644 0.8234332 ]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "probabilities:  [9.9959093e-01 3.5801725e-04 5.1070561e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.990396e-01 8.690428e-04 9.131176e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9952543e-01 4.1964566e-04 5.4969987e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [9.9955004e-01 3.9816930e-04 5.1758092e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "probabilities:  [9.9928916e-01 6.4131373e-04 6.9498856e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [9.9858546e-01 1.3095611e-03 1.0500254e-04]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.9871004e-01 1.1928771e-03 9.7068027e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9504733e-01 4.7244169e-03 2.2824152e-04]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.8309386e-01 1.6458252e-02 4.4796398e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "probabilities:  [9.8180765e-01 1.7765315e-02 4.2697586e-04]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "probabilities:  [9.921530e-01 7.595295e-03 2.516502e-04]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "probabilities:  [0.69645035 0.3013325  0.00221712]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [0.02554216 0.96454597 0.0099119 ]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "probabilities:  [0.00245329 0.08836088 0.90918577]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.992563e-01 6.677517e-04 7.583772e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [9.993098e-01 6.178752e-04 7.228410e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9940693e-01 5.2796642e-04 6.4993830e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9947101e-01 4.7063158e-04 5.8343281e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9952209e-01 4.2314554e-04 5.4756645e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9963069e-01 3.2193531e-04 4.7288697e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "probabilities:  [9.9959606e-01 3.5526056e-04 4.8756898e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "probabilities:  [9.991320e-01 7.893716e-04 7.877671e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9868304e-01 1.2166814e-03 1.0028317e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9948585e-01 4.6027955e-04 5.3892407e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "probabilities:  [9.9947768e-01 4.6730187e-04 5.5080920e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [9.9908936e-01 8.3370903e-04 7.6970224e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.9897987e-01 9.3880779e-04 8.1342827e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9890053e-01 1.0161729e-03 8.3221857e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.8659760e-01 1.3019009e-02 3.8336712e-04]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [9.9434680e-01 5.4451162e-03 2.0799322e-04]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9878317e-01 1.1327176e-03 8.4065468e-05]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [9.9570125e-01 4.1213888e-03 1.7730970e-04]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [9.7608405e-01 2.3453886e-02 4.6202171e-04]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.5240825e-01 4.6983067e-02 6.0863153e-04]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.09640834 0.8995026  0.00408907]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [0.011789   0.67321646 0.31499457]\n",
      "(270, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [0.0020039  0.0554085  0.94258755]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [9.9953616e-01 4.0866877e-04 5.5185992e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [9.98728454e-01 1.16889621e-03 1.02576334e-04]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "probabilities:  [9.8795229e-01 1.1622408e-02 4.2525408e-04]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [9.9847895e-01 1.4099777e-03 1.1104630e-04]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "probabilities:  [9.9928707e-01 6.4493006e-04 6.8046415e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "probabilities:  [9.9902761e-01 8.8919094e-04 8.3265790e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "probabilities:  [9.9749184e-01 2.3631216e-03 1.4512215e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "probabilities:  [9.9136931e-01 8.3368290e-03 2.9389327e-04]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [9.6999353e-01 2.9425519e-02 5.8095547e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [0.82540905 0.1730452  0.00154581]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [0.83666223 0.16202427 0.00131358]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "probabilities:  [0.02763459 0.9633407  0.00902473]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [0.00110199 0.02787793 0.97102004]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [9.9947006e-01 4.6473087e-04 6.5158179e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9937242e-01 5.5554503e-04 7.2054958e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [9.9959046e-01 3.5680569e-04 5.2832027e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9960107e-01 3.4642726e-04 5.2578904e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9959320e-01 3.5362420e-04 5.3200605e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9919993e-01 7.1972096e-04 8.0437334e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9857998e-01 1.3075422e-03 1.1247525e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9712914e-01 2.6944357e-03 1.7638794e-04]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9730551e-01 2.5341937e-03 1.6027004e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.98236060e-01 1.64552801e-03 1.18383476e-04]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9725026e-01 2.5996151e-03 1.5019160e-04]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "probabilities:  [9.900135e-01 9.657389e-03 3.291643e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.8946553  0.10413974 0.0012051 ]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "probabilities:  [0.16425839 0.83245975 0.00328191]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [0.01697304 0.97019607 0.01283095]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [0.00565407 0.25021318 0.74413276]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "probabilities:  [3.4641864e-04 5.7540946e-03 9.9389946e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [9.9985254e-01 1.1704206e-04 3.0450214e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9984443e-01 1.2412023e-04 3.1474912e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9982738e-01 1.3943081e-04 3.3157536e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9978107e-01 1.8032598e-04 3.8569699e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9976462e-01 1.9558746e-04 3.9757761e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "probabilities:  [9.9974102e-01 2.1681133e-04 4.2163483e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "probabilities:  [9.9970716e-01 2.4766498e-04 4.5223664e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "probabilities:  [9.9962568e-01 3.2326707e-04 5.0985618e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9946386e-01 4.7233122e-04 6.3860105e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "probabilities:  [9.9925715e-01 6.6717237e-04 7.5656106e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [9.9880958e-01 1.0945569e-03 9.5818330e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.8961151e-01 1.0023065e-02 3.6546809e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.94043416 0.05858893 0.00097691]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [0.55045843 0.44668755 0.00285403]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "probabilities:  [0.12159436 0.8742882  0.00411749]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [0.01644633 0.9623517  0.02120203]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [5.1959546e-04 1.0858862e-02 9.8862153e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "probabilities:  [9.9987268e-01 9.8697295e-05 2.8552511e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "probabilities:  [9.9986219e-01 1.0784416e-04 2.9914130e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "probabilities:  [9.9980551e-01 1.5666854e-04 3.7857819e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.9972612e-01 2.2960335e-04 4.4188859e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9936706e-01 5.5981084e-04 7.3195552e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9905521e-01 8.5614406e-04 8.8619570e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "probabilities:  [9.9652314e-01 3.2852991e-03 1.9164497e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [0.9436565  0.05526691 0.00107656]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [0.30155826 0.6949675  0.00347425]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [0.01116084 0.94117826 0.04766089]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [3.9224577e-04 8.1509594e-03 9.9145681e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.989287e-01 9.859039e-04 8.532368e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "probabilities:  [9.9758935e-01 2.2744816e-03 1.3610782e-04]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9518013e-01 4.6137371e-03 2.0618977e-04]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.9550265e-01 4.3115770e-03 1.8574116e-04]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.8799211e-01 1.1684487e-02 3.2341422e-04]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [0.7904785  0.20784682 0.00167464]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [0.21464162 0.7799456  0.00541284]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [0.01738471 0.9429637  0.03965163]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [6.2829279e-04 1.3716059e-02 9.8565567e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9971122e-01 2.4450227e-04 4.4197601e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "probabilities:  [9.996245e-01 3.228323e-04 5.265792e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [9.9967897e-01 2.7444164e-04 4.6648242e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "probabilities:  [9.997775e-01 1.856907e-04 3.687328e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "probabilities:  [9.9977452e-01 1.8820539e-04 3.7259641e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "probabilities:  [9.9965632e-01 2.9465149e-04 4.8938822e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "probabilities:  [9.9959320e-01 3.5331259e-04 5.3515483e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.9962676e-01 3.2346972e-04 4.9775812e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "probabilities:  [9.9967146e-01 2.8349517e-04 4.5029807e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9963903e-01 3.1255002e-04 4.8329839e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [9.9969769e-01 2.5962968e-04 4.2649692e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9969196e-01 2.6495551e-04 4.3149470e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9960619e-01 3.4358594e-04 5.0345770e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [9.9968636e-01 2.7146316e-04 4.2190259e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [9.9965370e-01 3.0158754e-04 4.4644727e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9947673e-01 4.6616461e-04 5.7018671e-05]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "probabilities:  [9.993374e-01 5.975399e-04 6.499327e-05]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9945790e-01 4.8451623e-04 5.7509093e-05]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9905640e-01 8.6560572e-04 7.8057485e-05]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "probabilities:  [9.9882454e-01 1.0876260e-03 8.7899149e-05]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9875915e-01 1.1513261e-03 8.9578054e-05]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9920350e-01 7.3138915e-04 6.5207932e-05]\n",
      "(270, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9881494e-01 1.1029887e-03 8.2034603e-05]\n",
      "(280, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9508989e-01 4.7183218e-03 1.9186217e-04]\n",
      "(290, 25)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "probabilities:  [9.5325059e-01 4.6039365e-02 7.1003003e-04]\n",
      "(300, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [0.5911945  0.40656283 0.00224266]\n",
      "(310, 25)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "probabilities:  [0.3564101  0.6405864  0.00300357]\n",
      "(320, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.05357339 0.93985075 0.00657578]\n",
      "(330, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.01094014 0.65225315 0.33680668]\n",
      "(340, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [7.3352212e-04 1.5359544e-02 9.8390698e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9978930e-01 1.7300741e-04 3.7710692e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9977881e-01 1.8273643e-04 3.8520942e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9971849e-01 2.3557380e-04 4.5867055e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9960953e-01 3.3667844e-04 5.3833010e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "probabilities:  [9.9877447e-01 1.1184585e-03 1.0706459e-04]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9832290e-01 1.5525939e-03 1.2452086e-04]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "probabilities:  [9.8924899e-01 1.0354464e-02 3.9646635e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.45564222 0.54074705 0.00361073]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.024828   0.97012097 0.00505096]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [0.00966018 0.7026561  0.2876837 ]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [4.3487802e-04 9.0248054e-03 9.9054039e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9966669e-01 2.9080734e-04 4.2576074e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.9949121e-01 4.5299632e-04 5.5764121e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "probabilities:  [9.9955779e-01 3.9172688e-04 5.0535553e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "probabilities:  [9.9949455e-01 4.5091752e-04 5.4619173e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "probabilities:  [9.9964714e-01 3.0887674e-04 4.3996948e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "probabilities:  [9.9956447e-01 3.8627983e-04 4.9245133e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "probabilities:  [9.991685e-01 7.584100e-04 7.318541e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "probabilities:  [9.9958605e-01 3.6706228e-04 4.6920297e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "probabilities:  [9.9972469e-01 2.3883181e-04 3.6516634e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "probabilities:  [9.9962628e-01 3.2996805e-04 4.3705182e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "probabilities:  [9.9945825e-01 4.8646217e-04 5.5315941e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [9.9952197e-01 4.2800809e-04 5.0074632e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9877220e-01 1.1375971e-03 9.0147070e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "probabilities:  [9.9877721e-01 1.1360575e-03 8.6712549e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "probabilities:  [9.9931073e-01 6.2797026e-04 6.1400737e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "probabilities:  [9.9794012e-01 1.9431033e-03 1.1666211e-04]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.7602606e-01 2.3488225e-02 4.8575981e-04]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.6172629  0.38067728 0.00205972]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [0.09728406 0.898201   0.00451495]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [0.01679124 0.9536938  0.029515  ]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.00287575 0.09190831 0.9052159 ]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [9.9987710e-01 9.4498675e-05 2.8334978e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [9.9988008e-01 9.2529001e-05 2.7426015e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9987745e-01 9.4902316e-05 2.7667473e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "probabilities:  [9.998840e-01 8.935667e-05 2.660034e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.9988854e-01 8.5414133e-05 2.5937814e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "probabilities:  [9.9988651e-01 8.7211978e-05 2.6203244e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [9.9988282e-01 9.0607275e-05 2.6580097e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9986899e-01 1.0182874e-04 2.9195964e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "probabilities:  [9.9985862e-01 1.1160005e-04 2.9765333e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9982589e-01 1.4031057e-04 3.3834556e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "probabilities:  [9.9980026e-01 1.6290756e-04 3.6822144e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9979144e-01 1.7217707e-04 3.6401441e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "probabilities:  [9.9970204e-01 2.5228990e-04 4.5691275e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "probabilities:  [9.9965358e-01 2.9726484e-04 4.9052884e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "probabilities:  [9.9891281e-01 9.9022733e-04 9.6892196e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "probabilities:  [9.9831188e-01 1.5676095e-03 1.2051014e-04]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "probabilities:  [9.9729770e-01 2.5469146e-03 1.5535440e-04]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "probabilities:  [9.9448401e-01 5.2797534e-03 2.3630538e-04]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "probabilities:  [9.7367001e-01 2.5762154e-02 5.6775927e-04]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [0.7704258  0.22786607 0.00170814]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "probabilities:  [0.30349115 0.6932949  0.00321397]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "probabilities:  [0.15202756 0.84444785 0.00352465]\n",
      "(270, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [0.02001848 0.96646124 0.01352031]\n",
      "(280, 25)\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "probabilities:  [7.4318436e-04 1.6589394e-02 9.8266739e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [9.9972743e-01 2.2823070e-04 4.4277676e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "probabilities:  [9.9974626e-01 2.1303016e-04 4.0812793e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [9.9977130e-01 1.9019823e-04 3.8474613e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "probabilities:  [9.9979693e-01 1.6691105e-04 3.6203404e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9972337e-01 2.3243685e-04 4.4243854e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "probabilities:  [9.9977201e-01 1.8956041e-04 3.8372553e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.997937e-01 1.711363e-04 3.518275e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "probabilities:  [9.9975759e-01 2.0334302e-04 3.9078397e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9974960e-01 2.1003778e-04 4.0423936e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "probabilities:  [9.9980325e-01 1.6330811e-04 3.3535456e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "probabilities:  [9.9975485e-01 2.0607616e-04 3.9140170e-05]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [9.9962652e-01 3.2290161e-04 5.0496965e-05]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.9962926e-01 3.2148138e-04 4.9197999e-05]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [9.9966097e-01 2.9306527e-04 4.6015477e-05]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [9.9959761e-01 3.5236435e-04 5.0067247e-05]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [9.9953711e-01 4.0967687e-04 5.3302334e-05]\n",
      "(210, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [9.9957353e-01 3.7637367e-04 5.0019560e-05]\n",
      "(220, 25)\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "probabilities:  [9.9939167e-01 5.4596004e-04 6.2261817e-05]\n",
      "(230, 25)\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "probabilities:  [9.9925083e-01 6.8177696e-04 6.7445500e-05]\n",
      "(240, 25)\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "probabilities:  [9.9929750e-01 6.4031529e-04 6.2165476e-05]\n",
      "(250, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.9946660e-01 4.8113402e-04 5.2281444e-05]\n",
      "(260, 25)\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "probabilities:  [9.9828053e-01 1.6126209e-03 1.0685704e-04]\n",
      "(270, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9751294e-01 2.3546850e-03 1.3239004e-04]\n",
      "(280, 25)\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "probabilities:  [9.9633348e-01 3.5072693e-03 1.5924916e-04]\n",
      "(290, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [9.923464e-01 7.412723e-03 2.408043e-04]\n",
      "(300, 25)\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "probabilities:  [0.6855376  0.31236872 0.00209374]\n",
      "(310, 25)\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "probabilities:  [0.1208472  0.8739752  0.00517765]\n",
      "(320, 25)\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "probabilities:  [0.02087295 0.9418569  0.03727019]\n",
      "(330, 25)\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "probabilities:  [0.00806357 0.44194984 0.5499866 ]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9977356e-01 1.8729384e-04 3.9197719e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "probabilities:  [9.9981457e-01 1.5133101e-04 3.4133180e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "probabilities:  [9.9982065e-01 1.4614675e-04 3.3237699e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [9.9980158e-01 1.6324371e-04 3.5175948e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "probabilities:  [9.9977654e-01 1.8524534e-04 3.8271308e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "probabilities:  [9.9977630e-01 1.8589709e-04 3.7725455e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [9.9974579e-01 2.1324531e-04 4.0975985e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "probabilities:  [9.9962926e-01 3.1916524e-04 5.1539155e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "probabilities:  [9.993542e-01 5.741887e-04 7.159478e-05]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "probabilities:  [9.9912912e-01 7.8702299e-04 8.3811006e-05]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "probabilities:  [9.9854654e-01 1.3433055e-03 1.1005286e-04]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "probabilities:  [9.9322951e-01 6.4862594e-03 2.8419084e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "probabilities:  [0.50869656 0.48814097 0.00316246]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "probabilities:  [0.05875941 0.9381744  0.00306615]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "probabilities:  [0.00965037 0.62831056 0.36203906]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "probabilities:  [2.2161247e-04 2.8378922e-03 9.9694049e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "probabilities:  [9.9982148e-01 1.4285876e-04 3.5629299e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "probabilities:  [9.9980277e-01 1.6034361e-04 3.6888072e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "probabilities:  [9.9971324e-01 2.4080693e-04 4.5898538e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "probabilities:  [9.9954283e-01 3.9690870e-04 6.0272807e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "probabilities:  [9.9909246e-01 8.1588270e-04 9.1680464e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "probabilities:  [9.9743795e-01 2.3975659e-03 1.6441896e-04]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "probabilities:  [9.8362136e-01 1.5862584e-02 5.1609729e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [0.8293621  0.16872768 0.00191027]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [0.07214097 0.92404395 0.00381506]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "probabilities:  [0.0123147  0.9554462  0.03223916]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [6.2649965e-04 1.4846050e-02 9.8452747e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [9.9973720e-01 2.1940973e-04 4.3431548e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "probabilities:  [9.9966526e-01 2.8558692e-04 4.9189122e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "probabilities:  [9.9975079e-01 2.0911105e-04 4.0032093e-05]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "probabilities:  [9.9977130e-01 1.9060897e-04 3.8118156e-05]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "probabilities:  [9.9963498e-01 3.1343527e-04 5.1635234e-05]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "probabilities:  [9.9942762e-01 5.0456380e-04 6.7732115e-05]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.9812609e-01 1.7363349e-03 1.3755629e-04]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "probabilities:  [9.9603325e-01 3.7499114e-03 2.1686508e-04]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "probabilities:  [9.9616337e-01 3.6362913e-03 2.0030992e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "probabilities:  [9.85788286e-01 1.38099855e-02 4.01612808e-04]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [0.4888274  0.5081371  0.00303543]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "probabilities:  [0.10786269 0.888136   0.00400128]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [0.01520087 0.9589774  0.02582168]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [4.4573177e-04 9.4013214e-03 9.9015290e-01]\n",
      "(50, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9958533e-01 3.6255404e-04 5.2041058e-05]\n",
      "(60, 25)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "probabilities:  [9.9951923e-01 4.2358562e-04 5.7217199e-05]\n",
      "(70, 25)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "probabilities:  [9.9793845e-01 1.9148743e-03 1.4667401e-04]\n",
      "(80, 25)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "probabilities:  [9.9854207e-01 1.3453398e-03 1.1256359e-04]\n",
      "(90, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9821329e-01 1.6602651e-03 1.2645304e-04]\n",
      "(100, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9831545e-01 1.5669336e-03 1.1762379e-04]\n",
      "(110, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [9.9888510e-01 1.0233012e-03 9.1615293e-05]\n",
      "(120, 25)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "probabilities:  [9.9962246e-01 3.3250466e-04 4.5005148e-05]\n",
      "(130, 25)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "probabilities:  [9.9685162e-01 2.9826623e-03 1.6575490e-04]\n",
      "(140, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [9.7990775e-01 1.9604819e-02 4.8745295e-04]\n",
      "(150, 25)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "probabilities:  [9.8966575e-01 1.0044830e-02 2.8941649e-04]\n",
      "(160, 25)\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "probabilities:  [9.3856144e-01 6.0633108e-02 8.0551504e-04]\n",
      "(170, 25)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "probabilities:  [0.0801395  0.9157603  0.00410021]\n",
      "(180, 25)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "probabilities:  [0.01198822 0.71039486 0.27761692]\n",
      "(190, 25)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "probabilities:  [0.00773449 0.3025152  0.6897503 ]\n",
      "(200, 25)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "probabilities:  [2.5805426e-04 4.1060830e-03 9.9563581e-01]\n"
     ]
    }
   ],
   "source": [
    "def calculate_probabilities(estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols):\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        # Loop through each decision point\n",
    "        for cycle in range(pdm_df[pdm_df['id']==id].shape[0]-params+1):\n",
    "            if cycle in array_decisions:\n",
    "                # Prepare data                           \n",
    "                norm_pdm_df = pd.DataFrame(scaler.transform(pdm_df[pdm_df['id']==id][cols_normalize_train][:params+cycle]),\n",
    "                 columns=cols_normalize_train,\n",
    "                 index=pdm_df[pdm_df['id']==id][:params+cycle].index)\n",
    "                print(norm_pdm_df.shape)\n",
    "                join_df = pdm_df[pdm_df['id']==id][:params+cycle][pdm_df[pdm_df['id']==id][:params+cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "                pdm_df_eval_online = join_df.reindex(columns = pdm_df[pdm_df['id']==id][cycle:params+cycle].columns)\n",
    "\n",
    "                seq_array_test_k = pdm_df_eval_online[sequence_cols].values[cycle:params+cycle]\n",
    "                seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1,params, len(sequence_cols))\n",
    "                                \n",
    "                \n",
    "                probabilities    = estimator.predict(seq_array_test_k).reshape(3)\n",
    "\n",
    "                # Predict\n",
    "                #with torch.no_grad():\n",
    "                 #   outputs = estimator(seq_tensor).squeeze()\n",
    "                #probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                print(\"probabilities: \", probabilities)\n",
    "                #print(\"seq_array_test_k: \", seq_array_test_k)\n",
    "                \n",
    "                \n",
    "    return probabilities\n",
    "\n",
    "probabilities= calculate_probabilities(estimator, validation_df, min_max_scaler, sequence_length, cols_normalize, sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "jG8Lip5w6Pt_",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m         counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n\u001b[1;32m---> 71\u001b[0m t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\u001b[38;5;241m=\u001b[39mcalculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, \u001b[43mdevice\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv):\n",
    "    counter = 0\n",
    "    for id in validation_df['id'].unique():\n",
    "        print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order                  = False\n",
    "\n",
    "        for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1):\n",
    "\n",
    "            if cycle in array_decisions:\n",
    "\n",
    "                norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==id][cols_normalize][:sequence_length+cycle]),\n",
    "                     columns=cols_normalize,\n",
    "                     index=validation_df[validation_df['id']==id][:sequence_length+cycle].index)\n",
    "                print(norm_validation_df.shape)\n",
    "                join_df = validation_df[validation_df['id']==id][:sequence_length+cycle][validation_df[validation_df['id']==id][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "                validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==id][cycle:sequence_length+cycle].columns)\n",
    "\n",
    "                seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "                seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "                #print(seq_array_validation_k.shape)\n",
    "                prob_RUL_smaller_DT    = estimator.predict(seq_array_validation_k).reshape(3)[2]\n",
    "                prob_RUL_smaller_w1    = estimator.predict(seq_array_validation_k).reshape(3)[1]\n",
    "\n",
    "                print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "\n",
    "                # evaluate decision heuristics\n",
    "                if order == False:\n",
    "                    if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "                        print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                        print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                        t_order_array[counter] = sequence_length+cycle\n",
    "                        order = True\n",
    "                        print('component ordering at cycle:', t_order_array[counter])\n",
    "\n",
    "                if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                    t_LC_array[counter] = sequence_length+cycle\n",
    "                    costs_rep_array[counter] = C_p\n",
    "                    print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    # print('component lifecycle:', t_LC)\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "                    costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "                    # print('delay time', max(t_order+L-t_LC, 0))\n",
    "                    # print('cost_delay_id:',cost_delay_id)\n",
    "                    # print('cost of stock:', cost_stock_id)\n",
    "                    break\n",
    "\n",
    "        if preventive_replacement == False:\n",
    "            t_LC_array[counter] = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "            print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if order == False:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "\n",
    "        print('True failure:', validation_df[validation_df['id']==id]['cycle'].iloc[-1])\n",
    "        print('-----------------------------------------')\n",
    "        counter+=1\n",
    "    \n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "        \n",
    "\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array=calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onea6sLe6PuA"
   },
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAvJN15m6PuA"
   },
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mmc0YIA6PuA"
   },
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkJNf5d26PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot = costs_rep_array+costs_delay_array+costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp4AF1KW6PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzXo4Qlw6PuB"
   },
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNUa2J36PuB"
   },
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def calculate_T_R_perfect(pdm_df, id, DT=10):\n",
    "    # Retrieve the perfect failure time for the ith component\n",
    "    T_F_perfect = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "    \n",
    "    # Calculate k as the largest integer such that k * DT < T_F_perfect\n",
    "    k = int(T_F_perfect // DT)  # Use floor division\n",
    "    T_R_perfect = k * DT  # Calculate T_R_perfect\n",
    "    \n",
    "    return T_R_perfect\n",
    "\n",
    "def calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = (np.mean(costs_rep_array) + np.mean(costs_delay_array) + np.mean(costs_stock_array))\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "def calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = np.mean(costs_rep_array)\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "# Usage remains the same\n",
    "# Range of C_p values\n",
    "C_p_values = np.array([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "C_c = 1000  # Constant corrective replacement cost\n",
    "M_hat_Policy1_With_ordering = []\n",
    "\n",
    "#M_hat_Policy1_Without_ordering = []\n",
    "\n",
    "#M_hat_Policy2_With_ordering = []\n",
    "#M_hat_Policy2_Without_ordering = []\n",
    "\n",
    "#M_hat_Policy3_With_ordering = []\n",
    "#M_hat_Policy3_Without_ordering = []\n",
    "\n",
    "# Calculate M_hat for each C_p\n",
    "for C_p in C_p_values:\n",
    "    #Call the function to get updated arrays for policy1 with ordering  \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat1 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_With_ordering.append(M_hat1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Call the function to get updated arrays for policy1 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, prob_RUL_smaller_DT_dict, replacement_probs_dict = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat2 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_Without_ordering.append(M_hat2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 with ordering\n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat3 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_With_ordering.append(M_hat3)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions = calculate_probabilities_for_pdm_policy_2_Without_ordering(\n",
    "    estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat4 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_Without_ordering.append(M_hat4)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 with ordering                   \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat5 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_With_ordering.append(M_hat5)\n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, T_R_k_optimal_dict = calculate_probabilities_for_pdm_policy_3_Without_ordering(\n",
    "    estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat6 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_Without_ordering.append(M_hat6)    \n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate C_p/C_c ratios\n",
    "C_p_C_c_ratios = C_p_values / C_c\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "policies = [\n",
    "    (M_hat_Policy1_With_ordering, 'Policy 1 With Ordering')\n",
    "]\n",
    "\n",
    "for data, label in policies:\n",
    "    plt.plot(C_p_C_c_ratios, data, marker='o', label=label)\n",
    "    print(f\"{label} data: {data}\")  # Print data for debugging\n",
    "\n",
    "plt.title('Decision Metric M_hat% vs C_p/C_c Ratio')\n",
    "plt.xlabel('C_p/C_c Ratio')\n",
    "plt.ylabel('Decision Metric M_hat%')\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.xticks(C_p_C_c_ratios)  # Set x-ticks to the calculated ratios\n",
    "plt.axhline(0, color='black', lw=0.5, ls='--')  # Add a horizontal line at y=0\n",
    "\n",
    "# Adjust y-axis limits to ensure all data is visible\n",
    "plt.ylim(bottom=max(1, plt.ylim()[0]), top=plt.ylim()[1]*1.1)\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('decision_metric_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6g7k8Yrftya"
   },
   "source": [
    "### This code calculates the expected cost per unit time using the LSTM model. It computes the mean of the total costs divided by the mean of the time to component failure (t_LC_array). This metric gives an estimate of the average cost incurred per unit time in the system, considering both maintenance and operational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3_N4kNM6PuC"
   },
   "outputs": [],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgoTz57gC_P"
   },
   "source": [
    "This code segment calculates the expected cost per unit time assuming perfect prognostics.\n",
    "`1. Perfect Prognostics Calculation:`\n",
    "> * It initializes an array `t_LC_perfect_array` to store the time of component failure for each unit in the validation dataset. This is calculated by dividing the last observed cycle number by the decision interval DT and then flooring the result to get the last decision cycle before failure.\n",
    "> * The loop iterates over each unique ID in the validation dataset, calculates the time of component failure for each unit, and stores it in\n",
    "`t_LC_perfect_array`.<br>\n",
    "> * `math.floor()` is used to round down the result to the nearest multiple of `DT`.\n",
    "> * Finally, the loop increments the counter for each unit.<br>\n",
    "\n",
    "`2. Cost Calculation:`\n",
    "> * `costs_perfect_array` is initialized with a value of `C_p`, representing the cost of preventive replacements. In a perfect scenario, only preventive replacements are made.\n",
    "> * This array holds the same cost value for each unit in the validation dataset.\n",
    "\n",
    "`3. Expected Cost Calculation:`\n",
    "> * `expected_cost_perfect` is calculated by taking the mean of `costs_perfect_array` and dividing it by the mean of `t_LC_perfect_array`.\n",
    "> * This calculation provides an estimate of the average cost per unit time assuming perfect prognostics, where components are replaced preventively at regular intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOD6NDfH6PuC"
   },
   "outputs": [],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT\n",
    "    counter+=1\n",
    "\n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-vK-DBi6PuC"
   },
   "outputs": [],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7_LJce6PuC"
   },
   "outputs": [],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KofjvavY6PuD"
   },
   "outputs": [],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42Nu-Rg6PuE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
