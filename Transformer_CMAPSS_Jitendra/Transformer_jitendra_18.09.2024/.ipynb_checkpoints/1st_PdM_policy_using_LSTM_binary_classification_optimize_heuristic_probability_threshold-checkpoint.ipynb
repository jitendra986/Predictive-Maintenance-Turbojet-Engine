{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "70-XsHkDGUKu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7825575034658721821]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BsYPkY7gYGx"
   },
   "source": [
    "# Binary classification\n",
    "Predict if an asset will fail within certain time frame (e.g. cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'binary_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification, \n",
    "# while trying to answer the question: is a specific engine going to fail within w cycles?\n",
    "w = 10\n",
    "# w0 = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w, 1, 0 )\n",
    "# train_df['label2'] = train_df['label1']\n",
    "# train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I want to separate the training set into a training and validation set. I will use 80 training sets for the training and 20 training sets as evaluation sets for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ID = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "        94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq_array creation, which is needed as input for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "zSInZu-EkFtf",
    "outputId": "eeaa639d-5fe6-41e1-a114-5de359a792c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 50, 25)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label_array creation, which is used as the \"true\" output in the training of the LSTM (more specifically, using the binary_cross_entropy loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target. \n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 100)           50400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 80,651\n",
      "Trainable params: 80,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "# history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "#                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "#           )\n",
    "\n",
    "# # list all data in history\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "# Optimize heuristic probability threshold by taking the training data and use the LSTM RUL predictor and implement also the decisions. The objective function is the total expected cost rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model which has already been trained in the past and can simply be called back already trained\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost definitions\n",
    "C_p = 100\n",
    "C_c = 500\n",
    "DT  = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,500,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal is to find the optimal Pr threshold that leads to minimization of the expected total cost rate. And then use this threshold in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizer_training_set(PR_thres):\n",
    "    \n",
    "    costs_array = np.zeros(80)\n",
    "    t_LC_array  = np.zeros(80)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for id in train_df['id'].unique():\n",
    "        # print(id)\n",
    "        preventive_replacement = False\n",
    "        \n",
    "        for cycle in range(train_df[train_df['id']==id].shape[0]-sequence_length+1): \n",
    "\n",
    "            if cycle in array_decisions:\n",
    "                seq_array_training_k = train_df[train_df['id']==id][sequence_cols].values[cycle:sequence_length+cycle]\n",
    "                seq_array_training_k = np.asarray(seq_array_training_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "                prob_RUL_smaller_DT = estimator.predict(seq_array_training_k).reshape(1)\n",
    "\n",
    "                # evaluate decision heuristics\n",
    "                if PR_thres <= prob_RUL_smaller_DT:\n",
    "                    t_LC_array[counter] = sequence_length+cycle\n",
    "                    costs_array[counter] = C_p\n",
    "                    preventive_replacement = True\n",
    "                    break\n",
    "\n",
    "        if preventive_replacement == False:\n",
    "            t_LC_array[counter] = train_df[train_df['id']==id]['cycle'].iloc[-1]\n",
    "            # print('ID:', id, ' component failure at t:', t_LC_array[counter])\n",
    "            costs_array[counter] = C_c\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "    expected_cost = np.mean(costs_array) / np.mean(t_LC_array)   # this is the objective function\n",
    "    \n",
    "    return expected_cost\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR_thres = np.arange(0.1, 1.0, 0.1)\n",
    "PR_thres = np.array([0.1, 0.3, 0.5, 0.7, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cost_on_grid = np.zeros(np.size(PR_thres))\n",
    "\n",
    "for i in range(np.size(PR_thres)):\n",
    "    expected_cost_on_grid[i] = minimizer_training_set(PR_thres[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51712993, 0.51216389, 0.53578263, 0.60940773, 0.78287771])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_cost_on_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.507292327203551"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perfect prognostics on the training set\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(80)\n",
    "counter=0\n",
    "for id in train_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(train_df[train_df['id']==id]['cycle'].iloc[-1] /DT) * DT    \n",
    "    counter+=1\n",
    "    \n",
    "costs_perfect_array = np.ones(80)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190., 280., 170., 180., 260., 180., 250., 150., 200., 220., 240.,\n",
       "       170., 160., 180., 200., 200., 270., 190., 150., 230., 190., 200.,\n",
       "       160., 140., 230., 190., 150., 160., 160., 190., 230., 190., 200.,\n",
       "       190., 180., 150., 170., 190., 120., 180., 210., 190., 200., 190.,\n",
       "       150., 250., 210., 230., 210., 190., 210., 210., 190., 250., 190.,\n",
       "       270., 130., 140., 230., 170., 180., 180., 170., 280., 150., 200.,\n",
       "       310., 190., 360., 130., 200., 210., 210., 160., 220., 210., 150.,\n",
       "       230., 190., 180.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PR_thres[np.argmin(expected_cost_on_grid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.93923723,  0.9603073 ,  5.61615002, 20.12949914, 54.32476798])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = (expected_cost_on_grid-expected_cost_perfect)/expected_cost_perfect *100\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_thres = np.array([0.6, 0.7, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cost_on_grid = np.zeros(np.size(PR_thres))\n",
    "\n",
    "for i in range(np.size(PR_thres)):\n",
    "    expected_cost_on_grid[i] = minimizer_training_set(PR_thres[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.4144385 , 20.12949914, 29.82711671])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = (expected_cost_on_grid-expected_cost_perfect)/expected_cost_perfect *100\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1545862732521006"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(minimizer_training_set(0.2)-expected_cost_perfect)/expected_cost_perfect *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.32476797777639"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(minimizer_training_set(0.90)-expected_cost_perfect)/expected_cost_perfect *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now take the value of the \"optimal\" heuristic probability threshold that you found by applying the decisions on the training set with the trained LSTM classifier and evaluate the decisions on the validation data set (ids 81 to 100) (i.e., eventually get the total expected cost rate and the error metric on the validation set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_array = np.zeros(20)\n",
    "t_LC_array  = np.zeros(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost definitions\n",
    "C_p = 100\n",
    "C_c = 500\n",
    "DT  = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 81  preventive replacement informed at cycle: 240.0\n",
      "component lifecycle: 240.0\n",
      "ID: 82  preventive replacement informed at cycle: 210.0\n",
      "component lifecycle: 210.0\n",
      "ID: 83  preventive replacement informed at cycle: 290.0\n",
      "component lifecycle: 290.0\n",
      "ID: 84  component failure at t: 267.0\n",
      "ID: 85  preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "ID: 86  preventive replacement informed at cycle: 270.0\n",
      "component lifecycle: 270.0\n",
      "ID: 87  preventive replacement informed at cycle: 170.0\n",
      "component lifecycle: 170.0\n",
      "ID: 88  preventive replacement informed at cycle: 210.0\n",
      "component lifecycle: 210.0\n",
      "ID: 89  preventive replacement informed at cycle: 210.0\n",
      "component lifecycle: 210.0\n",
      "ID: 90  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "ID: 91  preventive replacement informed at cycle: 130.0\n",
      "component lifecycle: 130.0\n",
      "ID: 92  preventive replacement informed at cycle: 340.0\n",
      "component lifecycle: 340.0\n",
      "ID: 93  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "ID: 94  component failure at t: 258.0\n",
      "ID: 95  preventive replacement informed at cycle: 280.0\n",
      "component lifecycle: 280.0\n",
      "ID: 96  component failure at t: 336.0\n",
      "ID: 97  preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "ID: 98  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "ID: 99  preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "ID: 100  preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for id in validation_df['id'].unique():\n",
    "    # print(id)\n",
    "    preventive_replacement = False\n",
    "    for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1): \n",
    "\n",
    "        if cycle in array_decisions:\n",
    "            # print(sequence_length+cycle)\n",
    "            seq_array_validation_k = validation_df[validation_df['id']==id][sequence_cols].values[cycle:sequence_length+cycle]\n",
    "            seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "            prob_RUL_smaller_DT = estimator.predict(seq_array_validation_k).reshape(1)\n",
    "            # print(prob_RUL_smaller_DT)\n",
    "\n",
    "            # evaluate decision heuristics\n",
    "            if 0.80 <= prob_RUL_smaller_DT:\n",
    "                t_LC_array[counter] = sequence_length+cycle\n",
    "                costs_array[counter] = C_p\n",
    "                print('ID:', id, ' preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                print('component lifecycle:', t_LC_array[counter])\n",
    "                preventive_replacement = True\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        t_LC_array[counter] = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "        print('ID:', id, ' component failure at t:', t_LC_array[counter])\n",
    "        costs_array[counter] = C_c\n",
    "        \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 500., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 500., 100., 500., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 210., 290., 267., 180., 270., 170., 210., 210., 150., 130.,\n",
       "       340., 150., 258., 280., 336., 200., 150., 180., 200.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.05"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT    \n",
    "    counter+=1\n",
    "    \n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 210., 290., 260., 180., 270., 170., 210., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 200., 150., 180., 200.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_perfect_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7238181406921511"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of the expected cost per unit time, Eqns. (3) and (4) of our paper.\n",
    "expected_cost_LSTM = np.mean(costs_array)/np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5923999095227325"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.239990952273246"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M*100 # error percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "name": "Predictive-Maintenance-using-LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
