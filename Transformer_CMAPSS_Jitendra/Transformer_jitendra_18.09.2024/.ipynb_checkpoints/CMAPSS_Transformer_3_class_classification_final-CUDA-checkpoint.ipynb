{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70-XsHkDGUKu",
    "outputId": "4a5f97ed-c7cc-44c4-fc13-65ae479147b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/eragroup/anaconda3/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: matplotlib in /home/eragroup/anaconda3/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /home/eragroup/anaconda3/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (62.3.2)\n",
      "Requirement already satisfied: wheel in /home/eragroup/anaconda3/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.33.6)\n",
      "Requirement already satisfied: requests in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /home/eragroup/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2019.9.11)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (1.26.19)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /home/eragroup/anaconda3/lib/python3.7/site-packages (0.17.4)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: setproctitle in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: platformdirs in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.5.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: setuptools in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (62.3.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (2.7.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (5.6.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pyyaml in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.12.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from wandb) (3.13.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/eragroup/anaconda3/lib/python3.7/site-packages (from click!=8.0.0,>=7.1->wandb) (0.23)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2019.9.11)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/eragroup/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click!=8.0.0,>=7.1->wandb) (3.8.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib numpy\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmWTwJyIYWDo"
   },
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Yeksw_AMYVaV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for max_split_size_mb\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3wOjKm186Pth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available GPU:  1\n",
      "Available devices:\n",
      "- <torch.cuda.device object at 0x7f11908b7510>\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#device_lib.list_local_devices()\n",
    "\n",
    "# Get the available devices\n",
    "devices = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print('number of available GPU: ',torch.cuda.device_count())  # Should print the number of visible devices\n",
    "\n",
    "# Print the available devices\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(f\"- {device}\")\n",
    "device = torch.device(\"cuda:0\")  # Use the first GPU\n",
    "\n",
    "# Print the current device\n",
    "#print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8307933184, 8512602112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 28 18:27:44 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P4000        Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 47%   45C    P0    31W / 105W |    195MiB /  8118MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3605      G   /usr/lib/xorg/Xorg                 49MiB |\n",
      "|    0   N/A  N/A      3624      G   /usr/bin/sddm-greeter              63MiB |\n",
      "|    0   N/A  N/A      8660      C   ...roup/anaconda3/bin/python       77MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4BsYPkY7gYGx"
   },
   "outputs": [],
   "source": [
    "# Multiclass classification\n",
    "#Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iklamKYlNjh"
   },
   "source": [
    "\n",
    "`os`: This module provides functions for interacting with the operating system. It's commonly used for tasks such as file manipulation and directory operations.<br>\n",
    "`sklearn.preprocessing`: This module from scikit-learn provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables.<br>\n",
    "`sklearn.metrics`: This module contains functions for evaluating model performance, such as computing confusion matrices, recall scores, and precision scores.<br>\n",
    "`multiclass_model_w1_30.h5`:The .h5 extension indicates that the model will be saved in the Hierarchical Data Format version 5 (HDF5) format, which is commonly used for storing large numerical datasets. The model will be saved with the filename **multiclass_model_w1_30.h5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "\n",
    "\n",
    "# define path to save model\n",
    "#model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "from data_preprocessor import DataPreprocessor\n",
    "\n",
    "# Initialize the preprocessor with the path to your training data file\n",
    "preprocessor = DataPreprocessor('PM_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpsNnInsevi"
   },
   "source": [
    "`train_df.sort_values(['id','cycle'])`: This line sorts the DataFrame **train_df** first by the 'id' column and then by the 'cycle' column. It ensures that the data is ordered by engine ID and cycle number, which may be necessary for certain analyses or modeling tasks. The sorted DataFrame is then assigned back to the variable **train_df**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing\n",
    "data preprocessing step, particularly for labeling the data for training purposes. Let's break down what each part of the code does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12138, 50, 25),\n",
       " (12138, 3),\n",
       " (1742, 50, 25),\n",
       " (1742, 3),\n",
       " (1751, 50, 25),\n",
       " (1751, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_array,dummy_label_array,seq_array_validation, dummy_label_array_validation,seq_array_test,dummy_label_array_test, sequence_cols = preprocessor.preprocess()\n",
    "seq_array.shape, dummy_label_array.shape, seq_array_validation.shape, dummy_label_array_validation.shape,seq_array_test.shape, dummy_label_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setting1', 'setting2', 'setting3', 'cycle_norm', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n"
     ]
    }
   ],
   "source": [
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2251, 29), (16138, 29), (2242, 29))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df,test_df,train_df = preprocessor.test_data_pdmPolicy()\n",
    "test_df.shape, train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4493, 29),\n",
       " Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
       "        's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
       "        's15', 's16', 's17', 's18', 's19', 's20', 's21', 'RUL', 'label1',\n",
       "        'label2'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df = pd.concat([validation_df, test_df], ignore_index=True)\n",
    "complete_df = pd.concat([train_df, pdm_df], ignore_index=True)\n",
    "pdm_df.shape, pdm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4195  0.03  392  2388  100.0  39.06  23.4190  191       0   \n",
       "1  14.62  ...  8.4318  0.03  392  2388  100.0  39.00  23.4236  190       0   \n",
       "2  14.62  ...  8.4178  0.03  390  2388  100.0  38.95  23.3442  189       0   \n",
       "3  14.62  ...  8.3682  0.03  392  2388  100.0  38.88  23.3739  188       0   \n",
       "4  14.62  ...  8.4294  0.03  393  2388  100.0  38.90  23.4044  187       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0  81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91  1406.63   \n",
       "1  81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25  1407.88   \n",
       "2  81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42  1396.40   \n",
       "3  81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89  1404.86   \n",
       "4  81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49  1409.58   \n",
       "\n",
       "      s5  ...     s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n",
       "0  14.62  ...  8.4455  0.03  391  2388  100.0  38.87  23.3365  239       0   \n",
       "1  14.62  ...  8.4573  0.03  392  2388  100.0  38.91  23.3452  238       0   \n",
       "2  14.62  ...  8.4522  0.03  394  2388  100.0  39.04  23.3610  237       0   \n",
       "3  14.62  ...  8.4403  0.03  392  2388  100.0  38.77  23.4206  236       0   \n",
       "4  14.62  ...  8.3971  0.03  392  2388  100.0  39.04  23.3311  235       0   \n",
       "\n",
       "   label2  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cycle_norm', 's1', 's10', 's11', 's12', 's13', 's14', 's15', 's16',\n",
       "       's17', 's18', 's19', 's2', 's20', 's21', 's3', 's4', 's5', 's6', 's7',\n",
       "       's8', 's9', 'setting1', 'setting2', 'setting3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_normalize_train,cols_normalize_validation,cols_normalize_test = preprocessor.normalize_data_pdmPolicy()\n",
    "cols_normalize_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-_dxq60Nf4"
   },
   "source": [
    ">`Data Labeling`: This part calculates the Remaining Useful Life (RUL) or Time to Failure for each engine by finding the maximum cycle number (cycle) for each engine ID (id). The result is stored in a DataFrame rul with columns 'id' and 'max'.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZubPMTD0T9z"
   },
   "source": [
    ">`Merge RUL with Training Data`:the RUL information is merged back into the original training DataFrame **train_df** based on the engine ID. This allows each row in train_df to have the corresponding maximum cycle number as well.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-eouxGv0gpG"
   },
   "source": [
    ">`Calculate RUL`: This line calculates the RUL by subtracting the current cycle number ('cycle') from the maximum cycle number ('max') for each engine. This represents how many more cycles the engine is expected to operate before failure.<br>\n",
    ">`Drop Unnecessary Columns`: After calculating RUL, the 'max' column, which was used temporarily to calculate RUL, is dropped from the DataFrame as it's no longer needed.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFd2pLr0odt"
   },
   "source": [
    "> `Labeling for Classification`: This part assigns labels to each data point based on the calculated RUL. It defines thresholds `w1` and `w0`, and assigns:\n",
    ">> * Label 1 ('label1') as 1 if RUL is less than or equal to 'w1', and 0 otherwise.\n",
    ">> * Label2 ('label2') as 1 if RUL is less than or equal to 'w1', 2 if RUL is less than or equal to 'w0', and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_8MNc2Z6Ptu"
   },
   "source": [
    " Now I want to separate the train_df set into a training/validation/test set. I will use 80% training sets for the training and 10% training sets as validation sets for hyperparameter tuning and the remaining 10% as test set for the PdM policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13rqadVW6Ptv"
   },
   "source": [
    "I separate into training and validation and test set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7siRw6Ptx"
   },
   "source": [
    "Perform the min max scaling on the training data and validation dataset\n",
    "use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYkFz5FqQplh"
   },
   "source": [
    ">`Create a copy of the cycle column`: This line creates a new column named 'cycle_norm' in the train_df DataFrame and initializes it with the values from the original 'cycle' column. This column will be normalized later.<br>\n",
    "> `Select columns for normalization`: This line selects all columns from **train_df** except 'id', 'cycle', 'RUL', 'label1', and 'label2'. These columns are the ones that will undergo normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AHbVv4RgN-"
   },
   "source": [
    "> `Initialize MinMaxScaler`: This line initializes a MinMaxScaler object from the scikit-learn preprocessing module. This scaler will be used to perform Min-Max normalization.<br>\n",
    "> `Perform Min-Max normalization`: This line applies Min-Max normalization to the selected columns (`cols_normalize`) of the `train_df` DataFrame.<br>\n",
    "> `min_max_scaler.fit_transform(train_df[cols_normalize])` computes the Min-Max normalization for the selected columns.<br>\n",
    "> The resulting normalized values are stored in a new DataFrame called `norm_train_df`, with the same index as `train_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEf7s4DS3jR"
   },
   "source": [
    "> `Join normalized DataFrame with the original DataFrame`: This line joins the normalized DataFrame (`norm_train_df`) with the original DataFrame (`train_df`) excluding the columns that were normalized.<br>\n",
    "> The resulting DataFrame `join_df` contains both the normalized columns and the original columns that were not normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF2-ITehT1_P"
   },
   "source": [
    "`Reorder columns`:\n",
    "> * This line reorders the columns of `join_df` to match the original order of columns in `train_df`.\n",
    "> * The reordered DataFrame is then assigned back to `train_df`, effectively replacing the original DataFrame with the normalized version.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## Vanilla Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `sequence_cols.extend(sensor_cols)`, it adds each element of `sensor_cols` to the end of `sequence_cols`.<br>\n",
    "After this operation, `sequence_cols` will contain 25 elements: 4 operational settings followed by 21 sensor readings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEV0vj7AkJOl"
   },
   "source": [
    "## generate sequences for each engine\n",
    "> * This creates a generator expression that iterates over unique engine IDs in the training data.<br>\n",
    "> * For each engine, it generates sequences using the `gen_sequence` function defined earlier.<br>\n",
    "> * Each sequence is a list of sensor data, and multiple sequences are generated for each engine.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Fc99j1rPQa"
   },
   "source": [
    "> * This concatenates all the generated sequences into a single numpy array.\n",
    "> * It converts the array to `float32` data type.\n",
    "> * The resulting `seq_array` contains the sequences of sensor data, with shape `(num_sequences, sequence_length, num_features)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718782949253,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "An8AqEX_6Pty"
   },
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu8ocOhxkaf"
   },
   "source": [
    "`Function Signature:` This function efficiently generates labels for each sequence of sensor data. It ensures that the labels are correctly aligned with the sequences and handles the special case where the first sequence uses the last label as its target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This function takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (id).<br>\n",
    ">> * `seq_length`: Length of the sequence window.<br>\n",
    ">> * `label`: List of column names representing the labels.\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[label].values:`\n",
    ">> * This line extracts the columns specified by label from the DataFrame id_df and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant label(s) needed for generating sequences.<br>\n",
    "\n",
    "`Label Generation:`\n",
    "> * `num_elements:`This line calculates the number of rows (elements) in the data matrix, which corresponds to the number of labels.<br>\n",
    "> * `return data_matrix[seq_length:num_elements, :]:`\n",
    ">> * This line returns the labels associated with each sequence.<br>\n",
    ">> * It removes the first `seq_length` labels because, for each engine (`id`), the first sequence of size `seq_length` uses the last label as its target. The previous labels are discarded.<br>\n",
    ">> * All subsequent sequences for the same engine (`id`) will have one label associated with them step by step.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When modeling multi-class classification problems using neural networks,<br>\n",
    "it is good practice to reshape the output attribute from a vector that contains values for each class value to be<br>\n",
    "a matrix with a boolean for each class value and whether or not a given instance has that class value or not.<br>\n",
    "This is called one hot encoding or creating dummy variables from a categorical variable.<br>\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qBrRZdYZiHb"
   },
   "source": [
    "`to_categorical` is a utility function in Keras that converts class vectors (integers) to binary class matrices.<br>\n",
    "`dummy_label_array = to_categorical(label_array):`This line applies one-hot encoding to the `label_array`.<br>\n",
    "`label_array` contains the labels associated with each sequence, where each label represents a class or category.<br>\n",
    "> * One-hot encoding converts these integer labels into binary vectors, where each vector has a length equal to the number of classes and contains a 1 in the position corresponding to the class and 0s elsewhere.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782965252,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "XnwyiGj06Pt0",
    "outputId": "b319666d-4293-4940-843e-a3bce535053c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1718782966954,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "qbAJg5KqY-In",
    "outputId": "22d56de8-00e6-4671-b0cd-d47c0e24276c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1742, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782969359,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "M40CNdn7krRw",
    "outputId": "9df2380d-c7b1-43df-cb41-ec51fb81c98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718782971301,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "dWhWIVl56Pt1",
    "outputId": "a9a57d55-f589-4306-954f-0d36aed732c2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "nb_features, nb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0h3i_FJg7pJ"
   },
   "source": [
    "`Extracting Feature and Output Dimensions:`\n",
    "> `nb_features:`Determines the number of features in the input sequence data.<br>\n",
    "> `nb_out:`Determines the number of output classes. It's extracted from the shape of the label array.<br>\n",
    "\n",
    "`Defining the Model Architecture:` describe in the code below.\n",
    "`Compiling the Model:` `model.compile(...)` Here, `categorical_crossentropy` is used as the loss function for multi-class classification.\n",
    "\n",
    "`Model Summary:`Prints a summary of the model architecture, including the layers and their parameters.\n",
    "\n",
    "`Training the Model:` `model.fit(...):` Trains the model on the training data. It specifies the input data (`seq_array`) and the corresponding labels (`dummy_label_array`). Other parameters include the number of epochs, batch size, validation split, verbosity, and callbacks.<br>\n",
    "\n",
    "\n",
    "`history.history.keys():` After training, this prints the keys of the history object, which contains information about training and validation metrics over each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOmuAimTDi5p"
   },
   "source": [
    "### Define the Dataset:\n",
    "Create a custom dataset class to handle your multivariate time series data with labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1718791179567,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "C5EsCykBKvsT",
    "outputId": "ebb36239-3bab-4478-9e21-ea72b12ac2ba"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#os.chdir('/content/drive/MyDrive/Colab Notebooks/LSTM_Antonis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6baBqY-Oli3"
   },
   "source": [
    "`Shuffling Batches:` By setting `shuffle=True` in the `DataLoader` for the training set, you ensure that the order of batches is shuffled each epoch. This maintains the temporal structure within each batch while still introducing variability in the order in which batches are processed.<br>\n",
    "\n",
    "`DataLoader for Validation:` Ensure `shuffle=False` for the validation set to maintain the sequence order during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1718791182861,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "8SbqCLucDib9"
   },
   "outputs": [],
   "source": [
    "# Import custom classes\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from dataset import MultivariateTimeSeriesDataset\n",
    "from model import TransformerTimeSeriesModel\n",
    "\n",
    "# Load initial parameters from params.json\n",
    "with open('params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Load hyperparameter search space from hyperparameters.json\n",
    "with open('hyperparameters.json', 'r') as f:\n",
    "    hp_config = json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage with seq_array and dummy_var\n",
    "#seq_array --> (12138, 50, 25)  # use this as your actual data\n",
    "#dummy_var --->  (12138, 3) # use this as your actual dummy variable (3 classes)\n",
    "\n",
    "#train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "#val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "# Shuffle batches by setting shuffle=True in DataLoader\n",
    "#train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.environ[\\'WANDB_API_KEY\\'] = \\'f05e720396a77affdb7e82b525ef1a912da7aaf4\\'\\n\\ntry:\\n    wandb.login()\\n    print(f\"Successfully logged in as: {wandb.api.viewer()[\\'entity\\']}\")\\nexcept wandb.errors.AuthenticationError:\\n    print(\"Failed to authenticate. Please check your API key.\")\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "os.environ['WANDB_API_KEY'] = 'f05e720396a77affdb7e82b525ef1a912da7aaf4'\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(f\"Successfully logged in as: {wandb.api.viewer()['entity']}\")\n",
    "except wandb.errors.AuthenticationError:\n",
    "    print(\"Failed to authenticate. Please check your API key.\")\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_entity = wandb.api.viewer()[\\'entity\\']\\nprint(f\"Currently logged in as: {current_entity}\")\\nif current_entity != \"jitendratiwari11\":\\n    raise ValueError(f\"Logged in as {current_entity}, but expected to be logged in as jitendra\")\\n\\nprint(wandb.api.viewer()[\\'entity\\'])\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "current_entity = wandb.api.viewer()['entity']\n",
    "print(f\"Currently logged in as: {current_entity}\")\n",
    "if current_entity != \"jitendratiwari11\":\n",
    "    raise ValueError(f\"Logged in as {current_entity}, but expected to be logged in as jitendra\")\n",
    "\n",
    "print(wandb.api.viewer()['entity'])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wandb.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQmBGIESY4qE"
   },
   "source": [
    "### Define the Transformer Model:\n",
    "Implement the vanilla Transformer architecture, ensuring it takes the dummy variable as an input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV8tK1SNV2e-"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Define the training loop with the loss function and optimizer, and include the dummy variable in the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_and_evaluate(config, num_epochs,use_early_stopping=True):\\n    model = TransformerTimeSeriesModel(\\n        input_dim=params[\\'input_dim\\'],\\n        seq_length=params[\\'seq_length\\'],\\n        num_classes=params[\\'num_classes\\'],\\n        model_dim=config[\\'model_dim\\'],\\n        num_heads=config[\\'num_heads\\'],\\n        num_layers=config[\\'num_layers\\'],\\n        dropout_rate=config[\\'dropout_rate\\']\\n    )\\n\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    model.to(device)\\n    \\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.AdamW(model.parameters(), lr=config[\\'learning_rate\\'], weight_decay=config[\\'weight_decay\\'])\\n    scaler = GradScaler()\\n\\n    # Set up datasets\\n    train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params[\\'seq_length\\'])\\n    val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params[\\'seq_length\\'])\\n\\n    # Set up data loaders\\n    train_loader = DataLoader(train_dataset, batch_size=config[\\'batch_size\\'], shuffle=True)\\n    val_loader = DataLoader(val_dataset, batch_size=config[\\'batch_size\\'], shuffle=False)\\n    \\n    train_losses = []\\n    val_losses = []\\n    val_accuracies = []\\n\\n    best_val_loss = float(\\'inf\\')\\n    best_val_accuracy = 0\\n    best_accuracy = 0\\n    best_model = None\\n    epochs_without_improvement = 0\\n\\n    for epoch in range(num_epochs):\\n        model.train()\\n        total_train_loss = 0.0\\n        train_correct = 0\\n        train_total = 0\\n        \\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\\n        for i, batch in enumerate(train_pbar):\\n            inputs, labels = batch\\n            inputs, labels = inputs.float().to(device), labels.float().to(device)\\n            \\n            with autocast():\\n                outputs_class = model(inputs)\\n                outputs_reshaped = outputs_class.view(-1, model.num_classes)\\n                labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\\n                loss = criterion(outputs_reshaped, labels_reshaped) / config[\\'gradient_accumulation_steps\\']\\n\\n            scaler.scale(loss).backward()\\n            \\n            if (i + 1) % config[\\'gradient_accumulation_steps\\'] == 0:\\n                scaler.step(optimizer)\\n                scaler.update()\\n                optimizer.zero_grad()\\n            \\n            total_train_loss += loss.item() * config[\\'gradient_accumulation_steps\\']\\n            \\n            \\n            current_train_loss = total_train_loss / (i + 1)\\n            train_pbar.set_postfix({\\'train loss\\': current_train_loss})\\n            \\n            if i % 100 == 0:\\n                torch.cuda.empty_cache()\\n                gc.collect()\\n\\n        avg_train_loss = total_train_loss / len(train_loader)\\n        train_losses.append(avg_train_loss)\\n\\n        \\n        #Validation loop\\n        model.eval()\\n        total_val_loss = 0.0\\n        all_preds = []\\n        all_labels = []\\n\\n        \\n        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\\n        with torch.no_grad():\\n            for batch in val_pbar:\\n                inputs, labels = batch\\n                inputs, labels = inputs.float().to(device), labels.float().to(device)\\n                \\n                with autocast():\\n                    outputs_class = model(inputs)\\n                    outputs_reshaped = outputs_class.view(-1, model.num_classes)\\n                    labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\\n                    val_loss = criterion(outputs_reshaped, labels_reshaped)\\n                \\n                total_val_loss += val_loss.item()\\n\\n                _, preds = torch.max(outputs_class, dim=2)\\n                all_preds.extend(preds.cpu().numpy().reshape(-1))\\n                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\\n\\n                current_val_loss = total_val_loss / (val_pbar.n + 1)\\n                val_pbar.set_postfix({\\'val loss\\': current_val_loss})\\n\\n        avg_val_loss = total_val_loss / len(val_loader)\\n        val_losses.append(avg_val_loss)\\n\\n        accuracy = accuracy_score(all_labels, all_preds)\\n        val_accuracies.append(accuracy)\\n        \\n        print(f\"Epoch {epoch+1}/{num_epochs}, \"\\n              f\"Train Loss: {avg_train_loss:.4f}, \"\\n              f\"Val Loss: {avg_val_loss:.4f}, \"\\n              f\"Val Accuracy: {accuracy:.2f}%\")\\n        \\n        wandb.log({\\n            \"epoch\": epoch,\\n            \"train_loss\": avg_train_loss,\\n            \"val_loss\": avg_val_loss,\\n            \"val_accuracy\": accuracy\\n        })\\n        if use_early_stopping:\\n            # Early stopping check\\n            if accuracy > best_val_accuracy:\\n                best_val_accuracy = accuracy\\n                best_val_loss = avg_val_loss\\n                best_model = model.state_dict()\\n                epochs_without_improvement = 0\\n            else:\\n                epochs_without_improvement += 1\\n\\n            if epochs_without_improvement >= params[\\'patience\\']:\\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\\n                break\\n        else:\\n            # If not using early stopping, always update best model\\n            best_val_accuracy = accuracy\\n            best_val_loss = avg_val_loss\\n            best_model = model.state_dict()\\n            \\n\\n        \\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n    #Load the best model before returning\\n    model.load_state_dict(best_model)\\n\\n    return model, train_losses, val_losses, val_accuracies\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def train_and_evaluate(config, num_epochs,use_early_stopping=True):\n",
    "    model = TransformerTimeSeriesModel(\n",
    "        input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=config['model_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout_rate=config['dropout_rate']\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Set up datasets\n",
    "    train_dataset = MultivariateTimeSeriesDataset(seq_array, dummy_label_array, params['seq_length'])\n",
    "    val_dataset = MultivariateTimeSeriesDataset(seq_array_validation, dummy_label_array_validation, params['seq_length'])\n",
    "\n",
    "    # Set up data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for i, batch in enumerate(train_pbar):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs_class = model(inputs)\n",
    "                outputs_reshaped = outputs_class.view(-1, model.num_classes)\n",
    "                labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped) / config['gradient_accumulation_steps']\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_train_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "            \n",
    "            \n",
    "            current_train_loss = total_train_loss / (i + 1)\n",
    "            train_pbar.set_postfix({'train loss': current_train_loss})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        \n",
    "        #Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.float().to(device), labels.float().to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs_class = model(inputs)\n",
    "                    outputs_reshaped = outputs_class.view(-1, model.num_classes)\n",
    "                    labels_reshaped = labels.view(-1, model.num_classes).argmax(dim=1)\n",
    "                    val_loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs_class, dim=2)\n",
    "                all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "\n",
    "                current_val_loss = total_val_loss / (val_pbar.n + 1)\n",
    "                val_pbar.set_postfix({'val loss': current_val_loss})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": accuracy\n",
    "        })\n",
    "        if use_early_stopping:\n",
    "            # Early stopping check\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= params['patience']:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            # If not using early stopping, always update best model\n",
    "            best_val_accuracy = accuracy\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    #Load the best model before returning\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef objective():\\n    global trial_counter\\n    with wandb.init() as run:\\n        config = wandb.config\\n        best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(config, num_epochs=config.num_epochs)\\n        \\n        best_val_accuracy = max(val_accuracies)\\n        wandb.log({\"best_val_accuracy\": best_val_accuracy})\\n        \\n        trial_counter = run.step + 1\\n        print(f\"Trial {trial_counter}/{hp_config[\\'num_trials\\']}, \"\\n              f\"Best Val Accuracy: {best_val_accuracy:.2f}\")\\n\\n        \\n        # Save the best model\\n        # torch.save(best_model.state_dict(), \\'best_model_3_class.pth\\')\\n        # wandb.save(\\'best_model_3_class.pth\\')\\n        \\n        return best_val_accuracy\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def objective():\n",
    "    global trial_counter\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(config, num_epochs=config.num_epochs)\n",
    "        \n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        \n",
    "        trial_counter = run.step + 1\n",
    "        print(f\"Trial {trial_counter}/{hp_config['num_trials']}, \"\n",
    "              f\"Best Val Accuracy: {best_val_accuracy:.2f}\")\n",
    "\n",
    "        \n",
    "        # Save the best model\n",
    "        # torch.save(best_model.state_dict(), 'best_model_3_class.pth')\n",
    "        # wandb.save('best_model_3_class.pth')\n",
    "        \n",
    "        return best_val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the hyperparameter search space\\nsweep_config = {\\n    \\'method\\': hp_config[\\'method\\'],\\n    \\'metric\\': {\\'name\\': hp_config[\\'metric_name\\'], \\'goal\\': hp_config[\\'metric_goal\\']},\\n    \\'parameters\\': {\\n        \\'model_dim\\': {\\'values\\': hp_config[\\'model_dim\\']},\\n        \\'num_heads\\': {\\'values\\': hp_config[\\'num_heads\\']},\\n        \\'num_layers\\': {\\'values\\': hp_config[\\'num_layers\\']},\\n        \\'dropout_rate\\': {\\'distribution\\': \\'uniform\\', \\'min\\': hp_config[\\'dropout_rate\\'][\\'min\\'], \\'max\\': hp_config[\\'dropout_rate\\'][\\'max\\']},\\n        \\'learning_rate\\': {\\'distribution\\': \\'log_uniform_values\\', \\'min\\': hp_config[\\'learning_rate\\'][\\'min\\'], \\'max\\': hp_config[\\'learning_rate\\'][\\'max\\']},\\n        \\'batch_size\\': {\\'values\\': hp_config[\\'batch_size\\']},\\n        \\'num_epochs\\': {\\'values\\': hp_config[\\'num_epochs\\']},\\n        \\'weight_decay\\': {\\'distribution\\': \\'log_uniform_values\\', \\'min\\': hp_config[\\'weight_decay\\'][\\'min\\'], \\'max\\': hp_config[\\'weight_decay\\'][\\'max\\']},\\n        \\'gradient_accumulation_steps\\': {\\'values\\': hp_config[\\'gradient_accumulation_steps\\']}\\n    }\\n}\\n\\n\\n# sweep_id = wandb.sweep(sweep_config, project=\"transformer_time_series\", entity=\"jitendratiwari11\")\\nsweep_id= \\'c3hgzsib\\'\\n# Start the sweep\\n#wandb.agent(sweep_id, function=objective, count=hp_config[\\'num_trials\\'])\\nwandb.agent(sweep_id, function=objective, count=hp_config[\\'num_trials\\'], project=\"transformer_time_series\", entity=\"jitendratiwari11\")\\n\\n# After all trials, find the best run\\napi = wandb.Api()\\nruns = api.runs(\"jitendratiwari11/transformer_time_series\")\\nbest_run = max(runs, key=lambda run: run.summary.get(\\'best_val_accuracy\\', 0))\\n\\nprint(f\"Best run: {best_run.name}\")\\nprint(f\"Best validation accuracy: {best_run.summary[\\'best_val_accuracy\\']}\")\\nprint(\"Best parameters:\")\\nprint(json.dumps(best_run.config, indent=4))\\n\\n# Update params.json with best parameters\\nparams.update(best_run.config)\\nwith open(\\'params.json\\', \\'w\\') as f:\\n    json.dump(params, f, indent=4)\\n\\nprint(\"Best parameters saved to params.json\")\\n\\n# # Run the best model for longer epochs\\n# print(\"Running best model for longer epochs...\")\\n# wandb.init(project=\"transformer_time_series\", name=\"best_model_long_run\", config=params, reinit=True)\\n\\n# best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=1000)\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the hyperparameter search space\n",
    "sweep_config = {\n",
    "    'method': hp_config['method'],\n",
    "    'metric': {'name': hp_config['metric_name'], 'goal': hp_config['metric_goal']},\n",
    "    'parameters': {\n",
    "        'model_dim': {'values': hp_config['model_dim']},\n",
    "        'num_heads': {'values': hp_config['num_heads']},\n",
    "        'num_layers': {'values': hp_config['num_layers']},\n",
    "        'dropout_rate': {'distribution': 'uniform', 'min': hp_config['dropout_rate']['min'], 'max': hp_config['dropout_rate']['max']},\n",
    "        'learning_rate': {'distribution': 'log_uniform_values', 'min': hp_config['learning_rate']['min'], 'max': hp_config['learning_rate']['max']},\n",
    "        'batch_size': {'values': hp_config['batch_size']},\n",
    "        'num_epochs': {'values': hp_config['num_epochs']},\n",
    "        'weight_decay': {'distribution': 'log_uniform_values', 'min': hp_config['weight_decay']['min'], 'max': hp_config['weight_decay']['max']},\n",
    "        'gradient_accumulation_steps': {'values': hp_config['gradient_accumulation_steps']}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"transformer_time_series\", entity=\"jitendratiwari11\")\n",
    "sweep_id= 'c3hgzsib'\n",
    "# Start the sweep\n",
    "#wandb.agent(sweep_id, function=objective, count=hp_config['num_trials'])\n",
    "wandb.agent(sweep_id, function=objective, count=hp_config['num_trials'], project=\"transformer_time_series\", entity=\"jitendratiwari11\")\n",
    "\n",
    "# After all trials, find the best run\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"jitendratiwari11/transformer_time_series\")\n",
    "best_run = max(runs, key=lambda run: run.summary.get('best_val_accuracy', 0))\n",
    "\n",
    "print(f\"Best run: {best_run.name}\")\n",
    "print(f\"Best validation accuracy: {best_run.summary['best_val_accuracy']}\")\n",
    "print(\"Best parameters:\")\n",
    "print(json.dumps(best_run.config, indent=4))\n",
    "\n",
    "# Update params.json with best parameters\n",
    "params.update(best_run.config)\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"Best parameters saved to params.json\")\n",
    "\n",
    "# # Run the best model for longer epochs\n",
    "# print(\"Running best model for longer epochs...\")\n",
    "# wandb.init(project=\"transformer_time_series\", name=\"best_model_long_run\", config=params, reinit=True)\n",
    "\n",
    "# best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Run the best model for longer epochs\\nprint(\"Running best model for longer epochs...\")\\nwandb.init(project=\"transformer_time_series\", name=\"experiment200\", config=params, reinit=True)\\n\\nbest_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=200, use_early_stopping=True)\\n\\n# Save the model after 1000 epochs\\ntorch.save(best_model.state_dict(), \\'best_model_after_200_epochs.pth\\')\\nprint(\"Model after 200 epochs saved to best_model_after_200_epochs.pth\")\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Run the best model for longer epochs\n",
    "print(\"Running best model for longer epochs...\")\n",
    "wandb.init(project=\"transformer_time_series\", name=\"experiment200\", config=params, reinit=True)\n",
    "\n",
    "best_model, train_losses, val_losses, val_accuracies = train_and_evaluate(params, num_epochs=200, use_early_stopping=True)\n",
    "\n",
    "# Save the model after 200 epochs\n",
    "torch.save(best_model.state_dict(), 'best_model_after_200_epochs.pth')\n",
    "print(\"Model after 200 epochs saved to best_model_after_200_epochs.pth\")\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Validation set created during preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Plot the losses and accuracy\\nplt.figure(figsize=(12, 4))\\nplt.subplot(1, 2, 1)\\nplt.plot(train_losses, label=\\'Train Loss\\')\\nplt.plot(val_losses, label=\\'Validation Loss\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Loss\\')\\nplt.legend()\\nplt.title(\\'Training and Validation Loss\\')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(val_accuracies, label=\\'Validation Accuracy\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'Accuracy (%)\\')\\nplt.legend()\\nplt.title(\\'Validation Accuracy\\')\\n\\nplt.tight_layout()\\nplt.savefig(\\'learning_curves_100_epochs.png\\')\\nplt.close()\\n\\nwandb.log({\\n    \"final_train_loss\": train_losses[-1],\\n    \"final_val_loss\": val_losses[-1],\\n    \"final_val_accuracy\": val_accuracies[-1],\\n    \"training_curves\": wandb.Image(\\'training_curves.png\\')\\n})\\n\\nprint(f\"Final train loss: {train_losses[-1]:.4f}\")\\nprint(f\"Final validation loss: {val_losses[-1]:.4f}\")\\nprint(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\\n\\nwandb.finish()\\n\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Plot the losses and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves_100_epochs.png')\n",
    "plt.close()\n",
    "\n",
    "wandb.log({\n",
    "    \"final_train_loss\": train_losses[-1],\n",
    "    \"final_val_loss\": val_losses[-1],\n",
    "    \"final_val_accuracy\": val_accuracies[-1],\n",
    "    \"training_curves\": wandb.Image('training_curves.png')\n",
    "})\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\" total nr of parameters as per best model: {sum(p.numel() for p in best_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "#estimator = estimator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Confusion matrix and Classification report\n",
    "\n",
    "For each test set, I need to give the on-line sensor data as input to the trained Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best_model_after_200_epochs.pth\n",
      "Total nr of parameters: 88163\n",
      "Confusion Matrix:\n",
      "[[72000    50     0]\n",
      " [ 2950  7050     0]\n",
      " [    0  1039  4461]]\n",
      "\n",
      "Custom Classification Report:\n",
      "Class 0:\n",
      "  F1-score: 0.9796\n",
      "  F2-score: 0.9913\n",
      "  R2-score: 0.7975\n",
      "\n",
      "Class 1:\n",
      "  F1-score: 0.7773\n",
      "  F2-score: 0.7323\n",
      "  R2-score: 0.6163\n",
      "\n",
      "Class 2:\n",
      "  F1-score: 0.8957\n",
      "  F2-score: 0.8429\n",
      "  R2-score: 0.8423\n",
      "\n",
      "Class macro avg:\n",
      "  F1-score: 0.8842\n",
      "  F2-score: 0.8555\n",
      "  R2-score: 0.7520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import TransformerTimeSeriesModel\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score, r2_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def custom_collate(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.array(labels), dtype=torch.long)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, y_prob):\n",
    "    classes = np.unique(y_true)\n",
    "    report = {}\n",
    "    \n",
    "    for cls in classes:\n",
    "        true_cls = (y_true == cls)\n",
    "        pred_cls = (y_pred == cls)\n",
    "        prob_cls = y_prob[:, cls]\n",
    "        \n",
    "        f1 = f1_score(true_cls, pred_cls, average='binary')\n",
    "        f2 = fbeta_score(true_cls, pred_cls, beta=2, average='binary')\n",
    "        r2 = r2_score(true_cls.astype(int), prob_cls)\n",
    "        \n",
    "        report[cls] = {\n",
    "            'F1-score': f1,\n",
    "            'F2-score': f2,\n",
    "            'R2-score': r2\n",
    "        }\n",
    "    \n",
    "    # Calculate macro average\n",
    "    macro_f1 = np.mean([report[cls]['F1-score'] for cls in classes])\n",
    "    macro_f2 = np.mean([report[cls]['F2-score'] for cls in classes])\n",
    "    macro_r2 = np.mean([report[cls]['R2-score'] for cls in classes])\n",
    "    \n",
    "    report['macro avg'] = {\n",
    "        'F1-score': macro_f1,\n",
    "        'F2-score': macro_f2,\n",
    "        'R2-score': macro_r2\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "model_path = 'best_model_after_200_epochs.pth'\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    estimator = TransformerTimeSeriesModel(\n",
    "        input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=params['model_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Load the model and move it to the appropriate device\n",
    "    estimator.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Total nr of parameters: {sum(p.numel() for p in estimator.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    estimator = estimator.to(device)\n",
    "    estimator.eval()\n",
    "\n",
    "    test_dataset = MultivariateTimeSeriesDataset(seq_array_test, dummy_label_array_test, params['seq_length'])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)   # Move inputs to GPU\n",
    "            labels = labels.to(device)   # Move labels to GPU\n",
    "\n",
    "            outputs = estimator(inputs)\n",
    "            probs = torch.softmax(outputs, dim=2)\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            all_preds.extend(preds.cpu().numpy().reshape(-1))  # Move predictions to CPU and store\n",
    "            all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))  # Move labels to CPU\n",
    "            all_probs.extend(probs.cpu().numpy().reshape(-1, params['num_classes']))  # Move probs to CPU\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Custom classification report\n",
    "    report = custom_classification_report(all_labels, all_preds, all_probs)\n",
    "    print(\"\\nCustom Classification Report:\")\n",
    "    for cls in report:\n",
    "        print(f\"Class {cls}:\")\n",
    "        for metric, value in report[cls].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(f\"No saved model found at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Class 0:\n",
      "Precision: 0.9606\n",
      "Recall: 0.9993\n",
      "F1-score: 0.9796\n",
      "ROC AUC: 0.9973\n",
      "Accuracy: 0.9993\n",
      "R2 Score: 0.7975\n",
      "F2 Score: 0.9913\n",
      "---\n",
      "Metrics for Class 1:\n",
      "Precision: 0.8662\n",
      "Recall: 0.7050\n",
      "F1-score: 0.7773\n",
      "ROC AUC: 0.9831\n",
      "Accuracy: 0.7050\n",
      "R2 Score: 0.6163\n",
      "F2 Score: 0.7323\n",
      "---\n",
      "Metrics for Class 2:\n",
      "Precision: 1.0000\n",
      "Recall: 0.8111\n",
      "F1-score: 0.8957\n",
      "ROC AUC: 0.9985\n",
      "Accuracy: 0.8111\n",
      "R2 Score: 0.8423\n",
      "F2 Score: 0.8429\n",
      "---\n",
      "Confusion Matrix:\n",
      "[[72000    50     0]\n",
      " [ 2950  7050     0]\n",
      " [    0  1039  4461]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix, r2_score, fbeta_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def evaluate_multiclass_model(y_true, y_pred, y_prob):\n",
    "    # Move everything to the CPU before using sklearn functions\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_prob = y_prob.cpu().numpy()\n",
    "    \n",
    "    # Compute precision, recall, and F1 score for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    \n",
    "    # Compute ROC AUC for each class\n",
    "    n_classes = y_prob.shape[1]\n",
    "    y_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    roc_auc = []\n",
    "    for i in range(n_classes):\n",
    "        roc_auc.append(roc_auc_score(y_bin[:, i], y_prob[:, i]))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Compute class-specific accuracy\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Compute R2 score for each class\n",
    "    r2_scores = []\n",
    "    for i in range(n_classes):\n",
    "        r2 = r2_score((y_true == i).astype(int), y_prob[:, i])\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Compute F2 score for each class\n",
    "    f2_scores = []\n",
    "    for i in range(n_classes):\n",
    "        f2 = fbeta_score((y_true == i).astype(int), (y_pred == i).astype(int), beta=2)\n",
    "        f2_scores.append(f2)\n",
    "    \n",
    "    # Create a dictionary to store all metrics\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'r2_score': r2_scores,\n",
    "        'f2_score': f2_scores,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Move data to the appropriate device (CUDA if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming all_labels, all_preds, and all_probs are on CUDA, move them to the device\n",
    "all_labels = torch.tensor(all_labels).to(device)\n",
    "all_preds = torch.tensor(all_preds).to(device)\n",
    "all_probs = torch.tensor(all_probs).to(device)\n",
    "\n",
    "# After your model prediction\n",
    "metrics = evaluate_multiclass_model(all_labels, all_preds, all_probs)\n",
    "\n",
    "# Print or visualize the results\n",
    "class_names = ['Class 0', 'Class 1', 'Class 2']  # Replace with your actual class names\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"Metrics for {class_name}:\")\n",
    "    print(f\"Precision: {metrics['precision'][i]:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall'][i]:.4f}\")\n",
    "    print(f\"F1-score: {metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc'][i]:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['class_accuracy'][i]:.4f}\")\n",
    "    print(f\"R2 Score: {metrics['r2_score'][i]:.4f}\")\n",
    "    print(f\"F2 Score: {metrics['f2_score'][i]:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics['confusion_matrix'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best_model_after_200_epochs.pth\n",
      " total nr of parameters as per best model: 88163\n",
      "Confusion Matrix:\n",
      "[[72000    50     0]\n",
      " [ 2950  7050     0]\n",
      " [    0  1039  4461]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     72050\n",
      "           1       0.87      0.70      0.78     10000\n",
      "           2       1.00      0.81      0.90      5500\n",
      "\n",
      "    accuracy                           0.95     87550\n",
      "   macro avg       0.94      0.84      0.88     87550\n",
      "weighted avg       0.95      0.95      0.95     87550\n",
      "\n",
      "Average R2 Score: 0.7520\n",
      "F2 Score (beta=2): 0.9524\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import TransformerTimeSeriesModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report, r2_score, fbeta_score\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'best_model_after_200_epochs.pth'# This file then contains the already trained network, so that you don't have to retrain every time\n",
    "\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = TransformerTimeSeriesModel(\n",
    "    input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=params['model_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Load the state dict\n",
    "    estimator.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\" total nr of parameters as per best model: {sum(p.numel() for p in estimator.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    \n",
    "    # Move the model to the appropriate device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    estimator = estimator.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    estimator.eval()\n",
    "\n",
    "    test_dataset = MultivariateTimeSeriesDataset(seq_array_test, dummy_label_array_test, params['seq_length'])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []  # To store probabilities for R2 score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = estimator(inputs)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=2)\n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "            all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "            all_probs.extend(probs.cpu().numpy().reshape(-1, params['num_classes']))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print classification report\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Compute R2 score\n",
    "    # For multiclass classification, we compute R2 for each class and then take the average\n",
    "    r2_scores = []\n",
    "    for i in range(params['num_classes']):\n",
    "        true_binary = (all_labels == i).astype(int)\n",
    "        pred_proba = all_probs[:, i]\n",
    "        r2 = r2_score(true_binary, pred_proba)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    print(f\"Average R2 Score: {avg_r2:.4f}\")\n",
    "\n",
    "    # Compute F2 score\n",
    "    f2_score = fbeta_score(all_labels, all_preds, beta=2, average='weighted')\n",
    "    print(f\"F2 Score (beta=2): {f2_score:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"No saved model found at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import TransformerTimeSeriesModel\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score, r2_score\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (keep your existing functions)\n",
    "\n",
    "model_path = 'best_model_after_200_epochs.pth'\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    estimator = TransformerTimeSeriesModel(\n",
    "        input_dim=params['input_dim'],\n",
    "        seq_length=params['seq_length'],\n",
    "        num_classes=params['num_classes'],\n",
    "        model_dim=params['model_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Load the model and move it to the appropriate device\n",
    "    estimator.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Total nr of parameters: {sum(p.numel() for p in estimator.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    estimator = estimator.to(device)\n",
    "    estimator.eval()\n",
    "\n",
    "    test_dataset = MultivariateTimeSeriesDataset(seq_array_test, dummy_label_array_test, params['seq_length'])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate)\n",
    "    \n",
    "    # Initialize lists to store epoch-wise metrics\n",
    "    epoch_r2_scores = []\n",
    "    epoch_f1_scores = []\n",
    "    epoch_f2_scores = []\n",
    "\n",
    "    # Simulate metrics for each epoch\n",
    "    num_epochs = 200  # Assuming 200 epochs as per your model name\n",
    "    for epoch in range(num_epochs):\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = estimator(inputs)\n",
    "                probs = torch.softmax(outputs, dim=2)\n",
    "\n",
    "                _, preds = torch.max(outputs, dim=2)\n",
    "                all_preds.extend(preds.cpu().numpy().reshape(-1))\n",
    "                all_labels.extend(labels.argmax(dim=2).cpu().numpy().reshape(-1))\n",
    "                all_probs.extend(probs.cpu().numpy().reshape(-1, params['num_classes']))\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        # Calculate metrics for this epoch\n",
    "        r2 = r2_score(all_labels, all_probs.argmax(axis=1))\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        f2 = fbeta_score(all_labels, all_preds, beta=2, average='macro')\n",
    "\n",
    "        epoch_r2_scores.append(r2)\n",
    "        epoch_f1_scores.append(f1)\n",
    "        epoch_f2_scores.append(f2)\n",
    "\n",
    "    # Plot R2, F1, and F2 scores vs epochs\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_r2_scores)\n",
    "    plt.title('R2 Score vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_f1_scores)\n",
    "    plt.title('F1 Score vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_f2_scores)\n",
    "    plt.title('F2 Score vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F2 Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_vs_epochs.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Plots saved as 'metric_vs_epochs.png'\")\n",
    "\n",
    "else:\n",
    "    print(f\"No saved model found at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vloF6HXQ6Pt4"
   },
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 1000\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ-vs1-x6Pt5"
   },
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles\n",
    "array_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KP5yjWw6Pt5"
   },
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqIzk2b96Pt5"
   },
   "outputs": [],
   "source": [
    "#test_df['cycle_norm'] = test_df['cycle']\n",
    "#test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm_df['cycle_norm'] = pdm_df['cycle']\n",
    "pdm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBklUcnv6Pt5"
   },
   "source": [
    "## PdM policy evaluation on a the whole (test data+ validation=pdm_df) set (ids 81 to 101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfyabzdL6Pt_"
   },
   "outputs": [],
   "source": [
    "#costs_rep_array   = np.zeros(10) # An array to store costs related to replacements.\n",
    "\n",
    "#costs_delay_array = np.zeros(10) # An array to store costs related to delays.\n",
    "#costs_stock_array = np.zeros(10) # An array to store costs related to stock.\n",
    "\n",
    "#t_LC_array        = np.zeros(10) # An array to store lead time.\n",
    "#t_order_array     = np.zeros(10) # An array to store order time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsWT1ebGbOdz"
   },
   "source": [
    "> 1. Initializes a counter variable to 0.\n",
    "> 2. Iterates over unique IDs in the `pdm_df` DataFrame.\n",
    "> 3. For each ID:\n",
    ">> * Sets flags for preventive replacement and ordering to False.<br>\n",
    ">> * Iterates over cycles within the range of the DataFrame.<br>\n",
    ">> * Checks if the current cycle is in the `array_decisions`.<br>\n",
    ">> * If it is, preprocesses the validation data for the Transformer model.<br>\n",
    ">> * Predicts the probability of RUL being smaller than w1 and DT (decision time) using the trained model.<br>\n",
    ">> * Evaluates decision heuristics:\n",
    ">>> * If no order has been placed yet and the cost of preventive replacement is less than or equal to the cost of waiting until `w1`, orders the component and sets the order time.<br>\n",
    ">>> * If the cost of preventive replacement is less than or equal to the cost of waiting until `DT`, performs preventive replacement, calculates related costs, and breaks the loop.<br>\n",
    ">> If preventive replacement is not performed:\n",
    ">>> * Sets the component failure time to the last cycle in the ID's data.<br>\n",
    ">>> * Sets replacement costs to `C_c`.<br>\n",
    ">>> * Calculates delay costs based on whether an order has been placed.<br>\n",
    ">> * Prints diagnostic information for each iteration.\n",
    ">> * Increments the counter.\n",
    "\n",
    "\n",
    "This code essentially simulates a decision-making process for component maintenance based on predictive models and cost considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDM Policy1\n",
    "\n",
    "1. `Policy Overview:` We first consider the simple dynamic PdM decision setting, in which one determines at each time step\n",
    "    tk whether a component should be preventively replaced or not. The assumption here is that the new\n",
    "    component is readily available when a preventive replacement is decided or a corrective replacement is\n",
    "    imposed. <br>\n",
    "2. `Decision Making Process:`\n",
    "> At each time step tk = k * ΔT, the policy decides whether to take action or not.<br>\n",
    "> The action arep,k can be either:\n",
    ">> a) DN (Do Nothing) <br>\n",
    ">> b) PR (Preventive Replacement)<br>\n",
    "\n",
    "3. `Decision Rule:`\n",
    "> If Pr(RULpred,k ≤ ΔT) < pthres, then Do Nothing (DN) <br>\n",
    "> Otherwise, perform Preventive Replacement (PR) <br>\n",
    "where:\n",
    "> RULpred,k is the predicted Remaining Useful Life at time tk <br>\n",
    "> ΔT is the time step <br>\n",
    "> pthres is a variable heuristic threshold <br>\n",
    "4. `Threshold Determination:`\n",
    "> Initially, pthres is set to cp/cc <br>\n",
    "> cp is the cost of preventive replacement <br>\n",
    "> cc is the cost of component failure <br>\n",
    "5. `Cost Considerations:`\n",
    "> PR action costs cp <br>\n",
    "> DN action risks a potential cost of Pr(RULpred,k ≤ ΔT) * cc <br>\n",
    "6. `Rationale:`\n",
    "> PR is performed only when its cost is less than the predicted risk of failure in the next time step. <br>\n",
    "7. `Input Requirements:`\n",
    ">  `current_cycle:` variable represents the current time step or cycle within the sequence of data for a specific engine or component. It is used to iterate through the sequence of cycles for each unique engine ID in the dataset. <br>\n",
    ">  The policy needs Pr(RULpred,k ≤ ΔT) from the prognostic algorithm. <br>\n",
    "8. `Outcome:`\n",
    "> The policy informs replacement decisions for each component. <br>\n",
    "> It determines C_rep(i) (replacement cost) and Tlc(i) (lifecycle time) for each component i. <br>\n",
    "\n",
    ">> 1. `t_LC_array(Lifecycle Time) :`\n",
    ">>> * This array represents `Tlc(i) = min[T_R(i), T_F(i)]` for each component. <br>\n",
    ">>> * In the code, it's set in two cases:\n",
    ">>> a) When a preventive replacement is decided: `t_LC_array[counter] = params['seq_length'] + current_cycle`\n",
    ">>> b) When no preventive replacement occurred (implying failure): `t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]`\n",
    ">>> * This aligns with the definition of `Tlc(i)` being the minimum of preventive replacement time or failure time.\n",
    ">> 2. `costs_rep_array (Replacement Cost):`\n",
    ">>> * This array represents C_rep(i) for each component.\n",
    ">>> * In the code, it's set as follows:\n",
    ">>> a) For preventive replacement: `costs_rep_array[counter] = C_p` <br>\n",
    ">>> b) For corrective replacement (when no preventive replacement occurred): `costs_rep_array[counter] = C_c` <br>\n",
    ">>> This directly implements the condition as mentioned in equation 8: `C_rep(i) = (cp, if T_R(i) < T_F(i), cc, else.`\n",
    "\n",
    "\n",
    "9. `t_order_array:`\n",
    "> * Meaning: This array stores the cycle times at which components are ordered. <br>\n",
    "> * Significance: It helps track when preventive maintenance actions are initiated, allowing for analysis of the timing of maintenance decisions. <br>\n",
    "\n",
    "10. `t_LC_array:`\n",
    "> Meaning: This array likely stores the lifecycle times for each component. <br>\n",
    "> Significance: It represents either the time of preventive replacement or the time of failure for each component, which is crucial for evaluating the effectiveness of the maintenance policy. <br>\n",
    "\n",
    "11. `costs_rep_array:`\n",
    "> * Meaning: This array stores the replacement costs for each component. <br>\n",
    "> * Significance: It captures the financial impact of replacements, whether they are preventive (C_p) or corrective (C_c). This is essential for assessing the cost-effectiveness of the maintenance strategy. <br>\n",
    "\n",
    "12. `costs_delay_array:`\n",
    "> * Meaning: This array stores the costs associated with delays in component replacement. <br>\n",
    "> * Significance: It represents the financial penalties incurred when a component fails before a replacement arrives, helping to quantify the impact of maintenance timing on overall costs. <br>\n",
    "\n",
    "13. `costs_stock_array:`\n",
    "> Meaning: This array stores the costs related to holding replacement components in stock. <br>\n",
    "> Significance: It captures the inventory holding costs when components are ordered too early, balancing the trade-off between early ordering to prevent failures and the costs of storing components. <bR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM policy 1 without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[cols_normalize_train])\n",
    "\n",
    "\n",
    "def prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols):\n",
    "    # Use data up to the current cycle, with a sequence length of params['seq_length']\n",
    "    norm_pdm_df = pd.DataFrame(\n",
    "        scaler.transform(pdm_df[pdm_df['id'] == id][cols_normalize_train][current_cycle - params['seq_length']:current_cycle]),\n",
    "        columns=cols_normalize_train,\n",
    "        index=pdm_df[pdm_df['id'] == id][current_cycle - params['seq_length']:current_cycle].index\n",
    "    )\n",
    "\n",
    "    join_df = pdm_df[pdm_df['id'] == id][current_cycle - params['seq_length']:current_cycle][pdm_df[pdm_df['id'] == id][current_cycle - params['seq_length']:current_cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "    \n",
    "    pdm_df_eval_online = join_df.reindex(columns=pdm_df[pdm_df['id'] == id][current_cycle - params['seq_length']:current_cycle].columns)\n",
    "    \n",
    "    seq_array_test_k = pdm_df_eval_online[sequence_cols].values\n",
    "    seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], len(sequence_cols))\n",
    "    \n",
    "    return seq_array_test_k\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[cols_normalize_train])\n",
    "\n",
    "def prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols):\n",
    "    \"\"\"\n",
    "    Prepare the input sequence data for the model, ensuring the output has a consistent shape of (1, seq_length, num_features).\n",
    "\n",
    "    Args:\n",
    "        pdm_df (DataFrame): The PdM data frame.\n",
    "        scaler (scaler object): Scaler for normalizing the data.\n",
    "        id (int): The ID of the engine or component.\n",
    "        current_cycle (int): The current cycle index.\n",
    "        params (dict): Dictionary of parameters.\n",
    "        cols_normalize_train (list): List of columns to normalize.\n",
    "        sequence_cols (list): List of sequence columns.\n",
    "\n",
    "    Returns:\n",
    "        seq_array_test_k (np.array): The prepared sequence array of shape (1, seq_length, num_features).\n",
    "    \"\"\"\n",
    "    # Define the start and end indices for slicing\n",
    "    start_idx = max(0, current_cycle - params['seq_length'])\n",
    "    end_idx = current_cycle\n",
    "\n",
    "    # Select the required data for normalization\n",
    "    norm_pdm_df = pd.DataFrame(\n",
    "        scaler.transform(pdm_df[pdm_df['id'] == id][cols_normalize_train].iloc[start_idx:end_idx]),\n",
    "        columns=cols_normalize_train,\n",
    "        index=pdm_df[pdm_df['id'] == id].iloc[start_idx:end_idx].index\n",
    "    )\n",
    "\n",
    "    # Combine normalized data with other columns\n",
    "    join_df = pdm_df[pdm_df['id'] == id].iloc[start_idx:end_idx][pdm_df[pdm_df['id'] == id].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "\n",
    "    # Reindex and ensure columns order\n",
    "    pdm_df_eval_online = join_df.reindex(columns=pdm_df[pdm_df['id'] == id].columns)\n",
    "\n",
    "    # Extract the sequence array\n",
    "    seq_array_test_k = pdm_df_eval_online[sequence_cols].values\n",
    "\n",
    "    # Ensure it has the correct shape\n",
    "    seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], len(sequence_cols))\n",
    "    \n",
    "    return seq_array_test_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import lognorm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.integrate import quad\n",
    "from scipy import optimize\n",
    "\n",
    "def fit_lognormal_cdf(probabilities):\n",
    "    # probabilities shape is (50, 3)\n",
    "    # We'll use the cumulative probability of failure over time\n",
    "    \n",
    "    time_steps = np.arange(1, probabilities.shape[0] + 1)\n",
    "    cumulative_failure_prob = np.cumsum(probabilities[:, 2])  # Assuming class 2 is \"failure imminent\"\n",
    "    \n",
    "    # Ensure probabilities are within (0, 1)\n",
    "    cumulative_failure_prob = np.clip(cumulative_failure_prob, 1e-6, 1-1e-6)\n",
    "    \n",
    "    # Define the lognormal CDF\n",
    "    def lognorm_cdf(x, s, loc, scale):\n",
    "        return lognorm.cdf(x, s, loc, scale)\n",
    "    \n",
    "    # Define the error function to minimize\n",
    "    def error(params, x, y):\n",
    "        return np.sum((lognorm_cdf(x, *params) - y) ** 2)\n",
    "    \n",
    "    # Initial guess for parameters\n",
    "    initial_guess = [1, 0, np.mean(time_steps)]\n",
    "    \n",
    "    # Fit the distribution\n",
    "    result = optimize.minimize(error, initial_guess, args=(time_steps, cumulative_failure_prob))\n",
    "    \n",
    "    # Extract fitted parameters\n",
    "    s, loc, scale = result.x\n",
    "    \n",
    "    # Convert lognorm parameters to mu and sigma\n",
    "    sigma = s\n",
    "    #mu = np.log(scale)\n",
    "    mu = np.log(scale) if scale > 0 else np.nan    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def plot_probabilities(pdm_df, estimator, scaler, params, cols_normalize_train, sequence_cols, device, id_range):\n",
    "    for id in id_range:\n",
    "        if id not in pdm_df['id'].unique():\n",
    "            print(f\"ID {id} not found in the dataset. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        probabilities_list = []\n",
    "        cycles = []\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            seq_array_test_k = prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "            seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = estimator(seq_tensor)\n",
    "                probabilities = torch.softmax(outputs.squeeze(), dim=1).cpu().numpy()\n",
    "\n",
    "            probabilities_list.append(probabilities)\n",
    "            cycles.append(current_cycle)\n",
    "\n",
    "        probabilities_array = np.array(probabilities_list)\n",
    "        cycles = np.array(cycles)\n",
    "        # Plot continuous probabilities\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        continuous_cycles = np.linspace(cycles.min(), cycles.max(), 1000)\n",
    "        for i in range(3):  # Assuming 3 classes\n",
    "            interp_func = interp1d(cycles, probabilities_array[:, -1, i], kind='cubic')\n",
    "            continuous_probs = interp_func(continuous_cycles)\n",
    "            plt.plot(continuous_cycles, continuous_probs, '-', label=f'Class {i}')\n",
    "        plt.xlabel('Cycle')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title(f'Continuous Probabilities for ID {id}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "# Usage\n",
    "id_range = range(81, 82)  # This will plot for IDs 1, 2, 3, 4, and 5\n",
    "plot_probabilities(pdm_df, estimator, scaler, params, cols_normalize_train, sequence_cols, device, id_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm_df.head(241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    \n",
    "    # Initialize dictionaries to store probabilities\n",
    "    prob_RUL_smaller_DT_dict = {}  # All probabilities\n",
    "    replacement_probs_dict = {}    # Probabilities at replacement\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        preventive_replacement = False\n",
    "        prob_RUL_smaller_DT_dict[id] = []\n",
    "        replacement_probs_dict[id] = []  # Store probabilities where replacement happens\n",
    "        \n",
    "        # Loop through each decision point\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data                           \n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Calculate probabilities\n",
    "                prob_RUL_smaller_DT = probabilities[-1, 2]\n",
    "                \n",
    "                # Store probability for plotting\n",
    "                \n",
    "                prob_RUL_smaller_DT_dict[id].append((current_cycle, prob_RUL_smaller_DT))\n",
    "                \n",
    "                \n",
    "                # Apply PdM policy 1\n",
    "                #pthres = C_p / C_c  # Heuristic threshold\n",
    "                pthres = 0.5  # Heuristic threshold\n",
    "\n",
    "                if prob_RUL_smaller_DT < pthres:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}\")\n",
    "                else:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}\")\n",
    "\n",
    "                if action == \"PR\":\n",
    "                    #T_R_i =  current_cycle + params['seq_length']\n",
    "                    T_R_i =  current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)  # Length of life cycle of ith component\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    \n",
    "                    # Store probability where replacement is made\n",
    "                    replacement_probs_dict[id].append((current_cycle, prob_RUL_smaller_DT))\n",
    "                    \n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, prob_RUL_smaller_DT_dict, replacement_probs_dict\n",
    "# Usage\n",
    "\n",
    "#t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, prob_RUL_smaller_DT_dict, replacement_probs_dict = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "                                                                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Function and Plot both overall and replacement Probabilities to track the Replacement of components.\n",
    "\n",
    "After running the function, you can plot the stored probabilities for each component:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probabilities for each component\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for id, probs in prob_RUL_smaller_DT_dict.items():\n",
    "    if probs:  # Check if there are any probabilities to plot\n",
    "        cycles, prob_values = zip(*probs)\n",
    "        plt.plot(cycles, prob_values, linestyle='-', marker='o', label=f'Component {id} Probabilities')\n",
    "\n",
    "# Plot the replacement probabilities for each component\n",
    "for id, probs in replacement_probs_dict.items():\n",
    "    if probs:  # Check if there are any replacement probabilities to plot\n",
    "        cycles, prob_values = zip(*probs)\n",
    "        plt.plot(cycles, prob_values, linestyle='', marker='x', markersize=8, label=f'Component {id} Replacements')\n",
    "\n",
    "plt.xlabel('Cycle')\n",
    "plt.ylabel('Probability of RUL ≤ ΔT')\n",
    "plt.title('Probability of Remaining Useful Life (RUL) for Discrete Time Steps and Replacements')\n",
    "\n",
    "# Position the legend outside the plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a JPG file\n",
    "plt.savefig('rul_probabilities_plot.jpg', format='jpg', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM Policy 1 with Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "                   \n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "\n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                # Calculate probabilities\n",
    "                prob_RUL_smaller_w1 = (probabilities[-1, 1] + probabilities[-1, 2])\n",
    "                prob_RUL_smaller_DT = probabilities[-1, 2]\n",
    "\n",
    "                # Apply optimized heuristic thresholds\n",
    "                p_order_thres = 0.11  # Optimized heuristic threshold for ordering\n",
    "                p_rep_thres = 0.5     # Optimized heuristic threshold for replacement\n",
    "\n",
    "                # Ordering decision\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    t_order_array[counter] = current_cycle \n",
    "                    order = True\n",
    "\n",
    "                # Replacement decision\n",
    "                if prob_RUL_smaller_DT < p_rep_thres:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}\")\n",
    "                else:\n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}\")\n",
    "\n",
    "\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                    costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "                                                                                               \n",
    "# Usage\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PDM policy 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Objective:` PDM policy 2 aims to find the optimal time for preventive replacement (T_{R,k}*) by minimizing the long-run expected maintenance cost per unit time. <br>\n",
    "2. `Decision Rule:` At each time step t_k, the policy decides:\n",
    "> * Perform Preventive Replacement (PR) if t_k + ΔT ≥ T_{R,k}*\n",
    "> * Do Nothing (DN) otherwise\n",
    "3. `Optimization Problem:` The policy solves an optimization problem at each time step to find T_{R,k}* by minimizing the objective function f(T_{R,k}) given in Equation 12. <br>\n",
    "4. `Components of the Objective Function (Equation 12):`\n",
    "> $f\\left(T_{\\mathrm{R}, k}\\right)=\\frac{\\mathrm{E}\\left[C_{\\mathrm{rep}}\\left(T_{\\mathrm{R}, k}\\right)\\right]}{\\mathrm{E}\\left[T_{\\mathrm{lc}}\\left(T_{\\mathrm{R}, k}\\right)\\right]}=\\frac{P_{\\mathrm{PR}} \\cdot c_{\\mathrm{p}}+\\left(1-P_{\\mathrm{PR}}\\right) \\cdot c_{\\mathrm{c}}}{P_{\\mathrm{PR}} \\cdot\\left(T_{\\mathrm{R}, k}\\right)+\\int_t^{T_{\\mathrm{R}, k}} t f_{R U L_{\\mathrm{Pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t}$\n",
    "> * E[C_{rep}(T_{R,k})]: Expected cost of replacement\n",
    "> * E[T_{lc}(T_{R,k})]: Expected lifecycle time\n",
    "> * P_{PR}: Probability of preventive replacement (defined in Equation 13)\n",
    "> * c_p: Cost of preventive replacement\n",
    "> * c_c: Cost of corrective replacement (failure)\n",
    "> * f_{RUL_{pred,k}}(t): Full distribution of the RUL prediction at time t_k\n",
    "> * The find_optimal_replacement_time function implements the objective function from PDM policy 2. It finds the optimal replacement time T_R_k by minimizing the expected cost per unit time. <br>\n",
    "\n",
    "5. Interpretation of Equation 12:\n",
    "> * Numerator: Represents the expected cost, considering both preventive and corrective replacements.\n",
    "> * Denominator: Represents the expected lifecycle time.\n",
    "> * By minimizing this ratio, the policy aims to find the optimal balance between cost and component lifetime.\n",
    "\n",
    "\n",
    "6. Probability of Preventive Replacement(Equation 13): \n",
    "> P_{PR} represents the probability that the component will be preventively replaced at T_{R,k}.\n",
    "> It's calculated by integrating the RUL prediction distribution from T_{R,k} to infinity.\n",
    "\n",
    "> $$\n",
    "P_{\\mathrm{PR}}=\\int_{T_{\\mathrm{R}, k}}^{\\infty} f_{R U L_{\\mathrm{Pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t\n",
    "$$       \n",
    "\n",
    "7. Full RUL Distribution: Unlike PDM policy 1, this policy uses the full distribution of the Remaining Useful Life (RUL) prediction, allowing for more nuanced decision-making.<br>\n",
    "\n",
    "8. The function `find_optimal_replacement_time` and its Parameters: function is designed to determine the optimal time for preventive replacement in a Predictive Maintenance (PdM) system. It uses a lognormal distribution fitted to the predicted probabilities of component failure to calculate the expected cost per unit time and find the optimal replacement time.<br>\n",
    " , representing the probability of needing a preventive replacement after T_R_K. <br>\n",
    "> `probabilities:` The predicted probabilities of class1 and class2 from the model. <br>\n",
    "> C_p, C_c, current_cycle, seq_length <br>\n",
    "> `Fitting the Lognormal Distribution:` `mu, sigma = fit_lognormal_cdf(probabilities):` This fits a lognormal distribution to the given probabilities, returning the parameters mu and sigma..\n",
    "> * `Probability of Preventive Replacement (P_PR):` This is the probability that the component survives until T_R_k. <br>\n",
    "> * `Expected Replacement Cost (E_C_rep) :` It's a weighted sum of preventive (C_p) and corrective (C_c) replacement costs.<br> \n",
    "> * `Expected Lifecycle Time (E_T_lc) :` The first term (P_PR * T_R_k) is the expected time if preventive replacement occurs\n",
    "and The sum calculates the expected time if failure occurs before T_R_k.<br>\n",
    "\n",
    "9. `Optimization:`\n",
    "> * Uses scipy's `minimize_scalar` function to find the T_R_k that minimizes the objective function.<br>\n",
    "> * The search is bounded between the current cycle and the end of the sequence.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import lognorm\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "from scipy.integrate import quad, IntegrationWarning\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_2_Without_ordering(estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    optimal_replacement_times = {}\n",
    "    rul_distributions = {}\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        preventive_replacement = False\n",
    "        optimal_replacement_times[id] = []\n",
    "        rul_distributions[id] = []\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle, DT, params)\n",
    "                optimal_replacement_times[id].append((current_cycle, T_R_k_optimal))\n",
    "                \n",
    "                # Store RUL distribution\n",
    "                mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "                rul_distributions[id].append((current_cycle, mu, sigma))\n",
    "\n",
    "                if DT >= T_R_k_optimal:     \n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle, DT, params):\n",
    "    \"\"\"Find the optimal replacement time for PdM policy 2.\"\"\"\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "\n",
    "    def objective_function(T_R_k):\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "        E_C_rep = P_PR * C_p + (1 - P_PR) * C_c\n",
    "        E_T_lc = P_PR * T_R_k + expected_time_to_failure(current_cycle, T_R_k, mu, sigma)\n",
    "        return E_C_rep / max(E_T_lc, 1e-6)  # Avoid division by zero\n",
    "\n",
    "    try:\n",
    "        result = minimize_scalar(objective_function, bounds=(0,  2 * DT), method='bounded')\n",
    "        return result.x\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization error: {str(e)}\")\n",
    "        return current_cycle + DT  # Return a default value\n",
    "\n",
    "\n",
    "\n",
    "def expected_time_to_failure(current_cycle, T_R_k, mu, sigma):\n",
    "    \n",
    "    def integrand(t):\n",
    "        return t * lognorm.pdf(t - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", IntegrationWarning)\n",
    "            integral_value, error = quad(integrand, current_cycle, T_R_k, \n",
    "                                         limit=200, epsabs=1e-6, epsrel=1e-6,\n",
    "                                         points=[current_cycle, T_R_k])\n",
    "        return integral_value\n",
    "    except Exception as e:\n",
    "        print(f\"Integration error: {str(e)}\")\n",
    "        # Return a default value or handle the error as appropriate\n",
    "        return T_R_k - current_cycle  # Simple fallback: assume failure at T_R_k\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def expected_time_to_failure(current_cycle,T_R_k, mu, sigma):\n",
    "    \n",
    "    def integrand(t):\n",
    "        return t * lognorm.pdf(t - current_cycle, mu, sigma)\n",
    "\n",
    "    #integral_value, error = quad(integrand, current_cycle, T_R_k, limit=200)\n",
    "    integral_value, error = quad(integrand, current_cycle, T_R_k, limit=200, epsabs=1e-6, epsrel=1e-6)\n",
    "\n",
    "    return integral_value\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Run the optimization function and compute the RUL distribution\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions = calculate_probabilities_for_pdm_policy_2_Without_ordering(\n",
    "    estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the RUL Distribution\n",
    "\n",
    " * Use the modified functions to calculate and plot the RUL distribution for each cycle: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "def plot_rul_distribution_for_each_id(rul_distributions, array_decisions, pdm_df):\n",
    "    \"\"\"\n",
    "    Plot the RUL distribution per cycle for each engine ID from cycle 1 to its failure cycle.\n",
    "    \n",
    "    Args:\n",
    "        rul_distributions (dict): Dictionary containing RUL distribution data for each engine ID.\n",
    "        array_decisions (numpy.ndarray): Array of discrete decision points (time steps).\n",
    "        pdm_df (pd.DataFrame): The data frame containing cycle information for each engine ID.\n",
    "    \"\"\"\n",
    "    for id, rul_data in rul_distributions.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot for each cycle from 1 to the failure cycle\n",
    "        for (current_cycle, mu, sigma) in rul_data:\n",
    "            # Only plot if cycle is in the decision points\n",
    "            if current_cycle in array_decisions:\n",
    "                # Generate x values for RUL distribution from current cycle to the end\n",
    "                discrete_times = np.arange(current_cycle, pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1] + 1)\n",
    "\n",
    "                # Calculate the lognormal PDF for the RUL distribution\n",
    "                f_rul_pred_k = lognorm.pdf(discrete_times - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "                plt.plot(discrete_times, f_rul_pred_k, linestyle='-', marker='o', label=f'RUL Distribution at Cycle {current_cycle}')\n",
    "\n",
    "        plt.xlabel('Cycle')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.title(f'RUL Distribution from Cycle 1 to Failure for Engine ID {id}')\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure as a JPG file for each engine ID\n",
    "        plt.savefig(f'rul_distribution_engine_{id}.jpg', format='jpg', bbox_inches='tight')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "# Calculate probabilities, RUL distributions, and optimal replacement times\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions = calculate_probabilities_for_pdm_policy_2_Without_ordering(\n",
    "    estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device\n",
    ")\n",
    "\n",
    "# Plot the RUL distributions from cycle 1 to the failure cycle for each engine ID\n",
    "plot_rul_distribution_for_each_id(rul_distributions, array_decisions, pdm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Objective Function per Cycle\n",
    " * use the returned optimal_replacement_times to visualize the replacement times over cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_combined_optimal_replacement_time(optimal_replacement_times, save_path='optimal_replacement_times.jpg'):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Use a color map with distinct colors\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(optimal_replacement_times)))\n",
    "    \n",
    "    for (id, replacement_data), color in zip(optimal_replacement_times.items(), colors):\n",
    "        cycles = [data[0] for data in replacement_data]\n",
    "        optimal_times = [data[1] for data in replacement_data]\n",
    "        \n",
    "        plt.plot(cycles, optimal_times, linestyle='-', marker='o', color=color, alpha=0.7, \n",
    "                 label=f'Engine ID {id}', markersize=3)\n",
    "\n",
    "    plt.xlabel('Cycle')\n",
    "    plt.ylabel('Optimal Replacement Time (T_R_k)')\n",
    "    plt.title('Optimal Replacement Time per Cycle for All Engine IDs')\n",
    "    \n",
    "    # Adjust y-axis limits\n",
    "    plt.ylim(bottom=0)  # Start from 0\n",
    "    ymax = max(max(data[1] for data in values) for values in optimal_replacement_times.values())\n",
    "    plt.ylim(top=ymax * 1.1)  # Add 10% margin at the top\n",
    "    \n",
    "    # Improve legend\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='x-small', ncol=2)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(save_path, format='jpg', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the combined optimal replacement times for all engine IDs and save the image\n",
    "plot_combined_optimal_replacement_time(optimal_replacement_times, save_path='combined_optimal_replacement_times.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    #ΔT = params['seq_length']  # Time interval between decision points\n",
    "    #w = (L // ΔT) * ΔT  # Adjusted lead time\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle, DT, params)\n",
    "                #print(\"current_cycle: \", current_cycle,\"||current_cycle + params['seq_length']: \", current_cycle + params['seq_length'], \"||T_R_k_optimal: \", T_R_k_optimal)\n",
    "                # Apply PDM policy 2\n",
    "                if DT >= T_R_k_optimal:     \n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                # Ordering decision\n",
    "                #prob_RUL_smaller_w1 = np.mean(probabilities[:, 1] + probabilities[:, 2])\n",
    "                prob_RUL_smaller_w1 = probabilities[-1, 1] + probabilities[-1, 2]\n",
    "                p_order_thres =0.11 #Heuristic threshold for ordering\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    t_order_array[counter] =  current_cycle                    \n",
    "                    order = True\n",
    "                    #print(f\"Component ordered at cycle: {t_order_array[counter]}\")\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i =  current_cycle\n",
    "                    #T_R_i =  T_R_k_optimal\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                    costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy2(probabilities, C_p, C_c, current_cycle, DT, params):\n",
    "    # Fit the lognormal distribution to the predicted probabilities\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "\n",
    "    def objective_function(T_R_k):\n",
    "        # Compute P_PR: Probability that the component will be preventively replaced at time T_R_k\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "        \n",
    "        # Calculate the expected cost of replacement\n",
    "        E_C_rep = P_PR * C_p + (1 - P_PR) * C_c\n",
    "        \n",
    "        # Calculate the expected time to failure\n",
    "        E_T_lc = P_PR * T_R_k + expected_time_to_failure(current_cycle,T_R_k, mu, sigma)\n",
    "        \n",
    "        return E_C_rep / E_T_lc\n",
    "\n",
    "    # Use minimize_scalar to find the optimal replacement time\n",
    "    result = minimize_scalar(objective_function, bounds=(0, 2 * DT), method='bounded')\n",
    "    T_R_k_optimal = result.x\n",
    "    # Add some debugging information\n",
    "    \n",
    "    print(f\"Current cycle: {current_cycle}\")\n",
    "    print(f\"Probabilities: {probabilities[-1]}\")\n",
    "    print(f\"Fitted mu, sigma: {mu}, {sigma}\")\n",
    "    print(f\"Optimal T_R_k: {T_R_k_optimal}\")\n",
    "    print(f\"Objective function value: {result.fun}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "        \n",
    "    return T_R_k_optimal\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def expected_time_to_failure(current_cycle,T_R_k, mu, sigma):\n",
    "    def integrand(t):\n",
    "        return t * lognorm.pdf(t - current_cycle, mu, sigma)\n",
    "\n",
    "    integral_value, error = quad(integrand, current_cycle, T_R_k, limit=200)\n",
    "    return integral_value\n",
    "\n",
    "\"\"\"\n",
    "# Usage\n",
    "# Calculate probabilities, RUL distributions, and optimal replacement times\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDM Policy 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Objective: `PDM Policy 3 represents a significant advancement in predictive maintenance by utilizing the full distribution of RUL data to inform maintenance decisions. The objective function captures both the expected costs of preventive and corrective actions and the additional costs associated with early replacements, providing a comprehensive framework for optimizing maintenance strategies. <br>\n",
    "2. `Explanation of Equation(14): ` The equation for the objective function in PDM Policy 3 is given as:\n",
    "> $f\\left(T_{\\mathrm{R}, k}\\right)=P_{\\mathrm{PR}} \\cdot C_{\\mathrm{p}}+\\left(1-P_{\\mathrm{PR}}\\right) \\cdot C_{\\mathrm{c}}+\\int_{T_{\\mathrm{R}, k}}^{\\infty}\\left(t-T_{\\mathrm{R}, k}\\right) \\cdot \\frac{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[C_{\\mathrm{rep}}\\right]}{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[T_{l \\mathrm{c}}\\right]} f_{R U L_{\\mathrm{pred}, k}}\\left(t-t_k\\right) \\mathrm{d} t$\n",
    "\n",
    "3. `Components of the equation: `\n",
    "\n",
    ">  1. `Expected Replacement Cost: `\n",
    ">> * The first two terms, $P_{\\mathrm{PR}} \\cdot C_{\\mathrm{p}}$ and $\\left(1-P_{\\mathrm{PR}}\\right) \\cdot C_{\\mathrm{c}}$ represent the expected costs associated with preventive and corrective replacements, respectively. <br>\n",
    ">> * $P_{\\mathrm{PR}}$ is the probability that the components will survive until the replacement time $T_{\\mathrm{R}, k}$ <br>\n",
    ">> * $ C_{\\mathrm{p}}$ is the cost of preventive replacement, and $ C_{\\mathrm{c}}$ is the cost of corrective replacement. <br>\n",
    "\n",
    "> 2. `Integral Term: `\n",
    ">> * The integral term quantifies the additional expected maintenance cost associated with an \"early\" replacement at $T_{\\mathrm{R}, k}$ <br>\n",
    ">> * The expression $ \\left(t-T_{\\mathrm{R}, k}\\right) $ represents the time lost due to an early replacement.\n",
    ">> * The term $ \\frac{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[C_{\\mathrm{rep}}\\right]}{\\mathrm{E}_{\\bar{T}_{\\mathrm{F}}}\\left[T_{l \\mathrm{c}}\\right]} $  is the long-run expected maintenance cost per unit time concerning the distribution of the population of components. It provides a scaling factor for the additional cost incurred by replacing the component early. <br> \n",
    ">> * $ f_{R U L_{\\mathrm{pred}, k}}\\left(t-t_k\\right) $is the predicted probability density function of the RUL, representing the likelihood of failure occurring at time t after the current cycle $t_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import lognorm\n",
    "from scipy.optimize import minimize_scalar\n",
    "            \n",
    "\n",
    "def calculate_probabilities_for_pdm_policy_3_Without_ordering(estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device):\n",
    "    n_ids = len(pdm_df['id'].unique())\n",
    "    t_LC_array = np.zeros(n_ids)\n",
    "    costs_rep_array = np.zeros(n_ids)\n",
    "    costs_delay_array = np.zeros(n_ids)\n",
    "    costs_stock_array = np.zeros(n_ids)\n",
    "    \n",
    "    T_R_k_optimal_dict = {id: [] for id in pdm_df['id'].unique()}\n",
    "    \n",
    "    #mu_TF = calculate_expected_values(train_df)\n",
    "\n",
    "    for counter, id in enumerate(pdm_df['id'].unique()):\n",
    "        preventive_replacement = False\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df, scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c,current_cycle, DT)\n",
    "                T_R_k_optimal_dict[id].append((current_cycle, T_R_k_optimal))\n",
    "\n",
    "                if DT >= T_R_k_optimal:     \n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(0, L - t_LC_array[counter]) * C_unav\n",
    "                    costs_stock_array[counter] = max(0, t_LC_array[counter] - L) * C_inv\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "    return t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, T_R_k_optimal_dict\n",
    "\n",
    "\n",
    "def calculate_expected_values(train_df):\n",
    "    # Use vectorized operations for speed\n",
    "    mu_TF = train_df.groupby('id')['cycle'].max().mean()\n",
    "    return float(mu_TF)  # Ensure we return a single float value\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c,current_cycle, DT):\n",
    "    mu, sigma = fit_lognormal_cdf(probabilities)\n",
    "    mu_TF = calculate_expected_values(train_df)\n",
    "    \n",
    "    def objective_function(T_R_k):\n",
    "        P_PR = 1 - lognorm.cdf(T_R_k - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "        \n",
    "        def integral_term(t):\n",
    "            return (t - T_R_k) * lognorm.pdf(t - current_cycle, s=sigma, scale=np.exp(mu))\n",
    "        \n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", IntegrationWarning)\n",
    "                additional_cost, error = quad(integral_term, T_R_k, 1e6, \n",
    "                                             limit=200, epsabs=1e-6, epsrel=1e-6,\n",
    "                                             points=[T_R_k, 1e6])\n",
    "            additional_cost *= C_p / mu_TF\n",
    "        except Exception as e:\n",
    "            print(f\"Integration error: {str(e)}\")\n",
    "            additional_cost = (T_R_k - current_cycle) * C_p / mu_TF  # Simple fallback\n",
    "        \n",
    "        return P_PR * C_p + (1 - P_PR) * C_c + additional_cost\n",
    "    \n",
    "    try:\n",
    "        result = minimize_scalar(objective_function, bounds=(0,2 * DT), method='bounded')\n",
    "        return result.x\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization error: {str(e)}\")\n",
    "        return current_cycle + DT  # Return a default value\n",
    "\n",
    "\n",
    "# Usage\n",
    "t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, T_R_k_optimal_dict = calculate_probabilities_for_pdm_policy_3_Without_ordering(\n",
    "    estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the T_R_K_optimal  per cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_T_R_k_optimal(T_R_k_optimal_dict, num_ids_to_plot=20, save_path='T_R_k_optimal_plot.jpg'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    ids_to_plot = list(T_R_k_optimal_dict.keys())[:num_ids_to_plot]\n",
    "    \n",
    "    max_cycle = 0\n",
    "    min_T_R_k = float('inf')\n",
    "    max_T_R_k = float('-inf')\n",
    "    \n",
    "    for id in ids_to_plot:\n",
    "        if T_R_k_optimal_dict[id]:\n",
    "            cycles, T_R_k_values = zip(*T_R_k_optimal_dict[id])\n",
    "            plt.plot(cycles, T_R_k_values, marker='o', linestyle='-', label=f'Engine ID {id}')\n",
    "            max_cycle = max(max_cycle, max(cycles))\n",
    "            min_T_R_k = min(min_T_R_k, min(T_R_k_values))\n",
    "            max_T_R_k = max(max_T_R_k, max(T_R_k_values))\n",
    "            print(f\"Engine ID {id}: {len(cycles)} data points\")\n",
    "        else:\n",
    "            print(f\"No data for Engine ID {id}\")\n",
    "    \n",
    "    if max_cycle == 0:\n",
    "        print(\"No valid data to plot\")\n",
    "        return\n",
    "    \n",
    "    plt.xlabel('Current Cycle')\n",
    "    plt.ylabel('Optimal Replacement Time (T_R_k_optimal)')\n",
    "    plt.title('Optimal Replacement Time vs Current Cycle')\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adjust axis limits to ensure all data is visible\n",
    "    plt.xlim(0, max_cycle * 1.1)\n",
    "    plt.ylim(min_T_R_k * 0.9, max_T_R_k * 1.1)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.savefig(save_path, format='jpg', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    t_order_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    t_LC_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_rep_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_delay_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    costs_stock_array = np.zeros(len(pdm_df['id'].unique()))\n",
    "    w = np.ceil(L / DT) * DT\n",
    "    #ΔT = params['seq_length']  # Time interval between decision points\n",
    "    #w = (L // ΔT) * ΔT  # Adjusted lead time\n",
    "\n",
    "    for id in pdm_df['id'].unique():\n",
    "        #print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order = False\n",
    "\n",
    "        for current_cycle in range(params['seq_length'], pdm_df[pdm_df['id'] == id].shape[0] + 1):\n",
    "            if current_cycle in array_decisions:\n",
    "                # Prepare data\n",
    "                seq_array_test_k = prepare_sequence_data(pdm_df,scaler, id, current_cycle, params, cols_normalize_train, sequence_cols)\n",
    "\n",
    "                # Convert to tensor\n",
    "                seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    outputs = estimator(seq_tensor).squeeze()\n",
    "                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "                #print(f\"Engine ID: {id}, Cycle: {current_cycle}\")\n",
    "\n",
    "                # Calculate optimal replacement time\n",
    "                T_R_k_optimal = find_optimal_replacement_time_for_PDM_Policy3(probabilities, C_p, C_c,current_cycle, DT)\n",
    "                #T_R_k_optimal_dict[id].append((current_cycle, T_R_k_optimal))\n",
    "                # Apply PDM policy 2\n",
    "                if DT >= T_R_k_optimal:     \n",
    "                    action = \"PR\"  # Preventive Replacement\n",
    "                    #print(f\"PR decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "                else:\n",
    "                    action = \"DN\"  # Do Nothing\n",
    "                    #print(f\"DN decided for ID {id} at cycle {current_cycle}. T_R_k_optimal: {T_R_k_optimal}, DT: {DT}\")\n",
    "\n",
    "                #print(f\"Action taken: {action}\")\n",
    "\n",
    "                # Ordering decision\n",
    "                prob_RUL_smaller_w1 = probabilities[-1, 1] + probabilities[-1, 2]\n",
    "                p_order_thres = 0.11  # Heuristic threshold for ordering\n",
    "                if not order and prob_RUL_smaller_w1 >= p_order_thres:\n",
    "                    t_order_array[counter] = current_cycle\n",
    "                    # t_order_array[counter] =  current_cycle+ w\n",
    "                    order = True\n",
    "                    #print(f\"Component ordered at cycle: {t_order_array[counter]}\")\n",
    "\n",
    "                # Preventive replacement decision\n",
    "                if action == \"PR\":\n",
    "                    T_R_i = current_cycle\n",
    "                    T_F_i = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "                    t_LC_array[counter] = min(T_R_i, T_F_i)\n",
    "                    costs_rep_array[counter] = C_p if T_R_i < T_F_i else C_c\n",
    "                    #print('Preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                    costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "                    break\n",
    "\n",
    "        if not preventive_replacement:\n",
    "            t_LC_array[counter] = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "            #print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if not order:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "                costs_stock_array[counter] = 0  # No stock cost if no order was placed\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "        #print('True failure:', pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1])\n",
    "        #print('-----------------------------------------')\n",
    "        counter += 1\n",
    "\n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "\n",
    "\n",
    "# Usage\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_cost = costs_rep_array +costs_delay_array+costs_stock_array\n",
    "#total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def calculate_T_R_perfect(pdm_df, id, DT=10):\n",
    "    # Retrieve the perfect failure time for the ith component\n",
    "    T_F_perfect = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "    \n",
    "    # Calculate k as the largest integer such that k * DT < T_F_perfect\n",
    "    k = int(T_F_perfect // DT)  # Use floor division\n",
    "    T_R_perfect = k * DT  # Calculate T_R_perfect\n",
    "    \n",
    "    return T_R_perfect\n",
    "\n",
    "def calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = (np.mean(costs_rep_array) + np.mean(costs_delay_array) + np.mean(costs_stock_array))\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "def calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = np.mean(costs_rep_array)\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "# Usage remains the same\n",
    "# Range of C_p values\n",
    "C_p_values = np.array([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "C_c = 1000  # Constant corrective replacement cost\n",
    "M_hat_Policy1_With_ordering = []\n",
    "M_hat_Policy1_Without_ordering = []\n",
    "\n",
    "M_hat_Policy2_With_ordering = []\n",
    "M_hat_Policy2_Without_ordering = []\n",
    "\n",
    "M_hat_Policy3_With_ordering = []\n",
    "M_hat_Policy3_Without_ordering = []\n",
    "\n",
    "# Calculate M_hat for each C_p\n",
    "for C_p in C_p_values:\n",
    "    #Call the function to get updated arrays for policy1 with ordering  \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat1 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_With_ordering.append(M_hat1)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy1 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, prob_RUL_smaller_DT_dict, replacement_probs_dict = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat2 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_Without_ordering.append(M_hat2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 with ordering\n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat3 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_With_ordering.append(M_hat3)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions = calculate_probabilities_for_pdm_policy_2_Without_ordering(\n",
    "    estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat4 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_Without_ordering.append(M_hat4)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 with ordering                   \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat5 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_With_ordering.append(M_hat5)\n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, T_R_k_optimal_dict = calculate_probabilities_for_pdm_policy_3_Without_ordering(\n",
    "    estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat6 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_Without_ordering.append(M_hat6)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate C_p/C_c ratios\n",
    "C_p_C_c_ratios = C_p_values / C_c\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "policies = [\n",
    "    (M_hat_Policy1_With_ordering, 'Policy 1 With Ordering'),\n",
    "    (M_hat_Policy1_Without_ordering, 'Policy 1 Without Ordering'),\n",
    "    (M_hat_Policy2_With_ordering, 'Policy 2 With Ordering'),\n",
    "    (M_hat_Policy2_Without_ordering, 'Policy 2 Without Ordering'),\n",
    "    (M_hat_Policy3_With_ordering, 'Policy 3 With Ordering'),\n",
    "    (M_hat_Policy3_Without_ordering, 'Policy 3 Without Ordering')\n",
    "]\n",
    "\n",
    "for data, label in policies:\n",
    "    plt.plot(C_p_C_c_ratios, data, marker='o', label=label)\n",
    "    print(f\"{label} data: {data}\")  # Print data for debugging\n",
    "\n",
    "plt.title('Decision Metric M_hat% vs C_p/C_c Ratio')\n",
    "plt.xlabel('C_p/C_c Ratio')\n",
    "plt.ylabel('Decision Metric M_hat%')\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.xticks(C_p_C_c_ratios)  # Set x-ticks to the calculated ratios\n",
    "plt.axhline(0, color='black', lw=0.5, ls='--')  # Add a horizontal line at y=0\n",
    "\n",
    "# Adjust y-axis limits to ensure all data is visible\n",
    "plt.ylim(bottom=max(1, plt.ylim()[0]), top=plt.ylim()[1]*1.1)\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('decision_metric_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "\n",
    "counter = 0\n",
    "for id in test_df['id'].unique():\n",
    "    print('ID:', id)\n",
    "    preventive_replacement = False\n",
    "    order = False\n",
    "\n",
    "    for current_cycle in range(test_df[test_df['id'] == id].shape[0] - params['seq_length'] + 1):\n",
    "\n",
    "        if current_cycle in array_decisions:\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[test_df['id'] == id][cols_normalize_train][:params['seq_length'] + current_cycle]),\n",
    "                                        columns=cols_normalize_train,\n",
    "                                        index=test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].index)\n",
    "\n",
    "            join_df = test_df[test_df['id'] == id][:params['seq_length'] + current_cycle][test_df[test_df['id'] == id][:params['seq_length'] + current_cycle].columns.difference(cols_normalize_train)].join(norm_test_df)\n",
    "            test_df_eval_online = join_df.reindex(columns=test_df[test_df['id'] == id][current_cycle:params['seq_length'] + current_cycle].columns)\n",
    "\n",
    "            seq_array_test_k = test_df_eval_online[sequence_cols].values[current_cycle:params['seq_length'] + current_cycle]\n",
    "            seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1, params['seq_length'], nb_features)\n",
    "\n",
    "            # Convert to tensor\n",
    "            seq_tensor = torch.tensor(seq_array_test_k, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Predict\n",
    "            outputs = estimator(seq_tensor).squeeze()\n",
    "            #print('outputs: ',outputs )\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().detach().numpy() #shape: [batch_size, sequence_length,num_classes]\n",
    "            #prob_RUL_smaller_DT = probabilities[-1,2] # class2 probability for last time steps\n",
    "            #prob_RUL_smaller_w1 = probabilities[-1,1] + probabilities[-1,2] # (class1+class2) for last time steps\n",
    "            \n",
    "            # Assuming 'probabilities' is your (1, 50, 3) tensor\n",
    "            prob_RUL_smaller_w1 = probabilities[:, 1] + probabilities[:, 2]\n",
    "            prob_RUL_smaller_DT = probabilities[:, 2]\n",
    "\n",
    "            \n",
    "            #print('Collective probabilities: ', probabilities)\n",
    "            #print('shape of Collective probabilities: ', probabilities.shape)\n",
    "            print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "            print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "            # Evaluate decision heuristics\n",
    "            if not order:#\n",
    "                # Order component\n",
    "                if C_p <= ( * C_c):\n",
    "                    print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                    t_order_array[counter] = params['seq_length'] + current_cycle\n",
    "                    order = True\n",
    "                    print('component ordering at cycle:', t_order_array[counter])\n",
    "            \n",
    "            # Perform preventive replacement\n",
    "            if C_p <= (prob_RUL_smaller_DT * C_c):\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                t_LC_array[counter] = params['seq_length'] + current_cycle\n",
    "                costs_rep_array[counter] = C_p\n",
    "                print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "\n",
    "                preventive_replacement = True\n",
    "                costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "                break\n",
    "\n",
    "    if not preventive_replacement:\n",
    "        t_LC_array[counter] = test_df[test_df['id'] == id]['cycle'].iloc[-1]\n",
    "        print('Component failure at t:', t_LC_array[counter])\n",
    "        costs_rep_array[counter] = C_c\n",
    "\n",
    "        if not order:\n",
    "            costs_delay_array[counter] = L * C_unav\n",
    "        else:\n",
    "            costs_delay_array[counter] = max(t_order_array[counter] + L - t_LC_array[counter], 0) * C_unav\n",
    "            costs_stock_array[counter] = max(t_LC_array[counter] - (t_order_array[counter] + L), 0) * C_inv\n",
    "\n",
    "    print('True failure:', test_df[test_df['id'] == id]['cycle'].iloc[-1])\n",
    "    print('-----------------------------------------')\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp4AF1KW6PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzXo4Qlw6PuB"
   },
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNUa2J36PuB"
   },
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6g7k8Yrftya"
   },
   "source": [
    "### This code calculates the expected cost per unit time using the LSTM model. It computes the mean of the total costs divided by the mean of the time to component failure (t_LC_array). This metric gives an estimate of the average cost incurred per unit time in the system, considering both maintenance and operational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3_N4kNM6PuC"
   },
   "outputs": [],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgoTz57gC_P"
   },
   "source": [
    "This code segment calculates the expected cost per unit time assuming perfect prognostics.\n",
    "`1. Perfect Prognostics Calculation:`\n",
    "> * It initializes an array `t_LC_perfect_array` to store the time of component failure for each unit in the validation dataset. This is calculated by dividing the last observed cycle number by the decision interval DT and then flooring the result to get the last decision cycle before failure.\n",
    "> * The loop iterates over each unique ID in the validation dataset, calculates the time of component failure for each unit, and stores it in\n",
    "`t_LC_perfect_array`.<br>\n",
    "> * `math.floor()` is used to round down the result to the nearest multiple of `DT`.\n",
    "> * Finally, the loop increments the counter for each unit.<br>\n",
    "\n",
    "`2. Cost Calculation:`\n",
    "> * `costs_perfect_array` is initialized with a value of `C_p`, representing the cost of preventive replacements. In a perfect scenario, only preventive replacements are made.\n",
    "> * This array holds the same cost value for each unit in the validation dataset.\n",
    "\n",
    "`3. Expected Cost Calculation:`\n",
    "> * `expected_cost_perfect` is calculated by taking the mean of `costs_perfect_array` and dividing it by the mean of `t_LC_perfect_array`.\n",
    "> * This calculation provides an estimate of the average cost per unit time assuming perfect prognostics, where components are replaced preventively at regular intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOD6NDfH6PuC"
   },
   "outputs": [],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(10)\n",
    "DT=10\n",
    "counter=0\n",
    "for id in test_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(test_df[test_df['id']==id]['cycle'].iloc[-1] /DT) * DT\n",
    "    counter+=1\n",
    "\n",
    "costs_perfect_array = np.ones(10)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-vK-DBi6PuC"
   },
   "outputs": [],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7_LJce6PuC"
   },
   "outputs": [],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KofjvavY6PuD"
   },
   "outputs": [],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42Nu-Rg6PuE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
