{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26732,
     "status": "ok",
     "timestamp": 1718627582985,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "70-XsHkDGUKu",
    "outputId": "6bb24682-dbae-4c4e-be23-bdb60fe62dee"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19128,
     "status": "ok",
     "timestamp": 1718627615090,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "3wOjKm186Pth",
    "outputId": "42183fc5-359b-4a36-ec2e-4e7ee208179f"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718627615091,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "4BsYPkY7gYGx"
   },
   "outputs": [],
   "source": [
    "# Multiclass classification\n",
    "#Predict if an asset will fail within two different intervals related to the two different decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iklamKYlNjh"
   },
   "source": [
    "`tensorflow.keras`: This imports the Keras module from TensorFlow, which provides an API for building and training deep learning models.<br>\n",
    "`os`: This module provides functions for interacting with the operating system. It's commonly used for tasks such as file manipulation and directory operations.<br>\n",
    "`sklearn.preprocessing`: This module from scikit-learn provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables.<br>\n",
    "`sklearn.metrics`: This module contains functions for evaluating model performance, such as computing confusion matrices, recall scores, and precision scores.<br>\n",
    "`tensorflow.keras.models.Sequential`: This class from Keras represents a sequential model, which is a linear stack of layers. It's used for building feedforward neural networks.<br>\n",
    "`tensorflow.keras.models.load_model`: This function is used to load a pre-trained Keras model from a file.<br>\n",
    "`tensorflow.keras.layers`: This module contains various types of layers that can be added to a Keras model, such as dense (fully connected) layers, dropout layers, and LSTM layers.\n",
    "`multiclass_model_w1_30.h5`:The .h5 extension indicates that the model will be saved in the Hierarchical Data Format version 5 (HDF5) format, which is commonly used for storing large numerical datasets. The model will be saved with the filename **multiclass_model_w1_30.h5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1718627620493,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'multiclass_model_w1_30.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1718627651010,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "JjbfnUZGgc3C",
    "outputId": "6d96f27d-6b20-4bec-a2be-2ed504a55ca0"
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1718627653067,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "XBjr189aTPt1",
    "outputId": "4c02043a-513d-4edd-cd67-6ea664a41de7"
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpsNnInsevi"
   },
   "source": [
    "`train_df.sort_values(['id','cycle'])`: This line sorts the DataFrame **train_df** first by the 'id' column and then by the 'cycle' column. It ensures that the data is ordered by engine ID and cycle number, which may be necessary for certain analyses or modeling tasks. The sorted DataFrame is then assigned back to the variable **train_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1718627659356,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "mo0v9RsVriPz"
   },
   "outputs": [],
   "source": [
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1718627661504,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "E7x-rVZz6Ptq",
    "outputId": "0c2cc3b7-5974-4b39-d653-d08f95eb88bc"
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing\n",
    "data preprocessing step, particularly for labeling the data for training purposes. Let's break down what each part of the code does:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-_dxq60Nf4"
   },
   "source": [
    ">`Data Labeling`: This part calculates the Remaining Useful Life (RUL) or Time to Failure for each engine by finding the maximum cycle number (cycle) for each engine ID (id). The result is stored in a DataFrame rul with columns 'id' and 'max'.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1718627665071,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718627666649,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "BnqCery8yupl",
    "outputId": "a8a61e79-462b-411c-83fb-fce3cb8f1957"
   },
   "outputs": [],
   "source": [
    "rul.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZubPMTD0T9z"
   },
   "source": [
    ">`Merge RUL with Training Data`:the RUL information is merged back into the original training DataFrame **train_df** based on the engine ID. This allows each row in train_df to have the corresponding maximum cycle number as well.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1718627669592,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "6Ycs7i7Qyu32"
   },
   "outputs": [],
   "source": [
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718627670706,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "AroVzMsuzHhp",
    "outputId": "b0765883-fee3-4fec-d93b-305ecb7a939f"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-eouxGv0gpG"
   },
   "source": [
    ">`Calculate RUL`: This line calculates the RUL by subtracting the current cycle number ('cycle') from the maximum cycle number ('max') for each engine. This represents how many more cycles the engine is expected to operate before failure.<br>\n",
    ">`Drop Unnecessary Columns`: After calculating RUL, the 'max' column, which was used temporarily to calculate RUL, is dropped from the DataFrame as it's no longer needed.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1718627674524,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "vulNLE0CztxT",
    "outputId": "c91c3d5a-e954-49a4-9148-e01114d896e3"
   },
   "outputs": [],
   "source": [
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBFd2pLr0odt"
   },
   "source": [
    "> `Labeling for Classification`: This part assigns labels to each data point based on the calculated RUL. It defines thresholds `w1` and `w0`, and assigns:\n",
    ">> * Label 1 ('label1') as 1 if RUL is less than or equal to 'w1', and 0 otherwise.\n",
    ">> * Label2 ('label2') as 1 if RUL is less than or equal to 'w1', 2 if RUL is less than or equal to 'w0', and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1718627687767,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "K6wpAOpjyvGl"
   },
   "outputs": [],
   "source": [
    "w1 = 30\n",
    "w0 = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1718627689346,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "6nJhPwEw1RgE",
    "outputId": "7f353f5c-749d-40cb-a975-21873c79143f"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_8MNc2Z6Ptu"
   },
   "source": [
    "## Now I want to separate the training set into a training and validation set. I will use 80 training sets for the training and 20 training sets as evaluation sets for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1718627694382,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Sdm_ZG3E6Ptv"
   },
   "outputs": [],
   "source": [
    "list_ID = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13rqadVW6Ptv"
   },
   "source": [
    "## I separate into training and validation set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1718627811514,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "2nwuKyvt6Ptw"
   },
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1718627815227,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "pM4-dDoC6Ptw",
    "outputId": "5d16f5b8-b639-404c-f09a-ce7349ae3148"
   },
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1718627818022,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "Q6aoJ3v76Ptx"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1718627820336,
     "user": {
      "displayName": "jitendra tiwari",
      "userId": "04882265798590373880"
     },
     "user_tz": -120
    },
    "id": "PyrZZu6_uCJS",
    "outputId": "52f0d9a5-f604-48b0-e839-6c7a3b120c71"
   },
   "outputs": [],
   "source": [
    "train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7siRw6Ptx"
   },
   "source": [
    "# Perform the min max scaling of the training data set\n",
    "# use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYkFz5FqQplh"
   },
   "source": [
    ">`Create a copy of the cycle column`: This line creates a new column named 'cycle_norm' in the train_df DataFrame and initializes it with the values from the original 'cycle' column. This column will be normalized later.<br>\n",
    "> `Select columns for normalization`: This line selects all columns from **train_df** except 'id', 'cycle', 'RUL', 'label1', and 'label2'. These columns are the ones that will undergo normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_loHDxEQnHc"
   },
   "outputs": [],
   "source": [
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AHbVv4RgN-"
   },
   "source": [
    "> `Initialize MinMaxScaler`: This line initializes a MinMaxScaler object from the scikit-learn preprocessing module. This scaler will be used to perform Min-Max normalization.<br>\n",
    "> `Perform Min-Max normalization`: This line applies Min-Max normalization to the selected columns (`cols_normalize`) of the `train_df` DataFrame.<br>\n",
    "> `min_max_scaler.fit_transform(train_df[cols_normalize])` computes the Min-Max normalization for the selected columns.<br>\n",
    "> The resulting normalized values are stored in a new DataFrame called `norm_train_df`, with the same index as `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA4WFngxQm4Z"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEf7s4DS3jR"
   },
   "source": [
    "> `Join normalized DataFrame with the original DataFrame`: This line joins the normalized DataFrame (`norm_train_df`) with the original DataFrame (`train_df`) excluding the columns that were normalized.<br>\n",
    "> The resulting DataFrame `join_df` contains both the normalized columns and the original columns that were not normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThKcf5Qf6Ptx"
   },
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF2-ITehT1_P"
   },
   "source": [
    "`Reorder columns`:\n",
    "> * This line reorders the columns of `join_df` to match the original order of columns in `train_df`.\n",
    "> * The reordered DataFrame is then assigned back to `train_df`, effectively replacing the original DataFrame with the normalized version.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWuYsg8eTyoJ"
   },
   "outputs": [],
   "source": [
    "train_df = join_df.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrXBeYUg6Ptx"
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5AtglSI1t99"
   },
   "outputs": [],
   "source": [
    "train_df['label2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74I2omLtQLLs"
   },
   "outputs": [],
   "source": [
    "train_df['label1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LaPWraKepMF"
   },
   "source": [
    "`Define a function to generate sequences:`\n",
    "> * This function generates sequences of sensor data for each engine (`id`) in the dataset.\n",
    "> * It takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (`id`).\n",
    ">> * `seq_length:` Length of the sequence window.\n",
    ">> * `seq_cols:` List of column names to include in the sequences.\n",
    "> * It iterates over the rows of `id_df`, creating sequences of length seq_length without padding.<br>\n",
    "> * It ensures that only sequences of the specified length are considered, which is important for sequence-based modeling tasks.<br>\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[seq_cols].values:`\n",
    ">> * This line extracts the columns specified by `seq_cols` from the DataFrame `id_df` and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant columns needed for creating sequences.\n",
    "\n",
    "`Sequence Generation:`\n",
    "> * `num_elements:` This line calculates the number of rows (elements) in the data_matrix.<br>\n",
    "> `for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):`\n",
    ">> * This is a loop that iterates over the rows of the data matrix to create sequences.<br>\n",
    ">> * It uses the `zip` function to iterate over two parallel lists: one list starts from 0 to `num_elements` - `seq_length`, and the other starts from `seq_length` to `num_elements`.<br>\n",
    ">> * This ensures that sequences of length `seq_length` are generated without overlap.<br>\n",
    ">> * `start` and `stop` define the start and end indices of each sequence.<br>\n",
    "> `yield data_matrix[start:stop, :]:`\n",
    ">> * This line yields (returns) each sequence as a 2D numpy array.<br>\n",
    ">> * It uses array slicing to extract the rows corresponding to the current sequence range.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSInZu-EkFtf"
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles. This sets the length of the sequence window to 50 cycles.\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAWprb25jV4h"
   },
   "outputs": [],
   "source": [
    "# pick the feature columns, This selects the columns to be included in the sequences.\n",
    "# sensor_cols contains the sensor data columns (s1 to s21).\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "#sequence_cols initially contains the operational settings columns (setting1, setting2, setting3, cycle_norm).\n",
    "# Then, it's extended to include the sensor data columns as well.\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEV0vj7AkJOl"
   },
   "source": [
    "## generate sequences for each engine\n",
    "> * This creates a generator expression that iterates over unique engine IDs in the training data.<br>\n",
    "> * For each engine, it generates sequences using the `gen_sequence` function defined earlier.<br>\n",
    "> * Each sequence is a list of sensor data, and multiple sequences are generated for each engine.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAsGvlwBjVpe"
   },
   "outputs": [],
   "source": [
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Fc99j1rPQa"
   },
   "source": [
    "> * This concatenates all the generated sequences into a single numpy array.\n",
    "> * It converts the array to `float32` data type.\n",
    "> * The resulting `seq_array` contains the sequences of sensor data, with shape `(num_sequences, sequence_length, num_features)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C8e0SuHjVbh"
   },
   "outputs": [],
   "source": [
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An8AqEX_6Pty"
   },
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu8ocOhxkaf"
   },
   "source": [
    "`Function Signature:` This function efficiently generates labels for each sequence of sensor data. It ensures that the labels are correctly aligned with the sequences and handles the special case where the first sequence uses the last label as its target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This function takes three arguments:\n",
    ">> * `id_df:` DataFrame containing data for a specific engine (id).<br>\n",
    ">> * `seq_length`: Length of the sequence window.<br>\n",
    ">> * `label`: List of column names representing the labels.\n",
    "\n",
    "`Data Preparation:`\n",
    "> * `data_matrix = id_df[label].values:`\n",
    ">> * This line extracts the columns specified by label from the DataFrame id_df and converts them to a numpy array.<br>\n",
    ">> * It selects only the relevant label(s) needed for generating sequences.<br>\n",
    "\n",
    "`Label Generation:`\n",
    "> * `num_elements:`This line calculates the number of rows (elements) in the data matrix, which corresponds to the number of labels.<br>\n",
    "> * `return data_matrix[seq_length:num_elements, :]:`\n",
    ">> * This line returns the labels associated with each sequence.<br>\n",
    ">> * It removes the first `seq_length` labels because, for each engine (`id`), the first sequence of size `seq_length` uses the last label as its target. The previous labels are discarded.<br>\n",
    ">> * All subsequent sequences for the same engine (`id`) will have one label associated with them step by step.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIDbZQrJ6Pty"
   },
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label2'])\n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjniT37X6Ptz"
   },
   "outputs": [],
   "source": [
    "# When modeling multi-class classification problems using neural networks,\n",
    "# it is good practice to reshape the output attribute from a vector that contains values for each class value to be\n",
    "# a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "# This is called one hot encoding or creating dummy variables from a categorical variable.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qBrRZdYZiHb"
   },
   "source": [
    "`to_categorical` is a utility function in Keras that converts class vectors (integers) to binary class matrices.<br>\n",
    "`dummy_label_array = to_categorical(label_array):`This line applies one-hot encoding to the `label_array`.<br>\n",
    "`label_array` contains the labels associated with each sequence, where each label represents a class or category.<br>\n",
    "> * One-hot encoding converts these integer labels into binary vectors, where each vector has a length equal to the number of classes and contains a 1 in the position corresponding to the class and 0s elsewhere.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEpXlgujjc11"
   },
   "outputs": [],
   "source": [
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3BMrY396Pt0"
   },
   "outputs": [],
   "source": [
    "dummy_label_array = to_categorical(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnwyiGj06Pt0"
   },
   "outputs": [],
   "source": [
    "dummy_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M40CNdn7krRw"
   },
   "outputs": [],
   "source": [
    "dummy_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj3Hmdzl9l2q"
   },
   "outputs": [],
   "source": [
    "type(dummy_label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWhWIVl56Pt1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out      = dummy_label_array.shape[1]\n",
    "nb_features, nb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0h3i_FJg7pJ"
   },
   "source": [
    "`Extracting Feature and Output Dimensions:`\n",
    "> `nb_features:`Determines the number of features in the input sequence data.<br>\n",
    "> `nb_out:`Determines the number of output classes. It's extracted from the shape of the label array.<br>\n",
    "\n",
    "`Defining the Model Architecture:` describe in the code below.\n",
    "`Compiling the Model:` `model.compile(...)` Here, `categorical_crossentropy` is used as the loss function for multi-class classification.\n",
    "\n",
    "`Model Summary:`Prints a summary of the model architecture, including the layers and their parameters.\n",
    "\n",
    "`Training the Model:` `model.fit(...):` Trains the model on the training data. It specifies the input data (`seq_array`) and the corresponding labels (`dummy_label_array`). Other parameters include the number of epochs, batch size, validation split, verbosity, and callbacks.<br>\n",
    "\n",
    "\n",
    "`history.history.keys():` After training, this prints the keys of the history object, which contains information about training and validation metrics over each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9k0p5-46Pt1"
   },
   "outputs": [],
   "source": [
    "# Next, we build a deep network.\n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.\n",
    "# Dropout is also applied after each LSTM layer to control overfitting.\n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "\n",
    "# Initializes a sequential model, which allows you to build a model layer by layer.\n",
    "model = Sequential()\n",
    "\n",
    "#Adds an LSTM layer to the model. The first LSTM layer has 100 units, returns sequences, and takes input in the shape of (sequence_length, nb_features).\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adds another LSTM layer with 50 units. This layer does not return sequences, indicating it's the final LSTM layer.\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adds a dense (fully connected) layer to the model with a softmax activation function. This layer produces the output classes.\n",
    "model.add(Dense(units=nb_out, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, dummy_label_array, epochs=10, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sWk2GlE6Pt2"
   },
   "source": [
    "# Every time I retrain the algorithm I get different training results, i.e., also different evaluation of the decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set created during the training (i.e., validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpVYSzXkmk5l"
   },
   "outputs": [],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u2aW0bj6Pt3"
   },
   "outputs": [],
   "source": [
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, dummy_label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK6St5XcVyd9"
   },
   "source": [
    "`y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\"):` Predicts the abels for the input sequences using the trained model. The predictions are hresholded at 0.5, meaning that any output probability greater than 0.5 is considered as class 1, otherwise class 0. The predictions are then converted to integers (0 or 1).<br>\n",
    "\n",
    "`y_true = dummy_label_array:` Sets the true labels from the dummy label array, which represents the actual labels of the data.\n",
    "then print the **confusion_matrix**\n",
    "\n",
    "`cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1)):` Computes the confusion matrix using the true labels (`y_true`) and the predicted labels `(y_pred). argmax(axis=1)` is used to convert one-hot encoded labels back to their original integer form before computing the confusion matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRwaYKfV6Pt4"
   },
   "outputs": [],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "# y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\") # this way (>0.5) the outcome goes from a probability to 0,1\n",
    "y_true = dummy_label_array\n",
    "\n",
    "# test_set = pd.DataFrame(y_pred)\n",
    "# # test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score, r2_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_pred and y_true are available\n",
    "# y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\")  # Predictions (0 or 1)\n",
    "# y_true = dummy_label_array  # True labels\n",
    "\n",
    "# Convert y_pred and y_true to one-hot encoded if they aren't already\n",
    "y_true_class = np.argmax(y_true, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "print(cm)\n",
    "\n",
    "# Function to compute F1-score, F2-score, and R2-score for each class\n",
    "def calculate_metrics(y_true_class, y_pred_class):\n",
    "    # F1-score for each class\n",
    "    f1 = f1_score(y_true_class, y_pred_class, average=None)\n",
    "    \n",
    "    # F2-score with beta=2 for each class\n",
    "    f2 = fbeta_score(y_true_class, y_pred_class, beta=2.0, average=None)\n",
    "    \n",
    "    # R2-score (optional for classification tasks, but usually used for regression tasks)\n",
    "    r2 = r2_score(y_true_class, y_pred_class)\n",
    "    \n",
    "    return f1, f2, r2\n",
    "\n",
    "# Compute F1, F2, and R2 scores for each class\n",
    "f1, f2, r2 = calculate_metrics(y_true_class, y_pred_class)\n",
    "\n",
    "# Print the results\n",
    "print(\"F1-score for each class:\", f1)\n",
    "print(\"F2-score for each class (beta=2.0):\", f2)\n",
    "print(\"R2-score:\", r2)\n",
    "\n",
    "# Additional classification metrics (optional)\n",
    "precision = precision_score(y_true_class, y_pred_class, average=None)\n",
    "recall = recall_score(y_true_class, y_pred_class, average=None)\n",
    "\n",
    "print(\"Precision for each class:\", precision)\n",
    "print(\"Recall for each class:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Second PdM policy evaluation on the validation set.\n",
    "\n",
    "For each validation set, I need to give the on-line sensor data as input to the trained LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_GqqVKV6Pt4"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vloF6HXQ6Pt4"
   },
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p    = 100\n",
    "C_c    = 1000\n",
    "C_unav = 10\n",
    "C_inv  = 1\n",
    "DT     = 10  # Decisions can be taken every DT=10\n",
    "L      = 20  # lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ-vs1-x6Pt5"
   },
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KP5yjWw6Pt5"
   },
   "outputs": [],
   "source": [
    "# estimator.predict(seq_array_validation_k).reshape(3) returns a vector with 3 elements\n",
    "# [Pr(RUL>w1), Pr(w0<RUL<=w1), Pr(RUL<=w0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqIzk2b96Pt5"
   },
   "outputs": [],
   "source": [
    "validation_df['cycle_norm'] = validation_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzX435pyxgxD"
   },
   "outputs": [],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAF7B0eOY1BN"
   },
   "outputs": [],
   "source": [
    "validation_df.head(242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBklUcnv6Pt5"
   },
   "source": [
    "## Second PdM policy evaluation on a the whole validation data set (ids 81 to 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfyabzdL6Pt_"
   },
   "outputs": [],
   "source": [
    "costs_rep_array   = np.zeros(20) # An array to store costs related to replacements.\n",
    "\n",
    "costs_delay_array = np.zeros(20) # An array to store costs related to delays.\n",
    "costs_stock_array = np.zeros(20) # An array to store costs related to stock.\n",
    "\n",
    "t_LC_array        = np.zeros(20) # An array to store lead time.\n",
    "t_order_array     = np.zeros(20) # An array to store order time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsWT1ebGbOdz"
   },
   "source": [
    "> 1. Initializes a counter variable to 0.\n",
    "> 2. Iterates over unique IDs in the `validation_df` DataFrame.\n",
    "> 3. For each ID:\n",
    ">> * Sets flags for preventive replacement and ordering to False.<br>\n",
    ">> * Iterates over cycles within the range of the DataFrame.<br>\n",
    ">> * Checks if the current cycle is in the `array_decisions`.<br>\n",
    ">> * If it is, preprocesses the validation data for the LSTM model.<br>\n",
    ">> * Predicts the probability of RUL being smaller than w1 and DT (decision time) using the trained model.<br>\n",
    ">> * Evaluates decision heuristics:\n",
    ">>> * If no order has been placed yet and the cost of preventive replacement is less than or equal to the cost of waiting until `w1`, orders the component and sets the order time.<br>\n",
    ">>> * If the cost of preventive replacement is less than or equal to the cost of waiting until `DT`, performs preventive replacement, calculates related costs, and breaks the loop.<br>\n",
    ">> If preventive replacement is not performed:\n",
    ">>> * Sets the component failure time to the last cycle in the ID's data.<br>\n",
    ">>> * Sets replacement costs to `C_c`.<br>\n",
    ">>> * Calculates delay costs based on whether an order has been placed.<br>\n",
    ">> * Prints diagnostic information for each iteration.\n",
    ">> * Increments the counter.\n",
    "\n",
    "\n",
    "This code essentially simulates a decision-making process for component maintenance based on predictive models and cost considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols):\n",
    "    \n",
    "    for id in pdm_df['id'].unique():\n",
    "        # Loop through each decision point\n",
    "        for cycle in range(pdm_df[pdm_df['id']==id].shape[0]-params+1):\n",
    "            if cycle in array_decisions:\n",
    "                # Prepare data                           \n",
    "                norm_pdm_df = pd.DataFrame(scaler.transform(pdm_df[pdm_df['id']==id][cols_normalize_train][:params+cycle]),\n",
    "                 columns=cols_normalize_train,\n",
    "                 index=pdm_df[pdm_df['id']==id][:params+cycle].index)\n",
    "                print(norm_pdm_df.shape)\n",
    "                join_df = pdm_df[pdm_df['id']==id][:params+cycle][pdm_df[pdm_df['id']==id][:params+cycle].columns.difference(cols_normalize_train)].join(norm_pdm_df)\n",
    "                pdm_df_eval_online = join_df.reindex(columns = pdm_df[pdm_df['id']==id][cycle:params+cycle].columns)\n",
    "\n",
    "                seq_array_test_k = pdm_df_eval_online[sequence_cols].values[cycle:params+cycle]\n",
    "                seq_array_test_k = np.asarray(seq_array_test_k).astype(np.float32).reshape(1,params, len(sequence_cols))\n",
    "                                \n",
    "                \n",
    "                probabilities    = estimator.predict(seq_array_test_k).reshape(3)\n",
    "\n",
    "                # Predict\n",
    "                #with torch.no_grad():\n",
    "                 #   outputs = estimator(seq_tensor).squeeze()\n",
    "                #probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                print(\"probabilities: \", probabilities)\n",
    "                #print(\"seq_array_test_k: \", seq_array_test_k)\n",
    "                \n",
    "                \n",
    "    return probabilities\n",
    "\n",
    "probabilities= calculate_probabilities(estimator, validation_df, min_max_scaler, sequence_length, cols_normalize, sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG8Lip5w6Pt_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device):\n",
    "    counter = 0\n",
    "    for id in validation_df['id'].unique():\n",
    "        print('ID:', id)\n",
    "        preventive_replacement = False\n",
    "        order                  = False\n",
    "\n",
    "        for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1):\n",
    "\n",
    "            if cycle in array_decisions:\n",
    "\n",
    "                norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==id][cols_normalize][:sequence_length+cycle]),\n",
    "                     columns=cols_normalize,\n",
    "                     index=validation_df[validation_df['id']==id][:sequence_length+cycle].index)\n",
    "                print(norm_validation_df.shape)\n",
    "                join_df = validation_df[validation_df['id']==id][:sequence_length+cycle][validation_df[validation_df['id']==id][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "                validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==id][cycle:sequence_length+cycle].columns)\n",
    "\n",
    "                seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "                seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "                #print(seq_array_validation_k.shape)\n",
    "                prob_RUL_smaller_DT    = estimator.predict(seq_array_validation_k).reshape(3)[2]\n",
    "                prob_RUL_smaller_w1    = estimator.predict(seq_array_validation_k).reshape(3)[1]\n",
    "\n",
    "                print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "\n",
    "                # evaluate decision heuristics\n",
    "                if order == False:\n",
    "                    if C_p <= prob_RUL_smaller_w1*C_c:\n",
    "                        print('prob_RUL_smaller_w1:', prob_RUL_smaller_w1)\n",
    "                        print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "                        t_order_array[counter] = sequence_length+cycle\n",
    "                        order = True\n",
    "                        print('component ordering at cycle:', t_order_array[counter])\n",
    "\n",
    "                if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                    print('prob_RUL_smaller_DT:', prob_RUL_smaller_DT)\n",
    "\n",
    "                    t_LC_array[counter] = sequence_length+cycle\n",
    "                    costs_rep_array[counter] = C_p\n",
    "                    print('preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                    # print('component lifecycle:', t_LC)\n",
    "                    preventive_replacement = True\n",
    "                    costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "                    costs_stock_array[counter]  = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "                    # print('delay time', max(t_order+L-t_LC, 0))\n",
    "                    # print('cost_delay_id:',cost_delay_id)\n",
    "                    # print('cost of stock:', cost_stock_id)\n",
    "                    break\n",
    "\n",
    "        if preventive_replacement == False:\n",
    "            t_LC_array[counter] = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "            print('Component failure at t:', t_LC_array[counter])\n",
    "            costs_rep_array[counter] = C_c\n",
    "\n",
    "            if order == False:\n",
    "                costs_delay_array[counter] = L * C_unav\n",
    "            else:\n",
    "                costs_delay_array[counter] = max(t_order_array[counter]+L-t_LC_array[counter], 0) * C_unav\n",
    "                costs_stock_array[counter] = max(t_LC_array[counter] -(t_order_array[counter]+L), 0)*C_inv\n",
    "\n",
    "        print('True failure:', validation_df[validation_df['id']==id]['cycle'].iloc[-1])\n",
    "        print('-----------------------------------------')\n",
    "        counter+=1\n",
    "    \n",
    "    return t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array\n",
    "        \n",
    "\n",
    "t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array=calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onea6sLe6PuA"
   },
   "outputs": [],
   "source": [
    "costs_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAvJN15m6PuA"
   },
   "outputs": [],
   "source": [
    "costs_delay_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mmc0YIA6PuA"
   },
   "outputs": [],
   "source": [
    "costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkJNf5d26PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot = costs_rep_array+costs_delay_array+costs_stock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp4AF1KW6PuB"
   },
   "outputs": [],
   "source": [
    "costs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzXo4Qlw6PuB"
   },
   "outputs": [],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNUa2J36PuB"
   },
   "outputs": [],
   "source": [
    "t_order_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def calculate_T_R_perfect(pdm_df, id, DT=10):\n",
    "    # Retrieve the perfect failure time for the ith component\n",
    "    T_F_perfect = pdm_df[pdm_df['id'] == id]['cycle'].iloc[-1]\n",
    "    \n",
    "    # Calculate k as the largest integer such that k * DT < T_F_perfect\n",
    "    k = int(T_F_perfect // DT)  # Use floor division\n",
    "    T_R_perfect = k * DT  # Calculate T_R_perfect\n",
    "    \n",
    "    return T_R_perfect\n",
    "\n",
    "def calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = (np.mean(costs_rep_array) + np.mean(costs_delay_array) + np.mean(costs_stock_array))\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "def calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df):\n",
    "    # Calculate the average costs and lifecycle times\n",
    "    average_costs = np.mean(costs_rep_array)\n",
    "    average_t_LC_array = np.mean(t_LC_array)\n",
    "\n",
    "    # Calculate T_R_perfect for each component and then find the average\n",
    "    T_R_perfect_array = []\n",
    "    for id in pdm_df['id'].unique():\n",
    "        T_R_perfect = calculate_T_R_perfect(pdm_df, id, DT=10)\n",
    "        T_R_perfect_array.append(T_R_perfect)\n",
    "\n",
    "    average_T_R_perfect = np.mean(T_R_perfect_array)\n",
    "\n",
    "    # Calculate the first part of the numerator\n",
    "    numerator_part1 = average_costs / average_t_LC_array\n",
    "\n",
    "    # Calculate the second part of the numerator\n",
    "    numerator_part2 = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the full numerator\n",
    "    numerator = numerator_part1 - numerator_part2\n",
    "\n",
    "    # Calculate the denominator\n",
    "    denominator = C_p / average_T_R_perfect\n",
    "\n",
    "    # Calculate the decision-oriented metric as a percentage\n",
    "    M_hat = (numerator / denominator) * 100 if denominator != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return M_hat\n",
    "\n",
    "# Usage remains the same\n",
    "# Range of C_p values\n",
    "C_p_values = np.array([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "C_c = 1000  # Constant corrective replacement cost\n",
    "M_hat_Policy1_With_ordering = []\n",
    "\n",
    "#M_hat_Policy1_Without_ordering = []\n",
    "\n",
    "#M_hat_Policy2_With_ordering = []\n",
    "#M_hat_Policy2_Without_ordering = []\n",
    "\n",
    "#M_hat_Policy3_With_ordering = []\n",
    "#M_hat_Policy3_Without_ordering = []\n",
    "\n",
    "# Calculate M_hat for each C_p\n",
    "for C_p in C_p_values:\n",
    "    #Call the function to get updated arrays for policy1 with ordering  \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_1_With_ordering(estimator, validation_df, sequence_length, cols_normalize, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat1 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_With_ordering.append(M_hat1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Call the function to get updated arrays for policy1 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, prob_RUL_smaller_DT_dict, replacement_probs_dict = calculate_probabilities_for_pdm_policy_1_Without_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat2 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy1_Without_ordering.append(M_hat2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 with ordering\n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_2_With_ordering(estimator, pdm_df,scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat3 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_With_ordering.append(M_hat3)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy2 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, optimal_replacement_times, rul_distributions = calculate_probabilities_for_pdm_policy_2_Without_ordering(\n",
    "    estimator, pdm_df, scaler, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "    \n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat4 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy2_Without_ordering.append(M_hat4)\n",
    "    \n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 with ordering                   \n",
    "    t_order_array, t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array = calculate_probabilities_for_pdm_policy_3_With_ordering(estimator, pdm_df,train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L,DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat5 = calculate_decision_metric_with_ordering(costs_rep_array, costs_delay_array, costs_stock_array, t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_With_ordering.append(M_hat5)\n",
    "    \n",
    "    # Call the function to get updated arrays for policy3 without ordering\n",
    "    t_LC_array, costs_rep_array, costs_delay_array, costs_stock_array, T_R_k_optimal_dict = calculate_probabilities_for_pdm_policy_3_Without_ordering(\n",
    "    estimator, pdm_df, scaler, train_df, params, cols_normalize_train, sequence_cols, C_p, C_c, L, DT, C_unav, C_inv, device)\n",
    "\n",
    "    # Calculate M_hat using the updated arrays\n",
    "    M_hat6 = calculate_decision_metric_without_ordering(costs_rep_array,  t_LC_array, C_p, pdm_df)\n",
    "    M_hat_Policy3_Without_ordering.append(M_hat6)    \n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate C_p/C_c ratios\n",
    "C_p_C_c_ratios = C_p_values / C_c\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "policies = [\n",
    "    (M_hat_Policy1_With_ordering, 'Policy 1 With Ordering')\n",
    "]\n",
    "\n",
    "for data, label in policies:\n",
    "    plt.plot(C_p_C_c_ratios, data, marker='o', label=label)\n",
    "    print(f\"{label} data: {data}\")  # Print data for debugging\n",
    "\n",
    "plt.title('Decision Metric M_hat% vs C_p/C_c Ratio')\n",
    "plt.xlabel('C_p/C_c Ratio')\n",
    "plt.ylabel('Decision Metric M_hat%')\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.xticks(C_p_C_c_ratios)  # Set x-ticks to the calculated ratios\n",
    "plt.axhline(0, color='black', lw=0.5, ls='--')  # Add a horizontal line at y=0\n",
    "\n",
    "# Adjust y-axis limits to ensure all data is visible\n",
    "plt.ylim(bottom=max(1, plt.ylim()[0]), top=plt.ylim()[1]*1.1)\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('decision_metric_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6g7k8Yrftya"
   },
   "source": [
    "### This code calculates the expected cost per unit time using the LSTM model. It computes the mean of the total costs divided by the mean of the time to component failure (t_LC_array). This metric gives an estimate of the average cost incurred per unit time in the system, considering both maintenance and operational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3_N4kNM6PuC"
   },
   "outputs": [],
   "source": [
    "expected_cost_LSTM = np.mean(costs_tot) / np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgoTz57gC_P"
   },
   "source": [
    "This code segment calculates the expected cost per unit time assuming perfect prognostics.\n",
    "`1. Perfect Prognostics Calculation:`\n",
    "> * It initializes an array `t_LC_perfect_array` to store the time of component failure for each unit in the validation dataset. This is calculated by dividing the last observed cycle number by the decision interval DT and then flooring the result to get the last decision cycle before failure.\n",
    "> * The loop iterates over each unique ID in the validation dataset, calculates the time of component failure for each unit, and stores it in\n",
    "`t_LC_perfect_array`.<br>\n",
    "> * `math.floor()` is used to round down the result to the nearest multiple of `DT`.\n",
    "> * Finally, the loop increments the counter for each unit.<br>\n",
    "\n",
    "`2. Cost Calculation:`\n",
    "> * `costs_perfect_array` is initialized with a value of `C_p`, representing the cost of preventive replacements. In a perfect scenario, only preventive replacements are made.\n",
    "> * This array holds the same cost value for each unit in the validation dataset.\n",
    "\n",
    "`3. Expected Cost Calculation:`\n",
    "> * `expected_cost_perfect` is calculated by taking the mean of `costs_perfect_array` and dividing it by the mean of `t_LC_perfect_array`.\n",
    "> * This calculation provides an estimate of the average cost per unit time assuming perfect prognostics, where components are replaced preventively at regular intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOD6NDfH6PuC"
   },
   "outputs": [],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT\n",
    "    counter+=1\n",
    "\n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-vK-DBi6PuC"
   },
   "outputs": [],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7_LJce6PuC"
   },
   "outputs": [],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KofjvavY6PuD"
   },
   "outputs": [],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P42Nu-Rg6PuE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
