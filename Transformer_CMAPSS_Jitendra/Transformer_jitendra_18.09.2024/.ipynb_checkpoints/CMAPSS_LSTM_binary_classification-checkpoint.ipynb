{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "70-XsHkDGUKu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 10306692680519447721]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BsYPkY7gYGx"
   },
   "source": [
    "# Binary classification\n",
    "Predict if an asset will fail within the next 10 cycles (i.e., one decision making time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'binary_model.h5'# This file then contains the already trained network, so that you don't have to retrain every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# read training data \n",
    "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.49</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.68</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>520.01</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.67</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>519.30</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
       "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
       "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
       "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
       "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
       "0      1400.60  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1      1403.14  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2      1404.20  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3      1401.87  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4      1406.22  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "...        ...    ...  ...     ...      ...      ...     ...   ...  ...   ...   \n",
       "20626  1428.63  14.62  ...  519.49  2388.26  8137.60  8.4956  0.03  397  2388   \n",
       "20627  1433.58  14.62  ...  519.68  2388.22  8136.50  8.5139  0.03  395  2388   \n",
       "20628  1428.18  14.62  ...  520.01  2388.24  8141.05  8.5646  0.03  398  2388   \n",
       "20629  1426.53  14.62  ...  519.67  2388.23  8139.29  8.5389  0.03  395  2388   \n",
       "20630  1432.14  14.62  ...  519.30  2388.26  8137.33  8.5036  0.03  396  2388   \n",
       "\n",
       "         s19    s20      s21  \n",
       "0      100.0  39.06  23.4190  \n",
       "1      100.0  39.00  23.4236  \n",
       "2      100.0  38.95  23.3442  \n",
       "3      100.0  38.88  23.3739  \n",
       "4      100.0  38.90  23.4044  \n",
       "...      ...    ...      ...  \n",
       "20626  100.0  38.49  22.9735  \n",
       "20627  100.0  38.30  23.1594  \n",
       "20628  100.0  38.44  22.9333  \n",
       "20629  100.0  38.29  23.0640  \n",
       "20630  100.0  38.37  23.0522  \n",
       "\n",
       "[20631 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification, \n",
    "# trying to answer the question: is a specific engine going to fail within w cycles?\n",
    "w = 10\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w, 1, 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I want to separate the training set into a training and validation set. I will use 80 training sets for the training and 20 training sets as evaluation sets for the PdM policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ID = np.arange(81,101,1) # I take the 20 last #TODO: make this random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I separate into training and validation set before any data scaling is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = train_df.loc[train_df['id'].isin(list_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.04</td>\n",
       "      <td>1589.91</td>\n",
       "      <td>1406.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8134.78</td>\n",
       "      <td>8.4455</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.87</td>\n",
       "      <td>23.3365</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.65</td>\n",
       "      <td>1586.25</td>\n",
       "      <td>1407.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8140.37</td>\n",
       "      <td>8.4573</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.91</td>\n",
       "      <td>23.3452</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.42</td>\n",
       "      <td>1396.40</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.75</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3610</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16141</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.41</td>\n",
       "      <td>1594.89</td>\n",
       "      <td>1404.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.12</td>\n",
       "      <td>8.4403</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.77</td>\n",
       "      <td>23.4206</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.41</td>\n",
       "      <td>1590.49</td>\n",
       "      <td>1409.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.21</td>\n",
       "      <td>8.3971</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.3311</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4493 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "16138   81      1   -0.0050    0.0003     100.0  518.67  642.04  1589.91   \n",
       "16139   81      2    0.0023    0.0002     100.0  518.67  642.65  1586.25   \n",
       "16140   81      3   -0.0005    0.0005     100.0  518.67  642.55  1586.42   \n",
       "16141   81      4   -0.0001   -0.0000     100.0  518.67  642.41  1594.89   \n",
       "16142   81      5    0.0024    0.0002     100.0  518.67  643.41  1590.49   \n",
       "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
       "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
       "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
       "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
       "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
       "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
       "\n",
       "            s4     s5  ...      s14     s15   s16  s17   s18    s19    s20  \\\n",
       "16138  1406.63  14.62  ...  8134.78  8.4455  0.03  391  2388  100.0  38.87   \n",
       "16139  1407.88  14.62  ...  8140.37  8.4573  0.03  392  2388  100.0  38.91   \n",
       "16140  1396.40  14.62  ...  8139.75  8.4522  0.03  394  2388  100.0  39.04   \n",
       "16141  1404.86  14.62  ...  8144.12  8.4403  0.03  392  2388  100.0  38.77   \n",
       "16142  1409.58  14.62  ...  8139.21  8.3971  0.03  392  2388  100.0  39.04   \n",
       "...        ...    ...  ...      ...     ...   ...  ...   ...    ...    ...   \n",
       "20626  1428.63  14.62  ...  8137.60  8.4956  0.03  397  2388  100.0  38.49   \n",
       "20627  1433.58  14.62  ...  8136.50  8.5139  0.03  395  2388  100.0  38.30   \n",
       "20628  1428.18  14.62  ...  8141.05  8.5646  0.03  398  2388  100.0  38.44   \n",
       "20629  1426.53  14.62  ...  8139.29  8.5389  0.03  395  2388  100.0  38.29   \n",
       "20630  1432.14  14.62  ...  8137.33  8.5036  0.03  396  2388  100.0  38.37   \n",
       "\n",
       "           s21  RUL  label1  \n",
       "16138  23.3365  239       0  \n",
       "16139  23.3452  238       0  \n",
       "16140  23.3610  237       0  \n",
       "16141  23.4206  236       0  \n",
       "16142  23.3311  235       0  \n",
       "...        ...  ...     ...  \n",
       "20626  22.9735    4       1  \n",
       "20627  23.1594    3       1  \n",
       "20628  22.9333    2       1  \n",
       "20629  23.0640    1       1  \n",
       "20630  23.0522    0       1  \n",
       "\n",
       "[4493 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df.id.isin(list_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the min max scaling of the training data set\n",
    "# use min_max_scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>cycle_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.425154</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.473456</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.732001</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.386193</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.661565</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387283</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.269082</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.704790</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>80</td>\n",
       "      <td>181</td>\n",
       "      <td>0.739884</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840361</td>\n",
       "      <td>0.756892</td>\n",
       "      <td>0.787812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>0.369473</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.498615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.736614</td>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16136</th>\n",
       "      <td>80</td>\n",
       "      <td>184</td>\n",
       "      <td>0.358382</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.728412</td>\n",
       "      <td>0.809926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291339</td>\n",
       "      <td>0.127551</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0.583815</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804217</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.661040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>0.177438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.509695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16138 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
       "0       1      1  0.456647  0.166667       0.0  0.0  0.183735  0.425154   \n",
       "1       1      2  0.606936  0.250000       0.0  0.0  0.283133  0.473456   \n",
       "2       1      3  0.248555  0.750000       0.0  0.0  0.343373  0.386193   \n",
       "3       1      4  0.537572  0.500000       0.0  0.0  0.343373  0.267715   \n",
       "4       1      5  0.387283  0.333333       0.0  0.0  0.349398  0.269082   \n",
       "...    ..    ...       ...       ...       ...  ...       ...       ...   \n",
       "16133  80    181  0.739884  0.666667       0.0  0.0  0.840361  0.756892   \n",
       "16134  80    182  0.416185  0.833333       0.0  0.0  0.783133  0.621554   \n",
       "16135  80    183  0.601156  0.500000       0.0  0.0  0.686747  0.736614   \n",
       "16136  80    184  0.358382  0.666667       0.0  0.0  0.789157  0.728412   \n",
       "16137  80    185  0.583815  0.500000       0.0  0.0  0.804217  0.805195   \n",
       "\n",
       "             s4   s5  ...       s15  s16       s17  s18  s19       s20  \\\n",
       "0      0.309757  0.0  ...  0.363986  0.0  0.363636  0.0  0.0  0.708661   \n",
       "1      0.352633  0.0  ...  0.411312  0.0  0.363636  0.0  0.0  0.661417   \n",
       "2      0.370527  0.0  ...  0.357445  0.0  0.181818  0.0  0.0  0.622047   \n",
       "3      0.331195  0.0  ...  0.166603  0.0  0.363636  0.0  0.0  0.566929   \n",
       "4      0.404625  0.0  ...  0.402078  0.0  0.454545  0.0  0.0  0.582677   \n",
       "...         ...  ...  ...       ...  ...       ...  ...  ...       ...   \n",
       "16133  0.787812  0.0  ...  0.748365  0.0  0.818182  0.0  0.0  0.181102   \n",
       "16134  0.743754  0.0  ...  0.863409  0.0  0.727273  0.0  0.0  0.141732   \n",
       "16135  0.878629  0.0  ...  0.714506  0.0  0.818182  0.0  0.0  0.141732   \n",
       "16136  0.809926  0.0  ...  0.667180  0.0  0.818182  0.0  0.0  0.291339   \n",
       "16137  0.661040  0.0  ...  0.769912  0.0  0.818182  0.0  0.0  0.149606   \n",
       "\n",
       "            s21  RUL  label1  cycle_norm  \n",
       "0      0.725482  191       0    0.000000  \n",
       "1      0.732001  190       0    0.002770  \n",
       "2      0.619473  189       0    0.005540  \n",
       "3      0.661565  188       0    0.008310  \n",
       "4      0.704790  187       0    0.011080  \n",
       "...         ...  ...     ...         ...  \n",
       "16133  0.369473    4       1    0.498615  \n",
       "16134  0.151786    3       1    0.501385  \n",
       "16135  0.037698    2       1    0.504155  \n",
       "16136  0.127551    1       1    0.506925  \n",
       "16137  0.177438    0       1    0.509695  \n",
       "\n",
       "[16138 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "zSInZu-EkFtf",
    "outputId": "eeaa639d-5fe6-41e1-a114-5de359a792c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 50, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we always take the measurements of the last 50 cycles as input!\n",
    "# Every sequence is reduced by a length of 50 (=sequence_length). We have 80 training sets, 80*50 = 4000 \"less\" inputs\n",
    "# train_df.shape = (16138, 30)\n",
    "# seq_array.shape = (12138, 50, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12138, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previous ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target. \n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 100)           50400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 80,651\n",
      "Trainable params: 80,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set created during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1393
    },
    "id": "FpVYSzXkmk5l",
    "outputId": "50af192e-e9df-46ad-ffa3-0b7bd0e3f29c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-79fcddb0a351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# summarize history for Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 3s 41ms/step - loss: 0.0333 - accuracy: 0.9854\n",
      "Accurracy: 0.9854177236557007\n"
     ]
    }
   ],
   "source": [
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n",
      "[[11178    80]\n",
      " [   97   783]]\n",
      "precision =  0.9073001158748552 \n",
      " recall =  0.8897727272727273\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "# y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\") # this way (>0.5) the outcome goes from a probability to 0,1\n",
    "y_true = label_array\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# compute precision and recall\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## PdM first policy evaluation on the validation set.\n",
    "\n",
    "For each validation set, I need to give the on-line sensor data as input to the trained LSTM.\n",
    "\n",
    "This will give me the probability of RUL_k<w at fixed time steps k*DT that are discrete decision making time steps.\n",
    "\n",
    "One issue that I see: The LSTM needs the measurements sequence from the last 50 cycles as input. This means that the PdM decision policy evaluation can only start from cycle 50 onwards. What if my component fails within the first 50 cycles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next cell simply calls the already trained model. If you dont want to retrain the LSTM net every time, you simply have to call this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions for the costs, taken by the 2019 RESS paper\n",
    "C_p = 100\n",
    "C_c = 10000\n",
    "DT  = 10  # Decisions can be taken every DT=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First PdM policy evaluation on a single validation data set (id=82). The scaling of the dataset is performed anew on-line every time a new data set appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['cycle_norm'] = validation_df['cycle']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle: 50\n",
      "(50, 29)\n",
      "Pr(RUL_k<=DT) = [8.3118786e-05]\n",
      "cycle: 60\n",
      "(60, 29)\n",
      "Pr(RUL_k<=DT) = [8.25756e-05]\n",
      "cycle: 70\n",
      "(70, 29)\n",
      "Pr(RUL_k<=DT) = [8.4648374e-05]\n",
      "cycle: 80\n",
      "(80, 29)\n",
      "Pr(RUL_k<=DT) = [8.443222e-05]\n",
      "cycle: 90\n",
      "(90, 29)\n",
      "Pr(RUL_k<=DT) = [8.584719e-05]\n",
      "cycle: 100\n",
      "(100, 29)\n",
      "Pr(RUL_k<=DT) = [8.555606e-05]\n",
      "cycle: 110\n",
      "(110, 29)\n",
      "Pr(RUL_k<=DT) = [8.70433e-05]\n",
      "cycle: 120\n",
      "(120, 29)\n",
      "Pr(RUL_k<=DT) = [8.8589586e-05]\n",
      "cycle: 130\n",
      "(130, 29)\n",
      "Pr(RUL_k<=DT) = [9.3702394e-05]\n",
      "cycle: 140\n",
      "(140, 29)\n",
      "Pr(RUL_k<=DT) = [9.642058e-05]\n",
      "cycle: 150\n",
      "(150, 29)\n",
      "Pr(RUL_k<=DT) = [9.965459e-05]\n",
      "cycle: 160\n",
      "(160, 29)\n",
      "Pr(RUL_k<=DT) = [0.00010582]\n",
      "cycle: 170\n",
      "(170, 29)\n",
      "Pr(RUL_k<=DT) = [0.0001263]\n",
      "cycle: 180\n",
      "(180, 29)\n",
      "Pr(RUL_k<=DT) = [0.00019634]\n",
      "cycle: 190\n",
      "(190, 29)\n",
      "Pr(RUL_k<=DT) = [0.00047395]\n",
      "cycle: 200\n",
      "(200, 29)\n",
      "Pr(RUL_k<=DT) = [0.3049912]\n",
      "preventive replacement informed at cycle: 200\n",
      "component lifecycle: 200\n"
     ]
    }
   ],
   "source": [
    "id=82\n",
    "preventive_replacement = False\n",
    "for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1): \n",
    "    \n",
    "    if cycle in array_decisions:\n",
    "        print('cycle:', sequence_length+cycle)\n",
    "        \n",
    "        norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==82][cols_normalize][:sequence_length+cycle]), \n",
    "                 columns=cols_normalize, \n",
    "                 index=validation_df[validation_df['id']==82][:sequence_length+cycle].index)\n",
    "        \n",
    "        join_df = validation_df[validation_df['id']==82][:sequence_length+cycle][validation_df[validation_df['id']==82][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "        validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==82][:sequence_length+cycle].columns)        \n",
    "        print(validation_df_eval_online.shape)\n",
    "        \n",
    "        seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "        seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "        prob_RUL_smaller_DT = estimator.predict(seq_array_validation_k).reshape(1)\n",
    "        print('Pr(RUL_k<=DT) =', prob_RUL_smaller_DT)\n",
    "    \n",
    "        # evaluate decision heuristics\n",
    "        if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "            t_LC = sequence_length+cycle\n",
    "            cost_id = C_p\n",
    "            print('preventive replacement informed at cycle:', t_LC)\n",
    "            print('component lifecycle:', t_LC)\n",
    "            preventive_replacement = True\n",
    "            break\n",
    "            \n",
    "if preventive_replacement == False:\n",
    "    t_LC = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "    print('Component failure at t:', t_LC)\n",
    "    cost_id = C_c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preventive_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First PdM policy evaluation on a the whole validation data set (ids 81 to 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_array = np.zeros(20)\n",
    "t_LC_array  = np.zeros(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 81  preventive replacement informed at cycle: 240.0\n",
      "component lifecycle: 240.0\n",
      "true failure time: 240\n",
      "------------------------------------------------------\n",
      "ID: 82  preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "true failure time: 214\n",
      "------------------------------------------------------\n",
      "ID: 83  preventive replacement informed at cycle: 280.0\n",
      "component lifecycle: 280.0\n",
      "true failure time: 293\n",
      "------------------------------------------------------\n",
      "ID: 84  preventive replacement informed at cycle: 260.0\n",
      "component lifecycle: 260.0\n",
      "true failure time: 267\n",
      "------------------------------------------------------\n",
      "ID: 85  preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "true failure time: 188\n",
      "------------------------------------------------------\n",
      "ID: 86  preventive replacement informed at cycle: 270.0\n",
      "component lifecycle: 270.0\n",
      "true failure time: 278\n",
      "------------------------------------------------------\n",
      "ID: 87  preventive replacement informed at cycle: 170.0\n",
      "component lifecycle: 170.0\n",
      "true failure time: 178\n",
      "------------------------------------------------------\n",
      "ID: 88  preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "true failure time: 213\n",
      "------------------------------------------------------\n",
      "ID: 89  preventive replacement informed at cycle: 210.0\n",
      "component lifecycle: 210.0\n",
      "true failure time: 217\n",
      "------------------------------------------------------\n",
      "ID: 90  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "true failure time: 154\n",
      "------------------------------------------------------\n",
      "ID: 91  preventive replacement informed at cycle: 130.0\n",
      "component lifecycle: 130.0\n",
      "true failure time: 135\n",
      "------------------------------------------------------\n",
      "ID: 92  preventive replacement informed at cycle: 330.0\n",
      "component lifecycle: 330.0\n",
      "true failure time: 341\n",
      "------------------------------------------------------\n",
      "ID: 93  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "true failure time: 155\n",
      "------------------------------------------------------\n",
      "ID: 94  preventive replacement informed at cycle: 250.0\n",
      "component lifecycle: 250.0\n",
      "true failure time: 258\n",
      "------------------------------------------------------\n",
      "ID: 95  preventive replacement informed at cycle: 280.0\n",
      "component lifecycle: 280.0\n",
      "true failure time: 283\n",
      "------------------------------------------------------\n",
      "ID: 96  preventive replacement informed at cycle: 330.0\n",
      "component lifecycle: 330.0\n",
      "true failure time: 336\n",
      "------------------------------------------------------\n",
      "ID: 97  preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "true failure time: 202\n",
      "------------------------------------------------------\n",
      "ID: 98  preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "true failure time: 156\n",
      "------------------------------------------------------\n",
      "ID: 99  preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "true failure time: 185\n",
      "------------------------------------------------------\n",
      "ID: 100  preventive replacement informed at cycle: 190.0\n",
      "component lifecycle: 190.0\n",
      "true failure time: 200\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for id in validation_df['id'].unique():\n",
    "    # print(id)\n",
    "    preventive_replacement = False\n",
    "    for cycle in range(validation_df[validation_df['id']==id].shape[0]-sequence_length+1): \n",
    "\n",
    "        if cycle in array_decisions:\n",
    "            \n",
    "            norm_validation_df = pd.DataFrame(min_max_scaler.transform(validation_df[validation_df['id']==id][cols_normalize][:sequence_length+cycle]), \n",
    "                 columns=cols_normalize, \n",
    "                 index=validation_df[validation_df['id']==id][:sequence_length+cycle].index)\n",
    "            \n",
    "            join_df = validation_df[validation_df['id']==id][:sequence_length+cycle][validation_df[validation_df['id']==id][:sequence_length+cycle].columns.difference(cols_normalize)].join(norm_validation_df)\n",
    "            validation_df_eval_online = join_df.reindex(columns = validation_df[validation_df['id']==id][cycle:sequence_length+cycle].columns)        \n",
    "        \n",
    "            \n",
    "            seq_array_validation_k = validation_df_eval_online[sequence_cols].values[cycle:sequence_length+cycle]\n",
    "            seq_array_validation_k = np.asarray(seq_array_validation_k).astype(np.float32).reshape(1,sequence_length, nb_features)\n",
    "            prob_RUL_smaller_DT = estimator.predict(seq_array_validation_k).reshape(1)\n",
    "            # print(prob_RUL_smaller_DT)\n",
    "\n",
    "            # evaluate decision heuristics\n",
    "            if C_p <= prob_RUL_smaller_DT*C_c:\n",
    "                t_LC_array[counter] = sequence_length+cycle\n",
    "                costs_array[counter] = C_p\n",
    "                print('ID:', id, ' preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                print('component lifecycle:', t_LC_array[counter])\n",
    "                preventive_replacement = True\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        t_LC_array[counter] = validation_df[validation_df['id']==id]['cycle'].iloc[-1]\n",
    "        print('ID:', id, ' component failure at t:', t_LC_array[counter])\n",
    "        costs_array[counter] = C_c\n",
    "\n",
    "    print('true failure time:', validation_df[validation_df['id']==id]['cycle'].iloc[-1] )\n",
    "    print('------------------------------------------------------')\n",
    "        \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 200., 280., 260., 180., 270., 170., 200., 210., 150., 130.,\n",
       "       330., 150., 250., 280., 330., 200., 150., 180., 190.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perfect prognostics\n",
    "import math\n",
    "t_LC_perfect_array  = np.zeros(20)\n",
    "counter=0\n",
    "for id in validation_df['id'].unique():\n",
    "    t_LC_perfect_array[counter] = math.floor(validation_df[validation_df['id']==id]['cycle'].iloc[-1] /DT) * DT    \n",
    "    counter+=1\n",
    "    \n",
    "costs_perfect_array = np.ones(20)*C_p # a perfect policy will only lead to preventive replacements\n",
    "\n",
    "expected_cost_perfect = np.mean(costs_perfect_array)/np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240., 210., 290., 260., 180., 270., 170., 210., 210., 150., 130.,\n",
       "       340., 150., 250., 280., 330., 200., 150., 180., 200.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_perfect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45977011494252873"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of the expected cost per unit time, Eqns. (3) and (4) of our paper.\n",
    "expected_cost_LSTM = np.mean(costs_array)/np.mean(t_LC_array)\n",
    "expected_cost_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio = costs_array/t_LC_array\n",
    "# np.mean(ratio) # indeed taking the mean of the ratio gives a different result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011494252873563237"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_LSTM - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1494252873563238"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "name": "Predictive-Maintenance-using-LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
